#+TITLE: Lab book Fluotracify
#+AUTHOR: Alexander Seltmann
#+LANGUAGE: en
#+PROPERTY: header-args :eval never-export :exports both
#+OPTIONS: toc:4
#+OPTIONS: H:4
#+HTML_HEAD_EXTRA: <style type="text/css">.example {background-color: #FBFBBF;}</style>
#+HTML_HEAD_EXTRA: <style type="text/css">pre.src-emacs-lisp {background-color: #F7ECFB;}</style>
#+HTML_HEAD_EXTRA: <style type="text/css">pre.src-sh {background-color: #F0FBE9;}</style>
#+HTML_HEAD_EXTRA: <style type="text/css">pre.src-tmux {background-color: #E1EED8;}</style>
#+HTML_HEAD_EXTRA: <style type="text/css">pre.src-python {background-color: #E6EDF4;}</style>
#+HTML_HEAD_EXTRA: <style type="text/css">pre.src-jupyter-python {background-color: #FAEAE1;}</style>
#+HTML_HEAD_EXTRA: <style type="text/css">details {padding: 1em; background-color: #f0f0f0; border-radius: 15px; color: hsl(157 75% 20%); font-size: 0.9em; box-shadow: 0.05em 0.1em 5px 0.01em  #00000057;}</style>
#+HTML_HEAD_EXTRA: <style type="text/css">details:hover {background:pink;}</style>
#+HTML_HEAD_EXTRA: <style type="text/css">table {display: block; overflow-x: auto; white-space: nowrap;}</style>


* Technical Notes
** README
*** General:
   - This file corresponds to my lab book for my doctoral thesis tackling
     artifact correction in Fluorescence Correlation Spectroscopy (FCS)
     measurements using Deep Neural Networks. It also contains notes taken
     during the process of setting up this workflow for reproducible research.
   - This file contains explanations of how things are organized, of the
     workflow for doing experiments, changes made to the code, and the observed
     behavior in the "* Data" section.
   - The branching model used is described in [[http://starpu-simgrid.gforge.inria.fr/misc/SIGOPS_paper.pdf][this paper]]. Therefore: if you
     are interested in the "* Data" section, you have to =git clone= the /data/
     branch of the repository. The /main/ branch is clean from any results, it
     contains only source code and the analysis.
   - This project is my take on [[https://en.wikipedia.org/wiki/Open-notebook_science][Open-notebook science]]. The idea was postulated in
     a blog post in 2006:
     #+BEGIN_QUOTE
     ... there is a URL to a laboratory notebook that is freely available and
     indexed on common search engines. It does not necessarily have to look like
     a paper notebook but it is essential that all of the information available
     to the researchers to make their conclusions is equally available to the
     rest of the world ---Jean-Claude Bradley
     #+END_QUOTE
   - Proposal on how to deal with truly private data (e.g. notes from a
     confidential meeting with a colleague), which might otherwise be noted in a
     normal Lab notebook: do not include them here. Only notes relevant to the
     current project should be taken
*** Code block languages used in this document

   #+BEGIN_SRC sh
     # This is a sh block for shell / bash scripting. In the context of this file,
     # these blocks are mainly used for operations on my local computer.
     # In the LabBook.html rendering of this document, these blocks will have a
     # light green colour (#F0FBE9)
   #+END_SRC

   #+BEGIN_SRC tmux
     # This block can open and access tmux sessions, used for shell scripting on
     # remote computing clusters.
     # In the LabBook.html rendering of this document, these blocks will have a
     # distinct light green colour (#E1EED8)
   #+END_SRC

   #+BEGIN_SRC python
     # This is a python block. In the context of this file, it is seldomly used
     # (only for examplary scripts.)
     # In the LabBook.html rendering of this document, these blocks will have a
     # light blue colour (#E6EDF4)
   #+END_SRC

   #+BEGIN_SRC jupyter-python :session /jpy:localhost#8889:704d35be-572a-4268-a70b-565164b8620f
     # This is a jupyter-python block. The code is sent to a jupyter kernel running
     # on a remote high performance computing cluster. Most of my jupyter code is
     # executed this way.
     # In the LabBook.html rendering of this document, these blocks will have a
     # light orange colour (#FAEAE1)
   #+END_SRC

   #+BEGIN_SRC emacs-lisp
     ;; This is a emacs-lisp block, the language used to customize Emacs, which is
     ;; sometimes necessary, since the reproducible workflow of this LabBook is
     ;; tightly integrated with Emacs and org-mode.
     ;; In the LabBook.html rendering of this document, these blocks will have a
     ;; light violet colour (#F7ECFB)
   #+END_SRC

   #+begin_example
     This is a literal example block. It can be used very flexibly - in the context
     of this document the output of most code blocks is displayed this way.
     In the LabBook.html rendering of this document, these blocks will have a light
     yellow colour (#FBFBBF)
   #+end_example

   #+begin_details
   #+begin_example
     This is a literal example block enclosed in a details block. This is useful to
     make the page more readable by collapsing large amounts of output.
     In the Labbook.html rendering of this document, the details block will have a
     light grey colour (#f0f0f0) and a pink color when hovering above it.
   #+end_example
   #+end_details

*** Experiments workflow:
   1) Create a new branch from =main=
   2) Print out the git log from the latest commit and the metadata
   3) Call the analysis scripts, follow the principles outlined in
      [[* Organization of code]]
   4) All machine learning runs are saved in =data/mlruns=, all other data in
      =data/#experiment-name=
   5) Add a ~** exp-<date>-<name>~" section to this file under [[* Data]]
   6) Commit/push the results of this separate branch
   7) Merge this new branch with the remote =data= branch
*** Example for experimental setup procedure

**** Setting starting a jupyter kernel from a remote jupyter session using =emacs-jupyter= in =org babel=
    :PROPERTIES:
    :CUSTOM_ID: sec-jupyter-setup
    :END:

*** tools used (notes)
**** Emacs =magit=
   - =gitflow-avh= (=magit-flow=) to follow the flow
   - possibly https://github.com/magit/magit-annex for large files. Follow this:
     https://git-annex.branchable.com/walkthrough/
   - maybe check out git-toolbelt at some point
     https://github.com/nvie/git-toolbelt#readme with
     https://nvie.com/posts/git-power-tools/
**** jupyter
   - emacs jupyter for running and connecting to kernel on server:
     https://github.com/dzop/emacs-jupyter
   - if I actually still would use .ipynb files, these might come handy:
     + jupytext: https://github.com/mwouts/jupytext
     + nbstripout: https://github.com/kynan/nbstripout
**** mlflow
   - https://docs.faculty.ai/user-guide/experiments/index.html and
     https://docs.microsoft.com/en-us/azure/databricks/_static/notebooks/hls-image-processing/02-image-segmentation-dl.html
**** tensorflow
   - https://www.tensorflow.org/tensorboard/image_summaries

** Template for data entry and setup notes:
*** exp-#date-#title
**** git:

    #+begin_src sh
    git log -1
    #+end_src

**** System Metadata:

    #+NAME: jp-metadata
    #+BEGIN_SRC jupyter-python :var _long="true"
      import os
      import pprint

      ramlist = os.popen('free -th').readlines()[-1].split()[1:]

      print('No of CPUs in system:', os.cpu_count())
      print('No of CPUs the current process can use:',
            len(os.sched_getaffinity(0)))
      print('load average:', os.getloadavg())
      print('os.uname(): ', os.uname())
      print('PID of process:', os.getpid())
      print('RAM total: {}, RAM used: {}, RAM free: {}'.format(
          ramlist[0], ramlist[1], ramlist[2]))

      !echo the current directory: $PWD
      !echo My disk usage:
      !df -h
      if _long:
          %conda list
          pprint.pprint(dict(os.environ), sort_dicts=False)

    #+END_SRC

**** Tmux setup and scripts
    :PROPERTIES:
    :CUSTOM_ID: scripts-tmux
    :END:

    #+NAME: setup-tmux
    #+BEGIN_SRC sh :session local
    rm ~/.tmux-local-socket-remote-machine
    REMOTE_SOCKET=$(ssh ara 'tmux ls -F "#{socket_path}"' | head -1)
    echo $REMOTE_SOCKET
    ssh ara -tfN \
        -L ~/.tmux-local-socket-remote-machine:$REMOTE_SOCKET
    #+END_SRC

    #+RESULTS: setup-tmux
    | rm:                                  | cannot                               | remove    | '/home/lex/.tmux-local-socket-remote-machine': | No | such | file | or | directory |
    | ye53nis@ara-login01.rz.uni-jena.de's | password:                            |           |                                                |    |      |      |    |           |
    | /tmp/tmux-67339/default              |                                      |           |                                                |    |      |      |    |           |
    | >                                    | ye53nis@ara-login01.rz.uni-jena.de's | password: |                                                |    |      |      |    |           |

**** SSH tunneling
    :PROPERTIES:
    :CUSTOM_ID: ssh-tunneling
    :END:

    Different applications can be run on the remote compute node. If I want to
    access them at the local machine, and open them with the browser, I use this
    tunneling script.

    #+NAME: ssh-tunnel
    #+BEGIN_SRC sh :session org-tunnel :var port="8889" :var node="node001"
    ssh -t -t ara -L $port:localhost:$port ssh $node -L $port:Localhost:$port
    #+END_SRC

    Apps I use that way:
    - Jupyter lab for running Python 3-Kernels
    - TensorBoard
    - Mlflow ui

**** jupyter scripts
    :PROPERTIES:
    :CUSTOM_ID: scripts-jp
    :END:

    Starting a jupyter instance on a server where the necessary libraries are
    installed is easy using this script:

    #+NAME: jpt-tmux
    #+BEGIN_SRC tmux :socket ~/.tmux-local-socket-remote-machine
    conda activate tf
    export PORT=8889
    export XDG_RUNTIME_DIR=''
    export XDG_RUNTIME_DIR=""
    jupyter lab --no-browser --port=$PORT
    #+END_SRC

    On the compute node of the HPC, the users' environment is managed through
    module files using the system [[https://lmod.readthedocs.io][Lmod]]. The =export XDG_RUNTIME_DIR= statements
    are needed because of a jupyter bug which did not let it start. Right now,
    =ob-tmux= does not support a =:var= header like normal =org-babel= does. So
    the =$port= variable has to be set here in the template.

    Now this port has to be tunnelled on our local computer (See
    [[#ssh-tunneling]]). While the tmux session above keeps running, no matter if
    Emacs is running or not, this following ssh tunnel needs to be active
    locally to connect to the notebook. If you close Emacs, it would need to be
    reestablished

*** Setup notes
**** Setting up a tmux connection from using =ob-tmux= in =org-babel=
    :PROPERTIES:
    :CUSTOM_ID: sec-tmux-setup
    :END:
    - prerequisite: tmux versions need to be the same locally and on the server.
      Let's verify that now.
      - the local tmux version:

        #+BEGIN_SRC sh
        tmux -V
        #+END_SRC

        #+RESULTS:
        : tmux 3.0a

      - the remote tmux version:

       #+BEGIN_SRC sh :session local
        ssh ara tmux -V
      #+END_SRC

        #+RESULTS:
        | ye53nis@ara-login01.rz.uni-jena.de's | password: |
        | tmux                                 | 3.0a      |

    - as is described in [[https://github.com/ahendriksen/ob-tmux][the ob-tmux readme]], the following code snippet creates
      a socket on the remote machine and forwards this socket to the local
      machine (note that =socket_path= was introduced in tmux version 2.2)

      #+BEGIN_SRC sh :session local
      REMOTE_SOCKET=$(ssh ara 'tmux ls -F "#{socket_path}"' | head -1)
      echo $REMOTE_SOCKET
      ssh ara -tfN \
          -L ~/.tmux-local-socket-remote-machine:$REMOTE_SOCKET
      #+END_SRC

      #+RESULTS:
      | ye53nis@ara-login01.rz.uni-jena.de's | password:                            |           |
      | /tmp/tmux-67339/default              |                                      |           |
      | >                                    | ye53nis@ara-login01.rz.uni-jena.de's | password: |

    - now a new tmux session with name =ob-NAME= is created when using a code
      block which looks like this: =#+BEGIN_SRC tmux :socket
      ~/.tmux-local-socket-remote-machine :session NAME=
    - Commands can be sent now to the remote tmux session, BUT note that the
      output is not printed yet
    - there is a workaround for getting output back to our LabBook.org: A [[#scripts-tmux][script]]
      which allows to print the output from the tmux session in an
      =#+begin_example=-Block below the tmux block by pressing =C-c C-o= or =C-c
      C-v C-o= when the pointer is inside the tmux block.

**** =emacs-jupyter= Setup

    =Emacs-jupyter= aims to be an API for a lot of functionalities of the
    =jupyter= project. The documentation can be found on [[https://github.com/dzop/emacs-jupyter][GitHub]].

    1. For the *whole document*: connect to a running jupyter instance
       1. =M-x jupyter-server-list-kernels=
          1. set server URL, e.g. =http://localhost:8889=
          2. set websocket URL, e.g. =http://localhost:8889=
       2. two possibilities
          1. kernel already exists $\to$ list of kernels and =kernel-ID= is displayed
          2. kernel does not exist $\to$ prompt asks if you want to start one $\to$
             *yes* $\to$ type kernel you want to start, e.g. =Python 3=
    2. In the *subtree* where you want to use =jupyter-python= blocks with =org
       babel=
       1. set the =:header-args:jupyter-python :session
          /jpy:localhost#kernel:8889-ID=
       2. customize the output folder using the following org-mode variable:
          #+BEGIN_SRC  emacs-lisp
            (setq org-babel-jupyter-resource-directory "./data/exp-test/plots")
          #+END_SRC

          #+RESULTS:
          : ./data/exp-test/plots
    3. For each *individual block*, the following customizations might be useful
       1. jupyter kernels can return multiple kinds of rich output (images,
          html, ...) or scalar data (plain text, numbers, lists, ...). To force
          a plain output, use =:results scalar=. To show the output in the
          minibuffer only, use =:results silent=
       2. to change the priority of different rich outputs, use =:display=
          header argument, e.g. =:display text/plain text/html= prioritizes
          plain text over html. All supported mimetypes in default order:
          1. text/org
          2. image/svg+xml, image/jpeg, image/png
          3. text/html
          4. text/markdown
          5. text/latex
          6. text/plain
       3. We can set jupyter to output pandas DataFrames as org tables
          automatically using the source block header argument =:pandoc t=
       4. useful keybindings
          - =M-i= to open the documentation for wherever your pointer is (like
            pressing =Shift-TAB= in Jupyter notebooks)
          - =C-c C-i= to interrupt the kernel, =C-c C-r= to restart the kernel

*** Notes on archiving
**** Exporting the LabBook.org to html in a twbs style
     - I am partial to the twitter bootstrap theme of html, since I like it's
       simple design, but clear structure with a nice table of contents at the
       side → the following org mode extension supports a seemless export to
       twitter bootstrap html: https://github.com/marsmining/ox-twbs
     - when installed, the export can be triggered via the command
       =(org-twbs-export-as-html)= or via the keyboard shortcut for export =C-c
       C-e= followed by =w= for Twitter bootstrap and =h= for saving the .html
     - _Things to configure:_
       - in general, there are multiple export options:
         https://orgmode.org/manual/Export-Settings.html
       - E.g. I set 2 =#+OPTIONS= keywords at the begin of the file: =toc:4= and
         =H:4= which make sure that in my export my sidebar table of contents
         will show numbered headings till a depth of 4.
       - I configured my code blocks so that they will not be evaluated when
         exporting (I would recommend this especially if you only export for
         archiving) and that both the code block and the output will be exported
         with the keyword: =#+PROPERTY: header-args :eval never-export :exports
         both=
       - To discriminate between code blocks for different languages I gave each
         of them a distinct colour using =#+HTML_HEAD_EXTRA: <style...= (see
         above)
       - I had to configure a style for =table=, so that the
         - =display: block; overflow-x: auto;= gets the table to be restricted
           to the width of the text and if it is larger, activates scrolling
         - =white-space: nowrap;= makes it that there is no wrap in a column, so
           it might be broader, but better readable if you have scrolling anyway
     - _Things to do before exporting / Troubleshooting while exporting:_
       - when using a dark theme for you emacs, the export of the code blocks
         might show some ugly dark backgrounds from the theme. If this becomes
         an issue, change to a light theme for the export with =M-x
         (load-theme)= and choose =solarized-light=
       - only in the =data= branch you set the git tags after merging. If you
         want to show them here, execute the corresponding function in [[Git TAGs]]
       - make sure your file links work properly! I recommend referencing your
         files relatively (e.g. [ [ f ile:./data/exp-XXXXXX-test/test.png]]
         without spaces). Otherwise there will be errors in your /*Messages*/
         buffer
       - There might be errors with your code blocks
         - e.g. the export function expects you to assign a default variable to
           your functions
         - if you call a function via the =#+CALL= mechanism, it wants you to
           include two parentheses for the function, e.g. =#+CALL: test()=
       - check indentation of code blocks inside lists
       - add a =details= block around large output cells. This makes them
         expandable. I added some =#+HTML_HEAD_EXTRA: <style...= inspired by
         [[https://alhassy.github.io/org-special-block-extras/#Folded-Details][alhassy]]. That's how the =details= block looks like:
         #+begin_example
         #+begin_details

         #+end_details
         #+end_example
       - If you reference a parameter with an underscore in the name, use the
         org markdown tricks to style them like code (~==~ or =~~=), otherwise
         the part after the underscore will be rendered like a subscript:
         =under_score= vs under_score
     - _Things to do after exporting:_
       - In my workflow, the exported =LabBook.html= with the overview of all
         experiments is in the =data= folder. If you move the file, you will
         have to fix the file links for the new location, e.g. via "Find and
         replace" =M-%=:
         - if you move the org file → in the org file find =[[file:./data/= and
           replace with =[[file:./= → then export with =C-c C-e w h=
         - if you export first with =C-c C-e w h= and move the html file to
           =data= → in the html file find =./data= and replace with =.=
** Organization of git

*** remote/origin/main branch:
  - contains all the source code in folder **src/** which is used for experiments.
  - contains the **LabBook.org** template
  - contains setup- and metadata files such as **MLproject** or **conda.yaml**
  - the log contains only lasting alterations on the folders and files mentioned
    above, which are e.g. used for conducting experiments or which introduce new
    features. Day-to-day changes in code
*** remote/origin/exp### branches:
  - if an experiment is done, the code and templates will be branched out from
    *main* in an *#experiment-name* branch, ### meaning some meaningful
    descriptor.
  - all data generated during the experiment (e.g. .csv files, plots, images,
    etc), is stored in a folder with the name **data/#experiment-name**, except
    machine learning-specific data and metadata from `mlflow` runs, which are
    saved under **data/mlruns** (this allows easily comparing machine learning
    runs with different experimental settings)
  - The **LabBook.org** file is essential
    - If possible, all code is executed from inside this file (meaning analysis
      scripts or calling the code from the **scr/** directory).
    - All other steps taken during an experiment are noted down, as well as
      conclusions or my thought process while conducting the experiment
    - Provenance data, such as Metadata about the environment the code was
      executed in, the command line output of the code, and some
*** remote/origin/develop branch:
  - this is the branch I use for day to day work on features and exploration.
    All of my current activity can be followed here.
*** remote/origin/data branch:
  - contains a full cronicle of the whole research process
  - all *#experiment-name* branches are merged here. Afterwards the original
    branch is deleted and on the data branch there is a *Git tag* which shows
    the merge commit to make accessing single experiments easy.
  - the *develop* branch is merged here as well.

*** Git TAGs
**** Stable versions:
**** All tags from git:
   #+begin_src sh :results output
    git push origin --tags
    git tag -n1
   #+end_src

   #+RESULTS:
   : exp-200402-test Merge branch 'exp-200402-test' into data
   : exp-200520-unet Merge branch 'exp-310520-unet' into data
   : exp-200531-unet Merge branch 'heads/exp-310520-unet' into data
   : exp-201231-clustsim exp-201231-clustsim
   : exp-210204-unet Add exp-210204-unet LabBook part 3
   : exp-310520-unet move exp-310520-unet to data branch manually
** Organization of code
*** scripts:
*** src/
**** fluotracify/
***** imports/
***** simulations/
***** training/
***** applications/
***** doc/
    - use Sphinx
      - follow this: https://daler.github.io/sphinxdoc-test/includeme.html
      - evtl export org-mode Readme to rst via https://github.com/msnoigrs/ox-rst
      - possibly heavily use
        http://www.sphinx-doc.org/en/master/usage/extensions/autodoc.html
    - for examples sphinx-galleries could be useful
      https://sphinx-gallery.github.io/stable/getting_started.html

**** nanosimpy/
    - cloned from dwaithe with refactoring for Python 3-compatibility

** Changes in this repository (without "* Data" in this file)
*** Changes in LabBook.org (without "* Data")
**** 2022-02-19
     - Add =#+HTML_HEAD_EXTRA: <style...= for =table= to enable scrolling if the
       table overflows
**** 2021-12-16
     - Add =details= blocks, corresponding =#+HTML_HEAD_EXTRA: <style...= and
       documentation in  [[Notes on archiving]]
**** 2021-08-05
     - Rename =master= branch to =main= branch
**** 2021-04-04
     - Add =#+OPTIONS: H:4= and =#+OPTIONS: toc:4= to show up to 4 levels of
       depth in the html (twbs) export of this LabBook in the table of contents
       at the side
     - I added [[Notes on archiving]]
**** 2020-11-04
    - update "jupyter scripts" in [[Template for data entry and setup notes:]]
      for new conda environment on server (now =conda activate tf-nightly=)
**** 2020-05-31
    - extend general documentation in README
    - Add code block examples
    - extend documentation on experiment workflow
    - move setup notes from README to "Template for data entry and setup notes"
    - remove emacs-lisp code for custom tmux block functions (not relevant
      enough)
    - change named "jpt-tmux" from starting a jupyter notebook to starting
      jupyter lab. Load a conda environment instead of using Lmod's =module
      load=
**** 2020-05-07
    - extend documentation on git model
    - extend documentation on jupyter setup
**** 2020-04-22
    - added parts of README which describe the experimental process
    - added templates for system metadata, tmux, jupyter setup
    - added organization of code
**** 2020-03-30
    - set up lab book and form git repo accoring to setup by Luka Stanisic et al
*** Changes in src/fluotracify
* Data
** exp-220316-publication1
*** Setup: Jupyter on local computer
   :PROPERTIES:
   :header-args:jupyter-python: :session /jpy:localhost#8888:a37e524a-8134-4d8f-b24a-367acaf1bdd3
   :END:
   1. let's start a conda environment in the sh session local and start
      jupterlab there.
      #+begin_src sh :session local
        conda activate tf
        jupyter lab --no-browser --port=8888
      #+end_src

      #+begin_example
sh-5.1$ [I 2023-01-03 14:36:05.432 ServerApp] jupyterlab | extension was successfully linked.
[I 2023-01-03 14:36:05.738 ServerApp] nbclassic | extension was successfully linked.
[I 2023-01-03 14:36:05.805 LabApp] JupyterLab extension loaded from /home/lex/Programme/miniconda3/envs/tf/lib/python3.9/site-packages/jupyterlab
[I 2023-01-03 14:36:05.805 LabApp] JupyterLab application directory is /home/lex/Programme/miniconda3/envs/tf/share/jupyter/lab
[I 2023-01-03 14:36:05.811 ServerApp] jupyterlab | extension was successfully loaded.
[I 2023-01-03 14:36:05.823 ServerApp] nbclassic | extension was successfully loaded.
[I 2023-01-03 14:36:05.824 ServerApp] Serving notebooks from local directory: /home/lex/Programme/drmed-git
[I 2023-01-03 14:36:05.824 ServerApp] Jupyter Server 1.4.1 is running at:
[I 2023-01-03 14:36:05.824 ServerApp] http://localhost:8888/lab?token=8b657ca59261218ed47d32775aa4dd87bac4e9116158bfbd
[I 2023-01-03 14:36:05.824 ServerApp]  or http://127.0.0.1:8888/lab?token=8b657ca59261218ed47d32775aa4dd87bac4e9116158bfbd
[I 2023-01-03 14:36:05.824 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[C 2023-01-03 14:36:05.837 ServerApp]

    To access the server, open this file in a browser:
        file:///home/lex/.local/share/jupyter/runtime/jpserver-7169-open.html
    Or copy and paste one of these URLs:
        http://localhost:8888/lab?token=8b657ca59261218ed47d32775aa4dd87bac4e9116158bfbd
     or http://127.0.0.1:8888/lab?token=8b657ca59261218ed47d32775aa4dd87bac4e9116158bfbd

      #+end_example

   2. I started a Python3 kernel using =jupyter-server-list-kernels=. Then I
      added the kernel ID to the =:PROPERTIES:= drawer of this (and following)
      subtrees.

      #+begin_example
      python3           03038b73-b2b5-49ce-a1dc-21afb6247d0f   a few seconds ago    starting   0
      #+end_example

   3. Test: (~#+CALL: jp-metadata(_long='True)~)
      #+CALL: jp-metadata(_long='True)

      #+RESULTS:
      #+begin_example
        No of CPUs in system: 4
        No of CPUs the current process can use: 4
        load average: (0.5380859375, 0.63232421875, 1.0)
        os.uname():  posix.uname_result(sysname='Linux', nodename='Topialex', release='5.15.60-1-MANJARO', version='#1 SMP PREEMPT Thu Aug 11 13:14:05 UTC 2022', machine='x86_64')
        PID of process: 196700
        RAM total: 16Gi, RAM used: 8,6Gi, RAM free: 5,9Gi
        the current directory: /home/lex/Programme/drmed-git
        My disk usage:
        Filesystem      Size  Used Avail Use% Mounted on
        dev             3,9G     0  3,9G   0% /dev
        run             3,9G  1,5M  3,9G   1% /run
        /dev/sda2       167G  133G   26G  85% /
        tmpfs           3,9G  197M  3,7G   6% /dev/shm
        tmpfs           3,9G   23M  3,9G   1% /tmp
        /dev/sda1       300M  264K  300M   1% /boot/efi
        tmpfs           784M  124K  784M   1% /run/user/1000# packages in environment at /home/lex/Programme/miniconda3/envs/tf:
        #
        # Name                    Version                   Build  Channel
        _libgcc_mutex             0.1                        main
        _openmp_mutex             4.5                       1_gnu
        absl-py                   1.0.0                    pypi_0    pypi
        alembic                   1.4.1                    pypi_0    pypi
        anyio                     2.2.0            py39h06a4308_1
        argon2-cffi               20.1.0           py39h27cfd23_1
        asteval                   0.9.25                   pypi_0    pypi
        astroid                   2.9.2                    pypi_0    pypi
        astunparse                1.6.3                    pypi_0    pypi
        async_generator           1.10               pyhd3eb1b0_0
        attrs                     21.2.0             pyhd3eb1b0_0
        babel                     2.9.1              pyhd3eb1b0_0
        backcall                  0.2.0              pyhd3eb1b0_0
        bleach                    4.0.0              pyhd3eb1b0_0
        brotlipy                  0.7.0           py39h27cfd23_1003
        ca-certificates           2021.10.26           h06a4308_2
        cachetools                4.2.4                    pypi_0    pypi
        certifi                   2021.10.8        py39h06a4308_0
        cffi                      1.14.6           py39h400218f_0
        charset-normalizer        2.0.4              pyhd3eb1b0_0
        click                     8.0.3                    pypi_0    pypi
        cloudpickle               2.0.0                    pypi_0    pypi
        cryptography              36.0.0           py39h9ce1e76_0
        cycler                    0.11.0                   pypi_0    pypi
        cython                    0.29.26                  pypi_0    pypi
        databricks-cli            0.16.2                   pypi_0    pypi
        debugpy                   1.5.1            py39h295c915_0
        decorator                 5.1.0              pyhd3eb1b0_0
        defusedxml                0.7.1              pyhd3eb1b0_0
        docker                    5.0.3                    pypi_0    pypi
        entrypoints               0.3              py39h06a4308_0
        fcsfiles                  2021.6.6                 pypi_0    pypi
        flake8                    4.0.1                    pypi_0    pypi
        flask                     2.0.2                    pypi_0    pypi
        flatbuffers               2.0                      pypi_0    pypi
        focuspoint                0.1                      pypi_0    pypi
        fonttools                 4.28.5                   pypi_0    pypi
        future                    0.18.2                   pypi_0    pypi
        gast                      0.4.0                    pypi_0    pypi
        gitdb                     4.0.9                    pypi_0    pypi
        gitpython                 3.1.24                   pypi_0    pypi
        google-auth               2.3.3                    pypi_0    pypi
        google-auth-oauthlib      0.4.6                    pypi_0    pypi
        google-pasta              0.2.0                    pypi_0    pypi
        greenlet                  1.1.2                    pypi_0    pypi
        grpcio                    1.43.0                   pypi_0    pypi
        gunicorn                  20.1.0                   pypi_0    pypi
        h5py                      3.6.0                    pypi_0    pypi
        idna                      3.3                pyhd3eb1b0_0
        importlib-metadata        4.8.2            py39h06a4308_0
        importlib_metadata        4.8.2                hd3eb1b0_0
        ipykernel                 6.4.1            py39h06a4308_1
        ipython                   7.29.0           py39hb070fc8_0
        ipython_genutils          0.2.0              pyhd3eb1b0_1
        isort                     5.10.1                   pypi_0    pypi
        itsdangerous              2.0.1                    pypi_0    pypi
        jedi                      0.18.0           py39h06a4308_1
        jinja2                    3.0.2              pyhd3eb1b0_0
        joblib                    1.1.0                    pypi_0    pypi
        json5                     0.9.6              pyhd3eb1b0_0
        jsonschema                3.2.0              pyhd3eb1b0_2
        jupyter_client            7.1.0              pyhd3eb1b0_0
        jupyter_core              4.9.1            py39h06a4308_0
        jupyter_server            1.4.1            py39h06a4308_0
        jupyterlab                3.2.1              pyhd3eb1b0_1
        jupyterlab_pygments       0.1.2                      py_0
        jupyterlab_server         2.8.2              pyhd3eb1b0_0
        keras                     2.7.0                    pypi_0    pypi
        keras-preprocessing       1.1.2                    pypi_0    pypi
        kiwisolver                1.3.2                    pypi_0    pypi
        lazy-object-proxy         1.7.1                    pypi_0    pypi
        ld_impl_linux-64          2.35.1               h7274673_9
        libclang                  12.0.0                   pypi_0    pypi
        libffi                    3.3                  he6710b0_2
        libgcc-ng                 9.3.0               h5101ec6_17
        libgomp                   9.3.0               h5101ec6_17
        libsodium                 1.0.18               h7b6447c_0
        libstdcxx-ng              9.3.0               hd4cf53a_17
        lmfit                     1.0.3                    pypi_0    pypi
        mako                      1.1.6                    pypi_0    pypi
        markdown                  3.3.6                    pypi_0    pypi
        markupsafe                2.0.1            py39h27cfd23_0
        matplotlib                3.5.1                    pypi_0    pypi
        matplotlib-inline         0.1.2              pyhd3eb1b0_2
        mccabe                    0.6.1                    pypi_0    pypi
        mistune                   0.8.4           py39h27cfd23_1000
        mlflow                    1.22.0                   pypi_0    pypi
        multipletau               0.3.3                    pypi_0    pypi
        mypy                      0.930                    pypi_0    pypi
        mypy-extensions           0.4.3                    pypi_0    pypi
        nbclassic                 0.2.6              pyhd3eb1b0_0
        nbclient                  0.5.3              pyhd3eb1b0_0
        nbconvert                 6.1.0            py39h06a4308_0
        nbformat                  5.1.3              pyhd3eb1b0_0
        ncurses                   6.3                  h7f8727e_2
        nest-asyncio              1.5.1              pyhd3eb1b0_0
        nodeenv                   1.6.0                    pypi_0    pypi
        notebook                  6.4.6            py39h06a4308_0
        numpy                     1.21.5                   pypi_0    pypi
        oauthlib                  3.1.1                    pypi_0    pypi
        openssl                   1.1.1l               h7f8727e_0
        opt-einsum                3.3.0                    pypi_0    pypi
        packaging                 21.3               pyhd3eb1b0_0
        pandas                    1.3.5                    pypi_0    pypi
        pandocfilters             1.4.3            py39h06a4308_1
        parso                     0.8.2              pyhd3eb1b0_0
        pexpect                   4.8.0              pyhd3eb1b0_3
        pickleshare               0.7.5           pyhd3eb1b0_1003
        pillow                    8.4.0                    pypi_0    pypi
        pip                       21.2.4           py39h06a4308_0
        platformdirs              2.4.1                    pypi_0    pypi
        prometheus-flask-exporter 0.18.7                   pypi_0    pypi
        prometheus_client         0.12.0             pyhd3eb1b0_0
        prompt-toolkit            3.0.20             pyhd3eb1b0_0
        protobuf                  3.19.1                   pypi_0    pypi
        ptyprocess                0.7.0              pyhd3eb1b0_2
        pyasn1                    0.4.8                    pypi_0    pypi
        pyasn1-modules            0.2.8                    pypi_0    pypi
        pycodestyle               2.8.0                    pypi_0    pypi
        pycparser                 2.21               pyhd3eb1b0_0
        pydot                     1.4.2                    pypi_0    pypi
        pyflakes                  2.4.0                    pypi_0    pypi
        pygments                  2.10.0             pyhd3eb1b0_0
        pylint                    2.12.2                   pypi_0    pypi
        pyopenssl                 21.0.0             pyhd3eb1b0_1
        pyparsing                 3.0.4              pyhd3eb1b0_0
        pyright                   0.0.13                   pypi_0    pypi
        pyrsistent                0.18.0           py39heee7806_0
        pysocks                   1.7.1            py39h06a4308_0
        python                    3.9.7                h12debd9_1
        python-dateutil           2.8.2              pyhd3eb1b0_0
        python-editor             1.0.4                    pypi_0    pypi
        pytz                      2021.3             pyhd3eb1b0_0
        pyyaml                    6.0                      pypi_0    pypi
        pyzmq                     22.3.0           py39h295c915_2
        querystring-parser        1.2.4                    pypi_0    pypi
        readline                  8.1                  h27cfd23_0
        requests                  2.26.0             pyhd3eb1b0_0
        requests-oauthlib         1.3.0                    pypi_0    pypi
        rsa                       4.8                      pypi_0    pypi
        scikit-learn              1.0.2                    pypi_0    pypi
        scipy                     1.7.3                    pypi_0    pypi
        seaborn                   0.11.2                   pypi_0    pypi
        send2trash                1.8.0              pyhd3eb1b0_1
        setuptools                58.0.4           py39h06a4308_0
        six                       1.16.0             pyhd3eb1b0_0
        smmap                     5.0.0                    pypi_0    pypi
        sniffio                   1.2.0            py39h06a4308_1
        sqlalchemy                1.4.29                   pypi_0    pypi
        sqlite                    3.37.0               hc218d9a_0
        sqlparse                  0.4.2                    pypi_0    pypi
        tabulate                  0.8.9                    pypi_0    pypi
        tensorboard               2.7.0                    pypi_0    pypi
        tensorboard-data-server   0.6.1                    pypi_0    pypi
        tensorboard-plugin-wit    1.8.0                    pypi_0    pypi
        tensorflow                2.7.0                    pypi_0    pypi
        tensorflow-estimator      2.7.0                    pypi_0    pypi
        tensorflow-io-gcs-filesystem 0.23.1                   pypi_0    pypi
        termcolor                 1.1.0                    pypi_0    pypi
        terminado                 0.9.4            py39h06a4308_0
        testpath                  0.5.0              pyhd3eb1b0_0
        threadpoolctl             3.0.0                    pypi_0    pypi
        tk                        8.6.11               h1ccaba5_0
        toml                      0.10.2                   pypi_0    pypi
        tomli                     2.0.0                    pypi_0    pypi
        tornado                   6.1              py39h27cfd23_0
        traitlets                 5.1.1              pyhd3eb1b0_0
        typing-extensions         4.0.1                    pypi_0    pypi
        tzdata                    2021e                hda174b7_0
        uncertainties             3.1.6                    pypi_0    pypi
        urllib3                   1.26.7             pyhd3eb1b0_0
        wcwidth                   0.2.5              pyhd3eb1b0_0
        webencodings              0.5.1            py39h06a4308_1
        websocket-client          1.2.3                    pypi_0    pypi
        werkzeug                  2.0.2                    pypi_0    pypi
        wheel                     0.37.0             pyhd3eb1b0_1
        wrapt                     1.13.3                   pypi_0    pypi
        xz                        5.2.5                h7b6447c_0
        zeromq                    4.3.4                h2531618_0
        zipp                      3.6.0              pyhd3eb1b0_0
        zlib                      1.2.11               h7f8727e_4

        Note: you may need to restart the kernel to use updated packages.
        {'SHELL': '/bin/bash',
         'SESSION_MANAGER': 'local/Topialex:@/tmp/.ICE-unix/986,unix/Topialex:/tmp/.ICE-unix/986',
         'XDG_CONFIG_DIRS': '/home/lex/.config/kdedefaults:/etc/xdg',
         'XDG_SESSION_PATH': '/org/freedesktop/DisplayManager/Session1',
         'CONDA_EXE': '/home/lex/Programme/miniconda3/bin/conda',
         '_CE_M': '',
         'LANGUAGE': 'en_GB',
         'TERMCAP': '',
         'LC_ADDRESS': 'de_DE.UTF-8',
         'LC_NAME': 'de_DE.UTF-8',
         'INSIDE_EMACS': '28.1,comint',
         'DESKTOP_SESSION': 'plasma',
         'LC_MONETARY': 'de_DE.UTF-8',
         'GTK_RC_FILES': '/etc/gtk/gtkrc:/home/lex/.gtkrc:/home/lex/.config/gtkrc',
         'XCURSOR_SIZE': '24',
         'GTK_MODULES': 'canberra-gtk-module',
         'XDG_SEAT': 'seat0',
         'PWD': '/home/lex/Programme/drmed-git',
         'LOGNAME': 'lex',
         'XDG_SESSION_DESKTOP': 'KDE',
         'XDG_SESSION_TYPE': 'x11',
         'CONDA_PREFIX': '/home/lex/Programme/miniconda3/envs/tf',
         'DSSI_PATH': '/home/lex/.dssi:/usr/lib/dssi:/usr/local/lib/dssi',
         'SYSTEMD_EXEC_PID': '877',
         'XAUTHORITY': '/home/lex/.Xauthority',
         'MOTD_SHOWN': 'pam',
         'GTK2_RC_FILES': '/etc/gtk-2.0/gtkrc:/home/lex/.gtkrc-2.0:/home/lex/.config/gtkrc-2.0',
         'HOME': '/home/lex',
         'LANG': 'de_DE.UTF-8',
         'LC_PAPER': 'de_DE.UTF-8',
         'VST_PATH': '/home/lex/.vst:/usr/lib/vst:/usr/local/lib/vst',
         'XDG_CURRENT_DESKTOP': 'KDE',
         'COLUMNS': '163',
         'CONDA_PROMPT_MODIFIER': '',
         'XDG_SEAT_PATH': '/org/freedesktop/DisplayManager/Seat0',
         'KDE_SESSION_UID': '1000',
         'XDG_SESSION_CLASS': 'user',
         'LC_IDENTIFICATION': 'de_DE.UTF-8',
         'TERM': 'xterm-color',
         '_CE_CONDA': '',
         'USER': 'lex',
         'CONDA_SHLVL': '1',
         'KDE_SESSION_VERSION': '5',
         'PAM_KWALLET5_LOGIN': '/run/user/1000/kwallet5.socket',
         'DISPLAY': ':0',
         'SHLVL': '2',
         'LC_TELEPHONE': 'de_DE.UTF-8',
         'LC_MEASUREMENT': 'de_DE.UTF-8',
         'XDG_VTNR': '1',
         'XDG_SESSION_ID': '2',
         'QT_LINUX_ACCESSIBILITY_ALWAYS_ON': '1',
         'CONDA_PYTHON_EXE': '/home/lex/Programme/miniconda3/bin/python',
         'MOZ_PLUGIN_PATH': '/usr/lib/mozilla/plugins',
         'XDG_RUNTIME_DIR': '/run/user/1000',
         'CONDA_DEFAULT_ENV': 'tf',
         'LC_TIME': 'de_DE.UTF-8',
         'QT_AUTO_SCREEN_SCALE_FACTOR': '0',
         'XCURSOR_THEME': 'breeze_cursors',
         'XDG_DATA_DIRS': '/home/lex/.local/share/flatpak/exports/share:/var/lib/flatpak/exports/share:/usr/local/share:/usr/share:/var/lib/snapd/desktop',
         'KDE_FULL_SESSION': 'true',
         'BROWSER': 'vivaldi-stable',
         'PATH': '/home/lex/Programme/miniconda3/envs/tf/bin:/home/lex/Programme/miniconda3/condabin:/home/lex/.local/bin:/bin:/usr/bin:/usr/local/bin:/usr/local/sbin:/usr/lib/jvm/default/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl:/var/lib/snapd/snap/bin',
         'DBUS_SESSION_BUS_ADDRESS': 'unix:path=/run/user/1000/bus',
         'LV2_PATH': '/home/lex/.lv2:/usr/lib/lv2:/usr/local/lib/lv2',
         'KDE_APPLICATIONS_AS_SCOPE': '1',
         'MAIL': '/var/spool/mail/lex',
         'LC_NUMERIC': 'de_DE.UTF-8',
         'LADSPA_PATH': '/home/lex/.ladspa:/usr/lib/ladspa:/usr/local/lib/ladspa',
         'CADENCE_AUTO_STARTED': 'true',
         '_': '/home/lex/Programme/miniconda3/envs/tf/bin/jupyter',
         'PYDEVD_USE_FRAME_EVAL': 'NO',
         'JPY_PARENT_PID': '156430',
         'CLICOLOR': '1',
         'PAGER': 'cat',
         'GIT_PAGER': 'cat',
         'MPLBACKEND': 'module://matplotlib_inline.backend_inline'}
      #+end_example

   4. Branch out git branch =exp-220316-publication1= from =main= (done via
      magit) and make sure you are on the correct branch

      #+begin_src sh :session local2
        cd /home/lex/Programme/drmed-git
        git status
      #+end_src

      #+RESULTS:
      #+begin_example
        sh-5.1$ cd /home/lex/Programme/drmed-git
        sh-5.1$ git status
        On branch exp-220316-publication1
        Your branch is up to date with 'origin/exp-220316-publication1'.
      #+end_example

   5. Create experiment folder including the plot folder for jupyter plots
      #+begin_src sh :session local2
        mkdir -p ./data/exp-220316-publication1/jupyter
      #+end_src

      #+RESULTS:

   6. set output directory for matplotlib plots in jupyter
      #+NAME: jupyter-set-output-directory
      #+begin_src emacs-lisp
        (setq org-babel-jupyter-resource-directory "./data/exp-220316-publication1/jupyter")
      #+end_src

      #+RESULTS: jupyter-set-output-directory
      : ./data/exp-220316-publication1/jupyter

   7.
*** Setup: Jupyter node on HPC
    :PROPERTIES:
    :header-args:jupyter-python: :session /jpy:localhost#8889:a37e524a-8134-4d8f-b24a-367acaf1bdd3
    :END:
    1. Set up tmux (if we haven't done that before) (=#+CALL:
       setup-tmux[:session local]=)
       #+CALL: setup-tmux[:session local2]

       #+RESULTS:
       |         |                                        |           |
       | sh-5.1$ | ye53nis@ara-login01.rz.uni-jena.de's | password: |
       | >       | ye53nis@ara-login01.rz.uni-jena.de's | password: |

    2. Request compute node
       #+BEGIN_SRC tmux :socket ~/.tmux-local-socket-remote-machine :session jpmux
         cd /
         srun -p b_standard --time=7-10:00:00 --ntasks-per-node=24 --mem-per-cpu=2000 --pty bash
       #+END_SRC


    3. Start Jupyter Lab (~#+CALL: jpt-tmux[:session jpmux]~)
       #+CALL: jpt-tmux[:session jpmux]

       #+RESULTS:
       #+begin_example
       (tf) [ye53nis@node117 /]$ jupyter lab --no-browser --port=$PORT
         [I 2023-01-03 22:14:33.399 ServerApp] jupyterlab | extension was successfully linked.
         [I 2023-01-03 22:14:40.846 ServerApp] nbclassic | extension was successfully linked.
         [I 2023-01-03 22:14:41.330 ServerApp] nbclassic | extension was successfully loaded.
         [I 2023-01-03 22:14:41.332 LabApp] JupyterLab extension loaded from /home/ye53nis/.conda/envs/tf/lib/python3.9/site-packages/jupyterlab
         [I 2023-01-03 22:14:41.332 LabApp] JupyterLab application directory is /home/ye53nis/.conda/envs/tf/share/jupyter/lab
         [I 2023-01-03 22:14:41.340 ServerApp] jupyterlab | extension was successfully loaded.
         [I 2023-01-03 22:14:41.342 ServerApp] Serving notebooks from local directory: /
         [I 2023-01-03 22:14:41.342 ServerApp] Jupyter Server 1.13.5 is running at:
         [I 2023-01-03 22:14:41.342 ServerApp] http://localhost:8889/lab?token=2ff8ed3f281a95c2bda81a0c453699c478ee1fd2e52e8bab
         [I 2023-01-03 22:14:41.342 ServerApp]  or http://127.0.0.1:8889/lab?token=2ff8ed3f281a95c2bda81a0c453699c478ee1fd2e52e8bab
         [I 2023-01-03 22:14:41.342 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
         [C 2023-01-03 22:14:41.456 ServerApp]

             To access the server, open this file in a browser:
                 file:///home/ye53nis/.local/share/jupyter/runtime/jpserver-158816-open.html
             Or copy and paste one of these URLs:
                 http://localhost:8889/lab?token=2ff8ed3f281a95c2bda81a0c453699c478ee1fd2e52e8bab
              or http://127.0.0.1:8889/lab?token=2ff8ed3f281a95c2bda81a0c453699c478ee1fd2e52e8bab
       #+end_example

    4. Create SSH Tunnel for jupyter lab to the local computer (e.g. ~#+CALL:
       ssh-tunnel(port="8889", node="node160")~)
       #+CALL: ssh-tunnel(port="8889", node="node117")
       #+RESULTS:
       |                   |           |                                        |           |   |          |      |      |             |
       | sh-5.1$           | sh-5.1$   | ye53nis@ara-login01.rz.uni-jena.de's | password: |   |          |      |      |             |
       | ye53nis@node117's | password: |                                        |           |   |          |      |      |             |
       | Last              | login:    | Tue                                    | Jan       | 3 | 22:15:58 | 2023 | from | login01.ara |

    5. I started a Python3 kernel using =jupyter-server-list-kernels=. Then I
       added the kernel ID to the =:PROPERTIES:= drawer of this (and following)
       subtrees.
       #+begin_example
       python3           c4f3acce-60c4-489d-922c-407da110fd6a   a few seconds ago    idle       1
       #+end_example

    6. Test (~#+CALL: jp-metadata(_long='True)~) and record metadata:
       #+CALL: jp-metadata(_long='True)

       #+RESULTS:
       #+begin_example
         No of CPUs in system: 48
         No of CPUs the current process can use: 24
         load average: (1658.36, 1661.86, 1648.89)
         os.uname():  posix.uname_result(sysname='Linux', nodename='node095', release='3.10.0-957.1.3.el7.x86_64', version='#1 SMP Thu Nov 29 14:49:43 UTC 2018', machine='x86_64')
         PID of process: 189118
         RAM total: 137G, RAM used: 18G, RAM free: 103G
         the current directory: /
         My disk usage:
         Filesystem           Size  Used Avail Use% Mounted on
         /dev/sda1             50G  5.0G   46G  10% /
         devtmpfs              63G     0   63G   0% /dev
         tmpfs                 63G  199M   63G   1% /dev/shm
         tmpfs                 63G   27M   63G   1% /run
         tmpfs                 63G     0   63G   0% /sys/fs/cgroup
         nfs01-ib:/home        80T   63T   18T  79% /home
         nfs03-ib:/pool/work  100T   72T   29T  72% /nfsdata
         nfs01-ib:/cluster    2.0T  496G  1.6T  25% /cluster
         /dev/sda3            6.0G  429M  5.6G   7% /var
         /dev/sda6            169G  3.8G  165G   3% /local
         /dev/sda5            2.0G  119M  1.9G   6% /tmp
         beegfs_nodev         524T  456T   69T  87% /beegfs
         tmpfs                 13G     0   13G   0% /run/user/67339# packages in environment at /home/ye53nis/.conda/envs/tf:
         #
         # Name                    Version                   Build  Channel
         _libgcc_mutex             0.1                        main
         _openmp_mutex             5.1                       1_gnu
         absl-py                   1.0.0                    pypi_0    pypi
         alembic                   1.7.7                    pypi_0    pypi
         anyio                     3.5.0            py39h06a4308_0
         argon2-cffi               21.3.0             pyhd3eb1b0_0
         argon2-cffi-bindings      21.2.0           py39h7f8727e_0
         asteval                   0.9.26                   pypi_0    pypi
         asttokens                 2.0.5              pyhd3eb1b0_0
         astunparse                1.6.3                    pypi_0    pypi
         attrs                     21.4.0             pyhd3eb1b0_0
         babel                     2.9.1              pyhd3eb1b0_0
         backcall                  0.2.0              pyhd3eb1b0_0
         beautifulsoup4            4.11.1           py39h06a4308_0
         bleach                    4.1.0              pyhd3eb1b0_0
         brotlipy                  0.7.0           py39h27cfd23_1003
         ca-certificates           2022.4.26            h06a4308_0
         cachetools                5.1.0                    pypi_0    pypi
         certifi                   2021.10.8        py39h06a4308_2
         cffi                      1.15.0           py39hd667e15_1
         charset-normalizer        2.0.4              pyhd3eb1b0_0
         click                     8.1.3                    pypi_0    pypi
         cloudpickle               2.0.0                    pypi_0    pypi
         cryptography              37.0.1           py39h9ce1e76_0
         cycler                    0.11.0                   pypi_0    pypi
         cython                    0.29.30                  pypi_0    pypi
         databricks-cli            0.16.6                   pypi_0    pypi
         debugpy                   1.5.1            py39h295c915_0
         decorator                 5.1.1              pyhd3eb1b0_0
         defusedxml                0.7.1              pyhd3eb1b0_0
         docker                    5.0.3                    pypi_0    pypi
         entrypoints               0.4              py39h06a4308_0
         executing                 0.8.3              pyhd3eb1b0_0
         fcsfiles                  2022.2.2                 pypi_0    pypi
         flask                     2.1.2                    pypi_0    pypi
         flatbuffers               1.12                     pypi_0    pypi
         fonttools                 4.33.3                   pypi_0    pypi
         future                    0.18.2                   pypi_0    pypi
         gast                      0.4.0                    pypi_0    pypi
         gitdb                     4.0.9                    pypi_0    pypi
         gitpython                 3.1.27                   pypi_0    pypi
         google-auth               2.6.6                    pypi_0    pypi
         google-auth-oauthlib      0.4.6                    pypi_0    pypi
         google-pasta              0.2.0                    pypi_0    pypi
         greenlet                  1.1.2                    pypi_0    pypi
         grpcio                    1.46.1                   pypi_0    pypi
         gunicorn                  20.1.0                   pypi_0    pypi
         h5py                      3.6.0                    pypi_0    pypi
         idna                      3.3                pyhd3eb1b0_0
         importlib-metadata        4.11.3                   pypi_0    pypi
         ipykernel                 6.9.1            py39h06a4308_0
         ipython                   8.3.0            py39h06a4308_0
         ipython_genutils          0.2.0              pyhd3eb1b0_1
         itsdangerous              2.1.2                    pypi_0    pypi
         jedi                      0.18.1           py39h06a4308_1
         jinja2                    3.0.3              pyhd3eb1b0_0
         joblib                    1.1.0                    pypi_0    pypi
         json5                     0.9.6              pyhd3eb1b0_0
         jsonschema                4.4.0            py39h06a4308_0
         jupyter_client            7.2.2            py39h06a4308_0
         jupyter_core              4.10.0           py39h06a4308_0
         jupyter_server            1.13.5             pyhd3eb1b0_0
         jupyterlab                3.3.2              pyhd3eb1b0_0
         jupyterlab_pygments       0.1.2                      py_0
         jupyterlab_server         2.12.0           py39h06a4308_0
         keras                     2.9.0                    pypi_0    pypi
         keras-preprocessing       1.1.2                    pypi_0    pypi
         kiwisolver                1.4.2                    pypi_0    pypi
         ld_impl_linux-64          2.38                 h1181459_0
         libclang                  14.0.1                   pypi_0    pypi
         libffi                    3.3                  he6710b0_2
         libgcc-ng                 11.2.0               h1234567_0
         libgomp                   11.2.0               h1234567_0
         libsodium                 1.0.18               h7b6447c_0
         libstdcxx-ng              11.2.0               h1234567_0
         lmfit                     1.0.3                    pypi_0    pypi
         mako                      1.2.0                    pypi_0    pypi
         markdown                  3.3.7                    pypi_0    pypi
         markupsafe                2.0.1            py39h27cfd23_0
         matplotlib                3.5.2                    pypi_0    pypi
         matplotlib-inline         0.1.2              pyhd3eb1b0_2
         mistune                   0.8.4           py39h27cfd23_1000
         mlflow                    1.26.0                   pypi_0    pypi
         multipletau               0.3.3                    pypi_0    pypi
         nbclassic                 0.3.5              pyhd3eb1b0_0
         nbclient                  0.5.13           py39h06a4308_0
         nbconvert                 6.4.4            py39h06a4308_0
         nbformat                  5.3.0            py39h06a4308_0
         ncurses                   6.3                  h7f8727e_2
         nest-asyncio              1.5.5            py39h06a4308_0
         notebook                  6.4.11           py39h06a4308_0
         numpy                     1.22.3                   pypi_0    pypi
         oauthlib                  3.2.0                    pypi_0    pypi
         openssl                   1.1.1o               h7f8727e_0
         opt-einsum                3.3.0                    pypi_0    pypi
         packaging                 21.3               pyhd3eb1b0_0
         pandas                    1.4.2                    pypi_0    pypi
         pandocfilters             1.5.0              pyhd3eb1b0_0
         parso                     0.8.3              pyhd3eb1b0_0
         pexpect                   4.8.0              pyhd3eb1b0_3
         pickleshare               0.7.5           pyhd3eb1b0_1003
         pillow                    9.1.1                    pypi_0    pypi
         pip                       21.2.4           py39h06a4308_0
         prometheus-flask-exporter 0.20.1                   pypi_0    pypi
         prometheus_client         0.13.1             pyhd3eb1b0_0
         prompt-toolkit            3.0.20             pyhd3eb1b0_0
         protobuf                  3.20.1                   pypi_0    pypi
         ptyprocess                0.7.0              pyhd3eb1b0_2
         pure_eval                 0.2.2              pyhd3eb1b0_0
         pyasn1                    0.4.8                    pypi_0    pypi
         pyasn1-modules            0.2.8                    pypi_0    pypi
         pycparser                 2.21               pyhd3eb1b0_0
         pygments                  2.11.2             pyhd3eb1b0_0
         pyjwt                     2.4.0                    pypi_0    pypi
         pyopenssl                 22.0.0             pyhd3eb1b0_0
         pyparsing                 3.0.4              pyhd3eb1b0_0
         pyrsistent                0.18.0           py39heee7806_0
         pysocks                   1.7.1            py39h06a4308_0
         python                    3.9.12               h12debd9_0
         python-dateutil           2.8.2              pyhd3eb1b0_0
         python-fastjsonschema     2.15.1             pyhd3eb1b0_0
         pytz                      2021.3             pyhd3eb1b0_0
         pyyaml                    6.0                      pypi_0    pypi
         pyzmq                     22.3.0           py39h295c915_2
         querystring-parser        1.2.4                    pypi_0    pypi
         readline                  8.1.2                h7f8727e_1
         requests                  2.27.1             pyhd3eb1b0_0
         requests-oauthlib         1.3.1                    pypi_0    pypi
         rsa                       4.8                      pypi_0    pypi
         scikit-learn              1.1.0                    pypi_0    pypi
         scipy                     1.8.1                    pypi_0    pypi
         seaborn                   0.11.2                   pypi_0    pypi
         send2trash                1.8.0              pyhd3eb1b0_1
         setuptools                61.2.0           py39h06a4308_0
         six                       1.16.0             pyhd3eb1b0_1
         smmap                     5.0.0                    pypi_0    pypi
         sniffio                   1.2.0            py39h06a4308_1
         soupsieve                 2.3.1              pyhd3eb1b0_0
         sqlalchemy                1.4.36                   pypi_0    pypi
         sqlite                    3.38.3               hc218d9a_0
         sqlparse                  0.4.2                    pypi_0    pypi
         stack_data                0.2.0              pyhd3eb1b0_0
         tabulate                  0.8.9                    pypi_0    pypi
         tensorboard               2.9.0                    pypi_0    pypi
         tensorboard-data-server   0.6.1                    pypi_0    pypi
         tensorboard-plugin-wit    1.8.1                    pypi_0    pypi
         tensorflow                2.9.0                    pypi_0    pypi
         tensorflow-estimator      2.9.0                    pypi_0    pypi
         tensorflow-io-gcs-filesystem 0.26.0                   pypi_0    pypi
         termcolor                 1.1.0                    pypi_0    pypi
         terminado                 0.13.1           py39h06a4308_0
         testpath                  0.5.0              pyhd3eb1b0_0
         threadpoolctl             3.1.0                    pypi_0    pypi
         tk                        8.6.11               h1ccaba5_1
         tornado                   6.1              py39h27cfd23_0
         traitlets                 5.1.1              pyhd3eb1b0_0
         typing-extensions         4.1.1                hd3eb1b0_0
         typing_extensions         4.1.1              pyh06a4308_0
         tzdata                    2022a                hda174b7_0
         uncertainties             3.1.6                    pypi_0    pypi
         urllib3                   1.26.9           py39h06a4308_0
         wcwidth                   0.2.5              pyhd3eb1b0_0
         webencodings              0.5.1            py39h06a4308_1
         websocket-client          0.58.0           py39h06a4308_4
         werkzeug                  2.1.2                    pypi_0    pypi
         wheel                     0.37.1             pyhd3eb1b0_0
         wrapt                     1.14.1                   pypi_0    pypi
         xz                        5.2.5                h7f8727e_1
         zeromq                    4.3.4                h2531618_0
         zipp                      3.8.0                    pypi_0    pypi
         zlib                      1.2.12               h7f8727e_2

         Note: you may need to restart the kernel to use updated packages.
         {'SLURM_CHECKPOINT_IMAGE_DIR': '/var/slurm/checkpoint',
          'SLURM_NODELIST': 'node095',
          'SLURM_JOB_NAME': 'bash',
          'XDG_SESSION_ID': '135386',
          'SLURMD_NODENAME': 'node095',
          'SLURM_TOPOLOGY_ADDR': 'node095',
          'SLURM_NTASKS_PER_NODE': '24',
          'HOSTNAME': 'login01',
          'SLURM_PRIO_PROCESS': '0',
          'SLURM_SRUN_COMM_PORT': '43002',
          'SHELL': '/bin/bash',
          'TERM': 'xterm-color',
          'SLURM_JOB_QOS': 'qstand',
          'SLURM_PTY_WIN_ROW': '48',
          'HISTSIZE': '1000',
          'TMPDIR': '/tmp',
          'SLURM_TOPOLOGY_ADDR_PATTERN': 'node',
          'SSH_CLIENT': '10.231.185.64 42170 22',
          'CONDA_SHLVL': '2',
          'CONDA_PROMPT_MODIFIER': '(tf) ',
          'WINDOWID': '0',
          'QTDIR': '/usr/lib64/qt-3.3',
          'QTINC': '/usr/lib64/qt-3.3/include',
          'SSH_TTY': '/dev/pts/19',
          'NO_PROXY': 'localhost,127.0.0.0/8,.uni-jena.de,141.35.0.0/16,10.0.0.0/8,192.168.0.0/16,172.0.0.0/8,fe80::/7,2001:638:1558::/24,vmaster,node001',
          'QT_GRAPHICSSYSTEM_CHECKED': '1',
          'SLURM_NNODES': '1',
          'USER': 'ye53nis',
          'http_proxy': 'http://internet4nzm.rz.uni-jena.de:3128',
          'LS_COLORS': 'rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=01;05;37;41:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.axv=01;35:*.anx=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=01;36:*.au=01;36:*.flac=01;36:*.mid=01;36:*.midi=01;36:*.mka=01;36:*.mp3=01;36:*.mpc=01;36:*.ogg=01;36:*.ra=01;36:*.wav=01;36:*.axa=01;36:*.oga=01;36:*.spx=01;36:*.xspf=01;36:',
          'CONDA_EXE': '/cluster/miniconda3/bin/conda',
          'SLURM_STEP_NUM_NODES': '1',
          'SLURM_JOBID': '1679082',
          'SRUN_DEBUG': '3',
          'FTP_PROXY': 'http://internet4nzm.rz.uni-jena.de:3128',
          'ftp_proxy': 'http://internet4nzm.rz.uni-jena.de:3128',
          'SLURM_NTASKS': '24',
          'SLURM_LAUNCH_NODE_IPADDR': '192.168.192.5',
          'SLURM_STEP_ID': '0',
          'TMUX': '/tmp/tmux-67339/default,14861,2',
          '_CE_CONDA': '',
          'CONDA_PREFIX_1': '/cluster/miniconda3',
          'SLURM_STEP_LAUNCHER_PORT': '43002',
          'SLURM_TASKS_PER_NODE': '24',
          'MAIL': '/var/spool/mail/ye53nis',
          'PATH': '/home/ye53nis/.conda/envs/tf/bin:/home/lex/Programme/miniconda3/envs/tf/bin:/home/lex/Programme/miniconda3/condabin:/home/lex/.local/bin:/bin:/usr/bin:/usr/local/bin:/usr/local/sbin:/usr/lib/jvm/default/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl:/var/lib/snapd/snap/bin:/home/lex/Programme/miniconda3/bin:/usr/sbin:/home/ye53nis/.local/bin:/home/ye53nis/bin',
          'SLURM_WORKING_CLUSTER': 'hpc:192.168.192.1:6817:8448',
          'SLURM_JOB_ID': '1679082',
          'CONDA_PREFIX': '/home/ye53nis/.conda/envs/tf',
          'SLURM_JOB_USER': 'ye53nis',
          'SLURM_STEPID': '0',
          'PWD': '/',
          'SLURM_SRUN_COMM_HOST': '192.168.192.5',
          'LANG': 'en_US.UTF-8',
          'SLURM_PTY_WIN_COL': '236',
          'SLURM_UMASK': '0022',
          'MODULEPATH': '/usr/share/Modules/modulefiles:/etc/modulefiles:/cluster/modulefiles',
          'SLURM_JOB_UID': '67339',
          'LOADEDMODULES': '',
          'SLURM_NODEID': '0',
          'TMUX_PANE': '%2',
          'SLURM_SUBMIT_DIR': '/',
          'SLURM_TASK_PID': '186350',
          'SLURM_NPROCS': '24',
          'SLURM_CPUS_ON_NODE': '24',
          'SLURM_DISTRIBUTION': 'block',
          'HTTPS_PROXY': 'http://internet4nzm.rz.uni-jena.de:3128',
          'https_proxy': 'http://internet4nzm.rz.uni-jena.de:3128',
          'SLURM_PROCID': '0',
          'HISTCONTROL': 'ignoredups',
          '_CE_M': '',
          'SLURM_JOB_NODELIST': 'node095',
          'SLURM_PTY_PORT': '41832',
          'HOME': '/home/ye53nis',
          'SHLVL': '3',
          'SLURM_LOCALID': '0',
          'SLURM_JOB_GID': '13280',
          'SLURM_JOB_CPUS_PER_NODE': '24',
          'SLURM_CLUSTER_NAME': 'hpc',
          'no_proxy': 'localhost,127.0.0.0/8,.uni-jena.de,141.35.0.0/16,10.0.0.0/8,192.168.0.0/16,172.0.0.0/8,fe80::/7,2001:638:1558::/24,vmaster,node001',
          'SLURM_GTIDS': '0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23',
          'SLURM_SUBMIT_HOST': 'login01',
          'HTTP_PROXY': 'http://internet4nzm.rz.uni-jena.de:3128',
         'SLURM_JOB_PARTITION': 'b_standard',
         'MATHEMATICA_HOME': '/cluster/apps/mathematica/12.3',
         'CONDA_PYTHON_EXE': '/cluster/miniconda3/bin/python',
         'LOGNAME': 'ye53nis',
         'SLURM_STEP_NUM_TASKS': '24',
         'QTLIB': '/usr/lib64/qt-3.3/lib',
         'SLURM_JOB_ACCOUNT': 'iaob',
         'SLURM_JOB_NUM_NODES': '1',
         'MODULESHOME': '/usr/share/Modules',
         'CONDA_DEFAULT_ENV': 'tf',
         'LESSOPEN': '||/usr/bin/lesspipe.sh %s',
         'SLURM_STEP_TASKS_PER_NODE': '24',
         'PORT': '8889',
         'SLURM_STEP_NODELIST': 'node095',
         'DISPLAY': ':0',
         'XDG_RUNTIME_DIR': '',
         'XAUTHORITY': '/home/lex/.Xauthority',
         'BASH_FUNC_module()': '() {  eval `/usr/bin/modulecmd bash $*`\n}',
         '_': '/home/ye53nis/.conda/envs/tf/bin/jupyter',
         'PYDEVD_USE_FRAME_EVAL': 'NO',
         'JPY_PARENT_PID': '187359',
         'CLICOLOR': '1',
         'PAGER': 'cat',
         'GIT_PAGER': 'cat',
         'MPLBACKEND': 'module://matplotlib_inline.backend_inline'}
       #+end_example

*** Setup: current git log
     #+BEGIN_SRC sh :session local2 :results org
       pwd
       git --no-pager log -5
     #+END_SRC

     #+RESULTS:
     #+begin_src org
     /home/lex/Programme/drmed-git
     commit d51b11eda090b9301e783ec35bdfd26c7bf0709c (HEAD -> exp-220316-publication1, origin/main, origin/exp-220316-publication1, origin/HEAD, main)
     Date:   Sun Feb 27 18:40:00 2022 +0100

         fix model input_size to None; else to crop_size

     commit c637444d8b798603629f6f0bd72ee55af7f81a5f
     Date:   Sun Feb 27 18:39:29 2022 +0100

         Fix function call correlate_and_fit

     commit 291c6619c12bc39d526137a43d976b3cb4881e50
     Date:   Sat Feb 26 20:04:07 2022 +0100

         Fix scale_trace; simplify tf_pad_trace call

     commit dcca8b9e17909a95b824c8a7b1fec52eeed198c3
     Date:   Thu Feb 24 16:11:39 2022 +0100

         test tf_pad_trace

     commit 6cf2da85748ef13f2e752bea8989a6d31549ced3
     Date:   Thu Feb 24 16:10:33 2022 +0100

         Fix tf_pad_trace
     #+end_src

*** Exp: simexps - ~weight=0~ vs =cut and shift= vs =avg=
    :PROPERTIES:
    :header-args:jupyter-python: :session /jpy:localhost#8889:a37e524a-8134-4d8f-b24a-367acaf1bdd3 :pandoc t
    :END:
   - To justify =cut and shift= as a method, let's eliminate the confounder of
     bad prediction and compare it to the alternative based on a segmentation
     we *know* from the simulations that is correct.
     - ~weight=0~ gives each time bin classified as 'dominated by clusters' a
       weight of 0.
     - ~cut and shift~ gives removes each time bin classified as 'dominated by
       clusters' and shifts all remaining time bins together.
   - After re-reading a lot of literature and putting it into text
     (<2023-01-02 Mo>), I decided to add the averaging method:
     1. segment trace in artifactual and non-artifactual (here: given by
        simulations)
     2. correlate all non-artifactual segments
     3. average correlations and fit the average
     #+BEGIN_SRC jupyter-python
       %cd /beegfs/ye53nis/drmed-git
     #+END_SRC

     #+RESULTS:
     : /beegfs/ye53nis/drmed-git

   - load modules
     #+BEGIN_SRC jupyter-python
       import datetime
       import logging
       import multipletau
       import os
       import scipy
       import sys

       import matplotlib.pyplot as plt
       import numpy as np
       import pandas as pd
       import seaborn as sns

       from pathlib import Path
       from pprint import pprint

       FLUOTRACIFY_PATH = '/beegfs/ye53nis/drmed-git/src/'
       sys.path.append(FLUOTRACIFY_PATH)

       from fluotracify.applications import correlate
       from fluotracify.simulations import (
          import_simulation_from_csv as isfc,
          analyze_simulations as ans,
       )

       logging.basicConfig(filename="/beegfs/ye53nis/drmed-git/data/exp-220316-publication1/jupyter.log",
                           filemode='w', format='%(asctime)s - %(message)s',
                           force=True,
                           level=logging.DEBUG)
       log = logging.getLogger(__name__)
       log.setLevel(logging.DEBUG)

       sns.set_theme(style="whitegrid", font_scale=2, palette='colorblind',
                     context='paper')

     #+END_SRC

     #+RESULTS:
     : 2023-01-03 23:23:38.245766: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
     : 2023-01-03 23:23:38.245806: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.

    #+BEGIN_SRC jupyter-python
      import importlib
      importlib.reload(isfc)
    #+END_SRC

    #+RESULTS:
    : <module 'fluotracify.applications.corr_fit_object' from '/home/lex/Programme/drmed-git/src/fluotracify/applications/corr_fit_object.py'>



   - load simulated data
     #+BEGIN_SRC jupyter-python :pandoc t
       col_per_example = 3
       # lab_thresh = 0.04
       # artifact = 0
       # model_type = 1
       fwhm = 250
       sim_path = Path('/beegfs/ye53nis/saves/firstartifact_Nov2020_test')

       sim, _, nsamples, sim_params = isfc.import_from_csv(
           folder=sim_path,
           header=12,
           frac_train=1,
           col_per_example=col_per_example,
           dropindex=None,
           dropcolumns=None)
       diffrates = sim_params.loc['diffusion rate of molecules in micrometer^2 / s'].astype(np.float32)
       nmols = sim_params.loc['number of fast molecules'].astype(np.float32)
       clusters = sim_params.loc['diffusion rate of clusters in micrometer^2 / s'].astype(np.float32)
       sim_columns = [f'{d:.4}-{c:.4}' for d, c in zip(
           np.repeat(diffrates, nsamples[0]),
           np.repeat(clusters, nsamples[0]))]

       sim_sep = isfc.separate_data_and_labels(array=sim,
                                               nsamples=nsamples,
                                               col_per_example=col_per_example)
       sim_dirty = sim_sep['0']
       sim_dirty.columns = sim_columns

       sim_labels = sim_sep['1']
       sim_labels.columns = sim_columns

       sim_clean = sim_sep['2']
       sim_clean.columns = sim_columns

       sim
     #+END_SRC

     #+RESULTS:
     :RESULTS:
     |       | trace001   | label001_1   | label001_2 | trace002   | label002_1   | label002_2 | trace003   | label003_1   | label003_2 | trace004   | ... | label097_2  | trace098    | label098_1 | label098_2  | trace099    | label099_1   | label099_2  | trace100    | label100_1   | label100_2  |
|-------+------------+--------------+------------+------------+--------------+------------+------------+--------------+------------+------------+-----+-------------+-------------+------------+-------------+-------------+--------------+-------------+-------------+--------------+-------------|
| 0     | 395.062347 | 4.538784e-20 | 395.062347 | 542.019287 | 8.120830e-03 | 501.415161 | 259.171783 | 2.228045e-24 | 259.171783 | 378.006470 | ... | 1231.325928 | 2721.381592 | 0.195874   | 1154.387695 | 1671.956787 | 6.682719e-04 | 1667.278809 | 1572.913452 | 1.325364e-02 | 1466.884277 |
| 1     | 395.732605 | 1.310606e-19 | 395.732605 | 676.451477 | 3.468467e-02 | 503.028076 | 263.082733 | 1.117190e-24 | 263.082733 | 365.738861 | ... | 1197.367310 | 2785.768066 | 0.203337   | 1159.073364 | 1749.072510 | 7.613653e-04 | 1743.742920 | 1544.390259 | 1.122567e-02 | 1454.584839 |
| 2     | 385.598785 | 6.306126e-22 | 385.598785 | 565.850403 | 1.276007e-02 | 502.050110 | 258.483124 | 8.280664e-26 | 258.483124 | 350.939362 | ... | 1229.265015 | 2961.105225 | 0.226369   | 1150.153320 | 1643.184204 | 6.983961e-04 | 1638.295532 | 1486.991211 | 1.182248e-02 | 1392.411377 |
| 3     | 375.055664 | 8.333913e-22 | 375.055664 | 569.737793 | 7.499466e-03 | 532.240479 | 252.117035 | 6.761740e-26 | 252.117035 | 364.043427 | ... | 1190.224854 | 3127.305664 | 0.243025   | 1183.104492 | 1713.993042 | 7.364776e-04 | 1708.837769 | 1427.290771 | 1.086318e-02 | 1340.385376 |
| 4     | 400.554443 | 2.098773e-21 | 400.554443 | 590.014893 | 7.808361e-03 | 550.973083 | 241.840240 | 7.160055e-28 | 241.840240 | 376.104645 | ... | 1268.028931 | 2997.608887 | 0.223969   | 1205.857422 | 1744.911865 | 6.761284e-04 | 1740.178955 | 1426.806763 | 9.973858e-03 | 1347.015869 |
| ...   | ...        | ...          | ...        | ...        | ...          | ...        | ...        | ...          | ...        | ...        | ... | ...         | ...         | ...        | ...         | ...         | ...          | ...         | ...         | ...          | ...         |
| 16379 | 433.562714 | 2.027369e-11 | 433.562714 | 624.462646 | 1.359731e-07 | 624.461975 | 643.004944 | 1.486138e-06 | 642.994568 | 518.733643 | ... | 1281.519775 | 1172.255371 | 0.000024   | 1172.062500 | 1347.495239 | 1.545398e-07 | 1347.494141 | 756.805908  | 3.299088e-13 | 756.805908  |
| 16380 | 462.284454 | 4.281444e-14 | 462.284454 | 616.137512 | 5.455384e-08 | 616.137268 | 597.266296 | 1.347712e-06 | 597.256836 | 487.652924 | ... | 1384.850098 | 1191.984253 | 0.000021   | 1191.816162 | 1482.415894 | 1.717639e-07 | 1482.414673 | 712.499878  | 2.858745e-13 | 712.499878  |
| 16381 | 472.551483 | 6.157024e-11 | 472.551483 | 612.926758 | 7.076798e-07 | 612.923218 | 615.009460 | 3.518227e-08 | 615.009216 | 516.941528 | ... | 1274.193848 | 1173.113770 | 0.000031   | 1172.869263 | 1520.151367 | 2.125578e-07 | 1520.149780 | 587.645203  | 2.861725e-13 | 587.645203  |
| 16382 | 486.679413 | 3.604344e-09 | 486.679382 | 637.962769 | 1.704117e-08 | 637.962708 | 616.344116 | 4.384124e-08 | 616.343811 | 502.372345 | ... | 1310.505981 | 1124.065552 | 0.000027   | 1123.853271 | 1572.194336 | 2.867827e-07 | 1572.192261 | 618.202820  | 4.085783e-13 | 618.202820  |
| 16383 | 489.893646 | 1.907032e-08 | 489.893555 | 614.733704 | 1.560388e-06 | 614.725891 | 614.638000 | 6.400571e-07 | 614.633545 | 511.408234 | ... | 1324.207275 | 1070.131104 | 0.000030   | 1069.894531 | 1602.530029 | 2.109545e-07 | 1602.528564 | 654.377380  | 5.819386e-13 | 654.377380  |

16384 rows × 9000 columns
     :END:


     #+BEGIN_SRC jupyter-python :pandoc t
       def label_correct_correlate(sim_dirty, sim_labels, sim_columns, lab_thresh, out_path):
           sim_labbool = sim_labels > lab_thresh
           sim_labbool.columns = sim_columns

           sim_cas, sim_del = pd.DataFrame(), pd.DataFrame()
           for i in range(len(sim_dirty.columns)):
               # cut and shift correction
               sim_cas_trace = np.delete(sim_dirty.iloc[:, i].values,
                                          sim_labbool.iloc[:, i].values)
               sim_cas_trace = pd.DataFrame(sim_cas_trace)
               sim_cas = pd.concat([sim_cas, sim_cas_trace], axis='columns')
               # weight=0 / delete correction
               sim_del_trace = np.where(sim_labbool.iloc[:, i].values == 1, 0,
                                        sim_dirty.iloc[:, i].values)
               sim_del_trace = pd.DataFrame(sim_del_trace)
               sim_del = pd.concat([sim_del, sim_del_trace], axis='columns')
           sim_cas.columns, sim_del.columns = sim_dirty.columns, sim_dirty.columns

           log.debug('label_correct_correlate: Finished "cut and shift" and "weight=0" correction.')

           # after correction
           lab_str = f'{lab_thresh}'.replace(".", "dot")
           cas_txt = f'labthresh-{lab_str}_cutandshift'
           del_txt = f'labthresh-{lab_str}_delete'
           correlate.correlate_timetrace_and_save(df=sim_cas, out_path=out_path, out_txt=cas_txt)
           correlate.correlate_timetrace_and_save(df=sim_del, out_path=out_path, out_txt=del_txt)

       def label_avg_correct_correlate(sim_dirty, sim_labels, sim_columns, lab_thresh, out_path):
           sim_labbool = sim_labels > lab_thresh
           sim_labbool.columns = sim_columns

           sim_avg = pd.DataFrame()
           for i in range(len(sim_dirty.columns)):
               # sim_labbool gives False (or 0) for parts deemed non-artifactual and
               # True (or 1) for parts deemed artifactual. Switch 0 and 1 for label() fct.
               # Take all connected segments with value 1 (non-artifactual) and give them a distinct label
               sim_segments = scipy.ndimage.label(~sim_labbool.iloc[:, i])
               time_and_corrs = []
               for u in np.unique(sim_segments[0]):
                   # ignore all parts with value 0 (artifactual)
                   if u == 0:
                       continue
                   # now get intensities for each label and correlate the parts
                   part = np.where(sim_segments[0] == u, sim_dirty.iloc[:, i], np.nan)
                   part = part[~np.isnan(part)]
                   if len(part) > 32:  # minimal length for multipletau
                       corr_fn = multipletau.autocorrelate(
                           a=part,
                           m=16,
                           deltat=1,
                           normalize=True)
                       time_and_corrs.append(corr_fn)
               max_autotime = max([len(c[1:, 0]) for c in time_and_corrs])
               autotime = [c[1:, 0] for c in time_and_corrs if len(c[1:, 0]) == max_autotime][0]
               corrs = [c[1:, 1] for c in time_and_corrs]
               # convert to pandas dataframe to easily compute the mean
               corr_df = pd.DataFrame(corrs)
               corr_df.columns = autotime
               sim_avg = pd.concat([sim_avg, corr_df.mean()], axis='columns')
           sim_avg.columns = sim_dirty.columns

           log.debug('label_avg_correct_correlate: Finished "averaging" correction.')

           lab_str = f'{lab_thresh}'.replace(".", "dot")
           avg_txt = f'labthresh-{lab_str}_avgcorrs'
           correlate.save_correlations(sim_avg, out_path, out_txt=avg_txt)

     #+END_SRC

     #+RESULTS:


   - plot simulated data with label thresholds of interest
     #+BEGIN_SRC jupyter-python
       out_path = "/beegfs/ye53nis/drmed-git/data/exp-220316-publication1/220517_simulations/"

       label_correct_correlate(
           sim_dirty=sim_dirty,
           sim_labels=sim_labels,
           sim_columns=sim_columns,
           lab_thresh=0.04,
           out_path=out_path)
     #+END_SRC

     #+RESULTS:


   - now order the correlations in respective folders
     #+BEGIN_SRC tmux :socket ~/.tmux-local-socket-remote-machine :session tmux
       cd /beegfs/ye53nis/drmed-git/data/exp-220316-publication1/220517_simulations
       mkdir -p labthresh-0.04-cutandshift/0.069
       mkdir -p labthresh-0.04-cutandshift/0.08
       mkdir -p labthresh-0.04-cutandshift/0.1
       mkdir -p labthresh-0.04-cutandshift/0.2
       mkdir -p labthresh-0.04-cutandshift/0.4
       mkdir -p labthresh-0.04-cutandshift/0.6
       mkdir -p labthresh-0.04-cutandshift/1.0
       mkdir -p labthresh-0.04-cutandshift/3.0
       mkdir -p labthresh-0.04-cutandshift/10.0
       mkdir -p labthresh-0.04-cutandshift/50.0

       mkdir -p labthresh-0.04-delete/0.069
       mkdir -p labthresh-0.04-delete/0.08
       mkdir -p labthresh-0.04-delete/0.1
       mkdir -p labthresh-0.04-delete/0.2
       mkdir -p labthresh-0.04-delete/0.4
       mkdir -p labthresh-0.04-delete/0.6
       mkdir -p labthresh-0.04-delete/1.0
       mkdir -p labthresh-0.04-delete/3.0
       mkdir -p labthresh-0.04-delete/10.0
       mkdir -p labthresh-0.04-delete/50.0
     #+END_SRC

     #+BEGIN_SRC tmux :socket ~/.tmux-local-socket-remote-machine :session tmux
       mv *cutandshift_0dot069* labthresh-0.04-cutandshift/0.069
       mv *cutandshift_0dot08* labthresh-0.04-cutandshift/0.08
       mv *cutandshift_0dot1* labthresh-0.04-cutandshift/0.1
       mv *cutandshift_0dot2* labthresh-0.04-cutandshift/0.2
       mv *cutandshift_0dot4* labthresh-0.04-cutandshift/0.4
       mv *cutandshift_0dot6* labthresh-0.04-cutandshift/0.6
       mv *cutandshift_1dot0* labthresh-0.04-cutandshift/1.0
       mv *cutandshift_3dot0* labthresh-0.04-cutandshift/3.0
       mv *cutandshift_10dot0* labthresh-0.04-cutandshift/10.0
       mv *cutandshift_50dot0* labthresh-0.04-cutandshift/50.0

       mv *delete_0dot069* labthresh-0.04-delete/0.069
       mv *delete_0dot08* labthresh-0.04-delete/0.08
       mv *delete_0dot1* labthresh-0.04-delete/0.1
       mv *delete_0dot2* labthresh-0.04-delete/0.2
       mv *delete_0dot4* labthresh-0.04-delete/0.4
       mv *delete_0dot6* labthresh-0.04-delete/0.6
       mv *delete_1dot0* labthresh-0.04-delete/1.0
       mv *delete_3dot0* labthresh-0.04-delete/3.0
       mv *delete_10dot0* labthresh-0.04-delete/10.0
       mv *delete_50dot0* labthresh-0.04-delete/50.0

     #+END_SRC

   - now the averaging method:
     #+BEGIN_SRC jupyter-python
       out_path = Path('/beegfs/ye53nis/drmed-git/data/exp-220316-publication1/230103_avg-correction')

       for name in set(sim_dirty.columns):
           out_folder = out_path / f'{name.split("-")[0]}'

           %mkdir -p $out_folder
           label_avg_correct_correlate(
               sim_dirty=sim_dirty.loc[:, name],
               sim_labels=sim_labels,
               sim_columns=sim_columns,
               lab_thresh=0.04,
               out_path=out_folder)

     #+END_SRC

     #+RESULTS:

     #+BEGIN_SRC jupyter-python
       set(sim_dirty.columns)
     #+END_SRC

     #+RESULTS:
     | 0.069-0.01 | 0.069-0.1 | 0.069-1.0 | 0.08-0.01 | 0.08-0.1 | 0.08-1.0 | 0.1-0.01 | 0.1-0.1 | 0.1-1.0 | 0.2-0.01 | 0.2-0.1 | 0.2-1.0 | 0.4-0.01 | 0.4-0.1 | 0.4-1.0 | 0.6-0.01 | 0.6-0.1 | 0.6-1.0 | 1.0-0.01 | 1.0-0.1 | 1.0-1.0 | 10.0-0.01 | 10.0-0.1 | 10.0-1.0 | 3.0-0.01 | 3.0-0.1 | 3.0-1.0 | 50.0-0.01 | 50.0-0.1 | 50.0-1.0 |

*** Exp: simexps - characterization of cutandshift
   :PROPERTIES:
   :header-args:jupyter-python: :session /jpy:localhost#8888:137e524a-8134-4d8f-b24a-367acaf1bdd3 :pandoc t
   :END:
   - We want to check the following properties of FCS trace cutting:
     1. does cutting introduce artifacts (Does condition of stationarity hold?)
        → plot of mean / median / mode of transit times in clean trace when cut
        with growing number of cuts and then shuffling the trace

     #+BEGIN_SRC jupyter-python
       %cd ~/Programme/drmed-git
     #+END_SRC

     #+RESULTS:
     : /home/lex/Programme/drmed-git

   - load modules
     #+BEGIN_SRC jupyter-python
       import logging
       import os
       import pdb
       import sys

       import matplotlib.pyplot as plt
       import numpy as np
       import pandas as pd
       import seaborn as sns

       from pathlib import Path
       from pprint import pprint

       FLUOTRACIFY_PATH = '/home/lex/Programme/drmed-git/src/'
       sys.path.append(FLUOTRACIFY_PATH)
       from fluotracify.applications import (corr_fit_object as cfo,
                                             correlate)
       from fluotracify.imports import ptu_utils as ptu
       from fluotracify.simulations import (
          import_simulation_from_csv as isfc,
          analyze_simulations as ans,
       )

       logging.basicConfig(filename="/home/lex/Programme/drmed-git/data/exp-220316-publication1/jupyter.log",
                           filemode='w', format='%(asctime)s - %(message)s',
                           force=True,
                           level=logging.DEBUG)

       sns.set_theme(style="whitegrid", font_scale=2, palette='colorblind',
                     context='paper')

     #+END_SRC

     #+RESULTS:

    #+BEGIN_SRC jupyter-python
      import importlib
      importlib.reload(correlate)
      importlib.reload(ans)
    #+END_SRC

    #+RESULTS:
    : <module 'fluotracify.simulations.analyze_simulations' from '/home/lex/Programme/drmed-git/src/fluotracify/simulations/analyze_simulations.py'>

   - load simulated data
     #+BEGIN_SRC jupyter-python :pandoc t
       col_per_example = 3
       lab_thresh = 0.04
       artifact = 0
       model_type = 1
       fwhm = 250
       sim_path = Path('/home/lex/Programme/drmed-collections/drmed-simexps/firstartifact_Nov2020_test')

       sim, _, nsamples, sim_params = isfc.import_from_csv(
           folder=sim_path,
           header=12,
           frac_train=1,
           col_per_example=col_per_example,
           dropindex=None,
           dropcolumns=None)

       diffrates = sim_params.loc['diffusion rate of molecules in micrometer^2 / s'].astype(np.float32)
       nmols = sim_params.loc['number of fast molecules'].astype(np.float32)
       clusters = sim_params.loc['diffusion rate of clusters in micrometer^2 / s'].astype(np.float32)
       sim_columns = [f'{d:.4}' for d, c in zip(
           np.repeat(diffrates, nsamples[0]),
           np.repeat(clusters, nsamples[0]))]

       sim_sep = isfc.separate_data_and_labels(array=sim,
                                               nsamples=nsamples,
                                               col_per_example=col_per_example)
       sim_dirty = sim_sep['0']
       sim_dirty.columns = sim_columns

       sim_labels = sim_sep['1']
       sim_labels.columns = sim_columns
       sim_labbool = sim_labels > lab_thresh
       sim_labbool.columns = sim_columns
       sim_clean = sim_sep['2']
       sim_clean.columns = sim_columns

       sim_dirty
     #+END_SRC

     #+RESULTS:
     :RESULTS:
     |       | 0.069       | 0.069       | 0.069       | 0.069      | 0.069      | 0.069       | 0.069      | 0.069       | 0.069      | 0.069      | ... | 50.0        | 50.0        | 50.0        | 50.0        | 50.0        | 50.0        | 50.0        | 50.0        | 50.0        | 50.0        |
|-------+-------------+-------------+-------------+------------+------------+-------------+------------+-------------+------------+------------+-----+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------|
| 0     | 1187.467896 | 907.677734  | 480.048798  | 454.669403 | 466.063232 | 384.734467  | 543.981323 | 640.921509  | 795.946167 | 410.471893 | ... | 1897.398193 | 2279.646484 | 3088.531006 | 2034.432495 | 2187.548584 | 2105.736084 | 1789.366577 | 2023.093994 | 2331.185791 | 2185.028076 |
| 1     | 1184.055176 | 945.760315  | 471.065216  | 458.392487 | 473.306152 | 395.165527  | 558.603088 | 622.421204  | 776.199402 | 409.149170 | ... | 1499.969849 | 2199.652100 | 3207.333008 | 1650.523926 | 2122.935791 | 2791.281006 | 1661.286377 | 1111.879761 | 1853.699585 | 1926.844971 |
| 2     | 1191.848877 | 980.117798  | 459.479706  | 426.087982 | 482.370209 | 425.123413  | 551.536072 | 624.498535  | 778.671265 | 400.971954 | ... | 1822.985229 | 2456.422607 | 2969.562500 | 1934.118286 | 1457.731812 | 2251.077393 | 1903.003662 | 2063.915527 | 2198.018066 | 2038.683105 |
| 3     | 1199.065918 | 974.313110  | 462.205566  | 444.041809 | 463.703125 | 434.186615  | 573.044128 | 626.252502  | 747.284058 | 393.178162 | ... | 1741.839355 | 2467.149414 | 2588.980957 | 2136.627686 | 1930.263672 | 2323.700684 | 2133.313721 | 1638.169312 | 1926.058716 | 1815.259521 |
| 4     | 1221.957397 | 968.779175  | 464.918030  | 455.205292 | 474.615540 | 437.029419  | 586.489136 | 619.319092  | 781.954102 | 406.468018 | ... | 2431.400879 | 2246.336670 | 3000.961182 | 1915.518066 | 2052.773682 | 2359.145508 | 1699.926147 | 1862.709595 | 2291.338379 | 1332.422241 |
| ...   | ...         | ...         | ...         | ...        | ...        | ...         | ...        | ...         | ...        | ...        | ... | ...         | ...         | ...         | ...         | ...         | ...         | ...         | ...         | ...         | ...         |
| 16379 | 506.409668  | 1012.403931 | 855.006226  | 674.470703 | 769.859192 | 2110.732178 | 799.565247 | 1981.221436 | 528.844604 | 483.055878 | ... | 1512.586548 | 3212.712891 | 1491.119995 | 1843.866943 | 1748.956665 | 2048.602051 | 1662.244385 | 2593.879639 | 1921.427856 | 1664.831909 |
| 16380 | 536.809692  | 1022.029724 | 840.287720  | 671.095215 | 738.908997 | 2118.984863 | 807.995789 | 2624.343262 | 552.687012 | 479.768372 | ... | 1661.331055 | 3190.809570 | 1770.193970 | 2081.854248 | 2164.372803 | 2295.646729 | 1846.683594 | 2038.272339 | 2222.708252 | 2122.753662 |
| 16381 | 570.668884  | 989.891235  | 839.180298  | 689.863586 | 695.739136 | 2033.681885 | 786.547852 | 3528.163574 | 572.166077 | 484.491211 | ... | 1643.470337 | 2564.206787 | 2025.219971 | 2104.706787 | 1792.828613 | 2106.199463 | 2087.914062 | 1457.817871 | 1874.736938 | 1683.072021 |
| 16382 | 562.505310  | 977.029785  | 1005.927673 | 683.250183 | 661.608337 | 2566.123047 | 805.594116 | 3731.086426 | 566.710571 | 489.289673 | ... | 1556.492188 | 2783.619385 | 1312.174561 | 2378.643311 | 2466.965576 | 2160.641357 | 1691.332520 | 2013.095093 | 1632.475708 | 1352.443237 |
| 16383 | 567.307373  | 1006.794067 | 982.376526  | 677.099854 | 657.040588 | 2545.080322 | 784.917969 | 3850.334717 | 570.241699 | 512.688232 | ... | 2127.414551 | 2448.062012 | 1398.359253 | 1665.321167 | 2241.687256 | 1823.699829 | 1340.686035 | 1972.661743 | 1550.770142 | 1727.808228 |

16384 rows × 1500 columns
     :END:


   - I wrote a small function to cut the simulated traces and shuffle the
     resulting chunks. Let's look at the results:
     #+BEGIN_SRC jupyter-python
       sim_clean_cut = ans.cut_simulations_and_shuffle_chunks(sim_clean, 1000)
     #+END_SRC

     #+RESULTS:
     : /home/lex/Programme/drmed-git/src/fluotracify/simulations/analyze_simulations.py:406: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
     :   # shuffle the list of series



     #+begin_src jupyter-python
       %%timeit
       sim_clean_cut = pd.DataFrame()
       # 230 µs ± 6.17
       pos_of_cuts = rng.choice(sim_clean.iloc[:, 0].index, 15000, replace=False, shuffle=False)
       # 3.81 ms ± 186 µs
       pos_of_cuts.sort()
       # 15.8 ms ± 796 µs
       trace = np.split(sim_clean.iloc[:, 0].to_numpy(), pos_of_cuts)
       # 1.3 ms ± 32.2 µs
       trace = rng.permuted(trace)
       # 2.97 ms ± 173 µs
       trace = np.concatenate(trace)

       trace = pd.Series(trace, name=sim_clean.iloc[:, 0].name)
       sim_clean_cut = pd.concat([sim_clean_cut, trace], axis=1)
     #+end_src

     #+RESULTS:
     : <magic-timeit>:9: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
     : 38.4 ms ± 1.7 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)


     #+BEGIN_SRC jupyter-python
       out_path = Path('/home/lex/Programme/drmed-git/data/exp-220316-publication1/220714_sim-cutandshift')
       for i in [1, 10, 100, 1000, 10000]:
           sim_clean_cut = ans.cut_simulations_and_shuffle_chunks(sim_clean, i)

           for name in set(sim_clean.columns):
               out_folder = out_path / f'{name}' /  f'{i}_cuts'
               out_txt = f'{i}-cuts'
               %mkdir -p $out_folder
               correlate.correlate_timetrace_and_save(sim_clean_cut.loc[:, name], out_folder, out_txt)

     #+END_SRC

     #+RESULTS:
     : /home/lex/Programme/drmed-git/src/fluotracify/simulations/analyze_simulations.py:471: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
     :   trace = rng.permuted(trace)

     #+BEGIN_SRC jupyter-python
       out_path = Path('/home/lex/Programme/drmed-git/data/exp-220316-publication1/220714_sim-cutandshift')
       for i in [2 , 4, 8, 20, 40, 80, 200, 400, 800]:
           sim_clean_cut = ans.cut_simulations_and_shuffle_chunks(sim_clean, i)

           for name in set(sim_clean.columns):
               out_folder = out_path / f'{name}' /  f'{i}_cuts'
               out_txt = f'{i}-cuts'
               %mkdir -p $out_folder
               correlate.correlate_timetrace_and_save(sim_clean_cut.loc[:, name], out_folder, out_txt)

     #+END_SRC

     #+RESULTS:

     #+BEGIN_SRC jupyter-python
       out_path = Path('/home/lex/Programme/drmed-git/data/exp-220316-publication1/220714_sim-cutandshift')
       for i in [0]:
           for name in set(sim_clean.columns):
               out_folder = out_path / f'{name}' /  f'{i}_cuts'
               out_txt = f'{i}-cuts'
               %mkdir -p $out_folder
               correlate.correlate_timetrace_and_save(sim_clean.loc[:, name], out_folder, out_txt)

     #+END_SRC

     #+RESULTS:

     #+BEGIN_SRC jupyter-python
       out_path = Path('/home/lex/Programme/drmed-git/data/exp-220316-publication1/220714_sim-cutandshift')
       for i in [0, 1, 2 , 4, 8, 10, 20, 40, 80, 100, 200, 400, 800, 1000, 10000]:
           if i == 0:
               for name in set(sim_clean.loc[:, '3.0'].columns):
                   out_folder = out_path / f'{name}' /  f'{i}_cuts'
                   out_txt = f'{i}-cuts'
                   %mkdir -p $out_folder
                   correlate.correlate_timetrace_and_save(sim_clean.loc[:, name], out_folder, out_txt)
           else:
               sim_clean_cut = ans.cut_simulations_and_shuffle_chunks(sim_clean.loc[:, '3.0'], i)
               for name in set(sim_clean.loc[:, '3.0'].columns):
                   out_folder = out_path / f'{name}' /  f'{i}_cuts'
                   out_txt = f'{i}-cuts'
                   %mkdir -p $out_folder
                   correlate.correlate_timetrace_and_save(sim_clean_cut.loc[:, name], out_folder, out_txt)

     #+END_SRC

     #+RESULTS:
     : /home/lex/Programme/drmed-git/src/fluotracify/simulations/analyze_simulations.py:519: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
     :   trace = rng.permuted(trace)

**** node 2
    :PROPERTIES:
    :header-args:jupyter-python: :session /jpy:localhost#8890:c37e524a-8134-4d8f-b24a-367acaf1bdd5
    :END:
    1. Set up tmux (if we haven't done that before) (=#+CALL:
       setup-tmux[:session local]=)
       #+CALL: setup-tmux[:session local]

       #+RESULTS:
       |         |                                        |           |
       | sh-5.1$ | ye53nis@ara-login01.rz.uni-jena.de's | password: |
       | >       | ye53nis@ara-login01.rz.uni-jena.de's | password: |

    2. Request compute node
       #+BEGIN_SRC tmux :socket ~/.tmux-local-socket-remote-machine :session jpmux2
         cd /
         srun -p s_standard --time=7-10:00:00 --ntasks-per-node=24 --mem-per-cpu=2000 --pty bash
       #+END_SRC

    3. Start Jupyter Lab (~#+CALL: jpt-tmux[:session jpmux]~)
       #+BEGIN_SRC tmux :socket ~/.tmux-local-socket-remote-machine :session jpmux2
       conda activate tf
       export PORT=8890
       export XDG_RUNTIME_DIR=''
       export XDG_RUNTIME_DIR=""
       jupyter lab --no-browser --port=$PORT
       #+END_SRC

       #+RESULTS:
       #+begin_example
         (tf) [ye53nis@node159 /]$ jupyter lab --no-browser --port=$PORT
         [I 2022-07-26 15:41:02.905 ServerApp] jupyterlab | extension was successfully linked.
         [I 2022-07-26 15:41:04.628 ServerApp] nbclassic | extension was successfully linked.
         [I 2022-07-26 15:41:04.768 ServerApp] nbclassic | extension was successfully loaded.
         [I 2022-07-26 15:41:04.771 LabApp] JupyterLab extension loaded from /home/ye53nis/.conda/envs/tf/lib/python3.9/site-packages/jupyterlab
         [I 2022-07-26 15:41:04.771 LabApp] JupyterLab application directory is /home/ye53nis/.conda/envs/tf/share/jupyter/lab
         [I 2022-07-26 15:41:04.781 ServerApp] jupyterlab | extension was successfully loaded.
         [I 2022-07-26 15:41:04.783 ServerApp] Serving notebooks from local directory: /
         [I 2022-07-26 15:41:04.783 ServerApp] Jupyter Server 1.13.5 is running at:
         [I 2022-07-26 15:41:04.783 ServerApp] http://localhost:8890/lab?token=d395885914fc03bc2970ad2f723a2cbcd46c9b2a23982d8f
         [I 2022-07-26 15:41:04.783 ServerApp]  or http://127.0.0.1:8890/lab?token=d395885914fc03bc2970ad2f723a2cbcd46c9b2a23982d8f
         [I 2022-07-26 15:41:04.783 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
         [C 2022-07-26 15:41:04.800 ServerApp]

             To access the server, open this file in a browser:
                 file:///home/ye53nis/.local/share/jupyter/runtime/jpserver-18143-open.html
             Or copy and paste one of these URLs:
                 http://localhost:8890/lab?token=d395885914fc03bc2970ad2f723a2cbcd46c9b2a23982d8f
              or http://127.0.0.1:8890/lab?token=d395885914fc03bc2970ad2f723a2cbcd46c9b2a23982d8f
       #+end_example

    4. Create SSH Tunnel for jupyter lab to the local computer (e.g. ~#+CALL:
       ssh-tunnel(port="8889", node="node160")~)
       #+CALL: ssh-tunnel[:session jpmux2](port="8890", node="node159")

       #+RESULTS:
       |                   |           |                                        |           |    |          |      |      |             |
       | sh-5.1$           | sh-5.1$   | ye53nis@ara-login01.rz.uni-jena.de's | password: |    |          |      |      |             |
       | ye53nis@node159's | password: |                                        |           |    |          |      |      |             |
       | Last              | login:    | Tue                                    | Jul       | 26 | 22:11:00 | 2022 | from | login01.ara |


   - =#+CALL: kill-jupyter()=
     #+CALL: kill-jupyter()

     #+RESULTS:
     :RESULTS:
     # [goto error]
     : ---------------------------------------------------------------------------
     : NameError                                 Traceback (most recent call last)
     : Input In [1], in <cell line: 1>()
     : ----> 1 os._exit(00)
     :
     : NameError: name 'os' is not defined
     :END:

   - ~#+CALL: prepare-jupyter(data_path="/beegfs/ye53nis/data/191113_Pex5_2_structured")~
     #+CALL: prepare-jupyter(data_path="/beegfs/ye53nis/data/191113_Pex5_2_structured")

     #+RESULTS:
     : /beegfs/ye53nis/drmed-git
     : 2022-07-27 12:53:09.499191: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
     : 2022-07-27 12:53:09.499274: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.

     #+BEGIN_SRC jupyter-python
       # threshold[0]: node2
       # threshold[1]: node2
       # threshold[2]: node2
       out_dir = output_path / 'Tb-PEX5-eGFP'
       pred_method = 'threshold'
       correction_method = 'delete_and_shift'
       threshold_ls = [5, 7, 10]

       for thr in threshold_ls:
           out_folder = out_dir / f'robust_thresh-{thr}'
           %mkdir -p $out_folder
           threshold_predict_correct_correlate_ptu(
               files=files_dirty3,
               pred_method=pred_method,
               pred_threshold=thr,
               correction_method=correction_method,
               out_path=out_folder)
     #+END_SRC

      #+RESULTS:
      : 2022-07-27 12:54:02.705997: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
      : 2022-07-27 12:54:02.706040: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)
      : 2022-07-27 12:54:02.706062: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (node159): /proc/driver/nvidia/version does not exist
      : 2022-07-27 12:54:02.706560: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
      : To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.

    #+CALL: kill-jupyter()

    #+RESULTS:
    :RESULTS:
    # [goto error]
    : ---------------------------------------------------------------------------
    : NameError                                 Traceback (most recent call last)
    : Input In [1], in <cell line: 1>()
    : ----> 1 os._exit(00)
    :
    : NameError: name 'os' is not defined
    :END:


    #+CALL: prepare-jupyter("/beegfs/ye53nis/data/191113_Pex5_2_structured", output_path="/beegfs/ye53nis/drmed-git/data/exp-220227-unet/2022-06-02_experimental-pex5/")

    #+RESULTS:
    : /beegfs/ye53nis/drmed-git
    : 2022-06-06 11:31:31.271711: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
    : 2022-06-06 11:31:31.271745: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.

      #+begin_src jupyter-python
        model_id = 4  # 484af

        out_dir = output_path / f'clean_delete_{model_name_ls[model_id]}/'

        os.makedirs(out_dir, exist_ok=True)

        predict_correct_correlate_ptu(
            files=files_clean,
            model_id=model_id,
            method='delete',
            out_path=out_dir)
      #+end_src

      #+RESULTS:
      : 2022-06-06 11:31:39.219733: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
      : 2022-06-06 11:31:39.219776: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)
      : 2022-06-06 11:31:39.219797: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (node152): /proc/driver/nvidia/version does not exist
      : 2022-06-06 11:31:39.220117: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
      : To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
      : WARNING:tensorflow:Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `

**** node 3
    :PROPERTIES:
    :header-args:jupyter-python: :session /jpy:localhost#8891:b37e524a-8134-4d8f-b24a-367acaf1bdd5
    :END:
    1. Set up tmux (if we haven't done that before) (=#+CALL:
       setup-tmux[:session local]=)
       #+CALL: setup-tmux[:session local]

       #+RESULTS:
       |         |                                        |           |
       | sh-5.1$ | ye53nis@ara-login01.rz.uni-jena.de's | password: |
       | >       | ye53nis@ara-login01.rz.uni-jena.de's | password:   |

    2. Request compute node
       #+BEGIN_SRC tmux :socket ~/.tmux-local-socket-remote-machine :session jpmux3
         cd /
         srun -p s_standard --time=7-10:00:00 --ntasks-per-node=24 --mem-per-cpu=2000 --pty bash
       #+END_SRC

    3. Start Jupyter Lab (~#+CALL: jpt-tmux[:session jpmux]~)
       #+BEGIN_SRC tmux :socket ~/.tmux-local-socket-remote-machine :session jpmux3
       conda activate tf
       export PORT=8891
       export XDG_RUNTIME_DIR=''
       export XDG_RUNTIME_DIR=""
       jupyter lab --no-browser --port=$PORT
       #+END_SRC

       #+RESULTS:
       #+begin_example
         (tf) [ye53nis@node313 /]$ jupyter lab --no-browser --port=$PORT
         [I 2022-07-26 15:41:17.402 ServerApp] jupyterlab | extension was successfully linked.
         [I 2022-07-26 15:41:18.325 ServerApp] nbclassic | extension was successfully linked.
         [I 2022-07-26 15:41:18.431 ServerApp] nbclassic | extension was successfully loaded.
         [I 2022-07-26 15:41:18.434 LabApp] JupyterLab extension loaded from /home/ye53nis/.conda/envs/tf/lib/python3.9/site-packages/jupyterlab
         [I 2022-07-26 15:41:18.434 LabApp] JupyterLab application directory is /home/ye53nis/.conda/envs/tf/share/jupyter/lab
         [I 2022-07-26 15:41:18.443 ServerApp] jupyterlab | extension was successfully loaded.
         [I 2022-07-26 15:41:18.444 ServerApp] Serving notebooks from local directory: /
         [I 2022-07-26 15:41:18.444 ServerApp] Jupyter Server 1.13.5 is running at:
         [I 2022-07-26 15:41:18.444 ServerApp] http://localhost:8891/lab?token=9471873c7815b3b2691a44102251c64823a1fb61f5076036
         [I 2022-07-26 15:41:18.444 ServerApp]  or http://127.0.0.1:8891/lab?token=9471873c7815b3b2691a44102251c64823a1fb61f5076036
         [I 2022-07-26 15:41:18.444 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
         [C 2022-07-26 15:41:18.460 ServerApp]

             To access the server, open this file in a browser:
                 file:///home/ye53nis/.local/share/jupyter/runtime/jpserver-94844-open.html
             Or copy and paste one of these URLs:
                 http://localhost:8891/lab?token=9471873c7815b3b2691a44102251c64823a1fb61f5076036
              or http://127.0.0.1:8891/lab?token=9471873c7815b3b2691a44102251c64823a1fb61f5076036
       #+end_example

    4. Create SSH Tunnel for jupyter lab to the local computer (e.g. ~#+CALL:
       ssh-tunnel(port="8889", node="node160")~)
       #+CALL: ssh-tunnel[:session jpmux3](port="8891", node="node313")

       #+RESULTS:
       |                   |           |                                        |           |    |          |      |      |             |
       | sh-5.1$           | sh-5.1$   | ye53nis@ara-login01.rz.uni-jena.de's | password: |    |          |      |      |             |
       | ye53nis@node313's | password: |                                        |           |    |          |      |      |             |
       | Last              | login:    | Tue                                    | Jul       | 26 | 22:11:17 | 2022 | from | login01.ara |



    #+CALL: prepare-jupyter()

    #+RESULTS:
    : /beegfs/ye53nis/drmed-git
    : 2022-07-27 12:31:19.642954: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
    : 2022-07-27 12:31:19.643031: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.

      #+BEGIN_SRC jupyter-python

        out_dir = output_path / f'af488'
        pred_method = 'threshold'
        correction_method = 'delete_and_shift'
        threshold_ls = [0.8, 1, 1.5, 2, 2.5]

        for thr in threshold_ls[4:]:
            out_folder = out_dir / f'robust_thresh-{thr}'
            # %mkdir -p $out_folder
            threshold_predict_correct_correlate_ptu(
                files=files_clean2,
                pred_method=pred_method,
                pred_threshold=thr,
                correction_method=correction_method,
                out_path=out_folder)
            break
      #+END_SRC

      #+RESULTS:
      : 2022-07-27 12:31:40.270292: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
      : 2022-07-27 12:31:40.270377: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)
      : 2022-07-27 12:31:40.270427: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (node313): /proc/driver/nvidia/version does not exist
      : 2022-07-27 12:31:40.271076: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
      : To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.

    #+CALL: kill-jupyter()

    #+RESULTS:
    : 6c6240a9-a9d6-4fac-936c-a02553ce2158


    #+CALL: prepare-jupyter("/beegfs/ye53nis/data/191113_Pex5_2_structured", output_path="/beegfs/ye53nis/drmed-git/data/exp-220227-unet/2022-06-02_experimental-pex5/")

    #+RESULTS:
    : /beegfs/ye53nis/drmed-git

      #+begin_src jupyter-python
        model_id = 5  # 0cd20

        out_dir = output_path / f'clean_delete_{model_name_ls[model_id]}/'

        os.makedirs(out_dir, exist_ok=True)

        predict_correct_correlate_ptu(
            files=files_clean,
            model_id=model_id,
            method='delete',
            out_path=out_dir)
      #+end_src

      #+RESULTS:
      : 2022-06-06 16:04:56.835044: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
      : 2022-06-06 16:04:56.835085: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)
      : 2022-06-06 16:04:56.835105: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (node155): /proc/driver/nvidia/version does not exist
      : 2022-06-06 16:04:56.835386: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
      : To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
      : WARNING:tensorflow:Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.

*** Exp: simexps - prediction by threshold
   :PROPERTIES:
   :header-args:jupyter-python: :session /jpy:localhost#8888:137e524a-8134-4d8f-b24a-367acaf1bdd3 :pandoc t
   :END:
   - As basline for peak prediction compare UNET performance against common peak
     finding algorithms:
     - robust peak detection algorithm using z-scores
       https://stackoverflow.com/questions/22583391/peak-signal-detection-in-realtime-timeseries-data
     - rolling ball
       https://scikit-image.org/docs/stable/auto_examples/segmentation/plot_rolling_ball.html#d-signal-filtering
     - scipy peak width finding
       https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.peak_widths.html#scipy.signal.peak_widths

     #+BEGIN_SRC jupyter-python
       %cd ~/Programme/drmed-git
     #+END_SRC

     #+RESULTS:
     : /home/lex/Programme/drmed-git

   - load modules
     #+BEGIN_SRC jupyter-python
       import logging
       import os
       import pdb
       import sys

       import matplotlib.pyplot as plt
       import numpy as np
       import pandas as pd
       import seaborn as sns

       from pathlib import Path
       from pprint import pprint

       FLUOTRACIFY_PATH = '/home/lex/Programme/drmed-git/src/'
       sys.path.append(FLUOTRACIFY_PATH)
       from fluotracify.applications import (corr_fit_object as cfo,
                                             correlate)
       from fluotracify.imports import ptu_utils as ptu
       from fluotracify.simulations import (
          import_simulation_from_csv as isfc,
          analyze_simulations as ans,
       )

       logging.basicConfig(filename="/home/lex/Programme/drmed-git/data/exp-220316-publication1/jupyter.log",
                           filemode='w', format='%(asctime)s - %(message)s',
                           force=True,
                           level=logging.DEBUG)

       sns.set_theme(style="whitegrid", font_scale=2, palette='colorblind',
                     context='paper')

     #+END_SRC

     #+RESULTS:
     : 2022-08-11 13:27:20.978319: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
     : 2022-08-11 13:27:20.978362: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.

    #+BEGIN_SRC jupyter-python
      import importlib
      importlib.reload(correlate)
      importlib.reload(ans)
    #+END_SRC

    #+RESULTS:
    : <module 'fluotracify.simulations.analyze_simulations' from '/home/lex/Programme/drmed-git/src/fluotracify/simulations/analyze_simulations.py'>

   - load simulated data
     #+BEGIN_SRC jupyter-python :pandoc t
       col_per_example = 3
       lab_thresh = 0.04
       artifact = 0
       model_type = 1
       fwhm = 250
       sim_path = Path('/home/lex/Programme/drmed-collections/drmed-simexps/firstartifact_Nov2020_test')

       sim, _, nsamples, sim_params = isfc.import_from_csv(
           folder=sim_path,
           header=12,
           frac_train=1,
           col_per_example=col_per_example,
           dropindex=None,
           dropcolumns=None)

       diffrates = sim_params.loc['diffusion rate of molecules in micrometer^2 / s'].astype(np.float32)
       nmols = sim_params.loc['number of fast molecules'].astype(np.float32)
       clusters = sim_params.loc['diffusion rate of clusters in micrometer^2 / s'].astype(np.float32)
       sim_columns = [f'{d:.4}' for d, c in zip(
           np.repeat(diffrates, nsamples[0]),
           np.repeat(clusters, nsamples[0]))]

       sim_sep = isfc.separate_data_and_labels(array=sim,
                                               nsamples=nsamples,
                                               col_per_example=col_per_example)
       sim_dirty = sim_sep['0']
       sim_dirty.columns = sim_columns

       sim_labels = sim_sep['1']
       sim_labels.columns = sim_columns
       sim_labbool = sim_labels > lab_thresh
       sim_labbool.columns = sim_columns
       sim_clean = sim_sep['2']
       sim_clean.columns = sim_columns

       sim_dirty
     #+END_SRC

     #+RESULTS:
     :RESULTS:
     |       | 0.069       | 0.069       | 0.069       | 0.069      | 0.069      | 0.069       | 0.069      | 0.069       | 0.069      | 0.069      | ... | 50.0        | 50.0        | 50.0        | 50.0        | 50.0        | 50.0        | 50.0        | 50.0        | 50.0        | 50.0        |
|-------+-------------+-------------+-------------+------------+------------+-------------+------------+-------------+------------+------------+-----+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------|
| 0     | 1187.467896 | 907.677734  | 480.048798  | 454.669403 | 466.063232 | 384.734467  | 543.981323 | 640.921509  | 795.946167 | 410.471893 | ... | 1897.398193 | 2279.646484 | 3088.531006 | 2034.432495 | 2187.548584 | 2105.736084 | 1789.366577 | 2023.093994 | 2331.185791 | 2185.028076 |
| 1     | 1184.055176 | 945.760315  | 471.065216  | 458.392487 | 473.306152 | 395.165527  | 558.603088 | 622.421204  | 776.199402 | 409.149170 | ... | 1499.969849 | 2199.652100 | 3207.333008 | 1650.523926 | 2122.935791 | 2791.281006 | 1661.286377 | 1111.879761 | 1853.699585 | 1926.844971 |
| 2     | 1191.848877 | 980.117798  | 459.479706  | 426.087982 | 482.370209 | 425.123413  | 551.536072 | 624.498535  | 778.671265 | 400.971954 | ... | 1822.985229 | 2456.422607 | 2969.562500 | 1934.118286 | 1457.731812 | 2251.077393 | 1903.003662 | 2063.915527 | 2198.018066 | 2038.683105 |
| 3     | 1199.065918 | 974.313110  | 462.205566  | 444.041809 | 463.703125 | 434.186615  | 573.044128 | 626.252502  | 747.284058 | 393.178162 | ... | 1741.839355 | 2467.149414 | 2588.980957 | 2136.627686 | 1930.263672 | 2323.700684 | 2133.313721 | 1638.169312 | 1926.058716 | 1815.259521 |
| 4     | 1221.957397 | 968.779175  | 464.918030  | 455.205292 | 474.615540 | 437.029419  | 586.489136 | 619.319092  | 781.954102 | 406.468018 | ... | 2431.400879 | 2246.336670 | 3000.961182 | 1915.518066 | 2052.773682 | 2359.145508 | 1699.926147 | 1862.709595 | 2291.338379 | 1332.422241 |
| ...   | ...         | ...         | ...         | ...        | ...        | ...         | ...        | ...         | ...        | ...        | ... | ...         | ...         | ...         | ...         | ...         | ...         | ...         | ...         | ...         | ...         |
| 16379 | 506.409668  | 1012.403931 | 855.006226  | 674.470703 | 769.859192 | 2110.732178 | 799.565247 | 1981.221436 | 528.844604 | 483.055878 | ... | 1512.586548 | 3212.712891 | 1491.119995 | 1843.866943 | 1748.956665 | 2048.602051 | 1662.244385 | 2593.879639 | 1921.427856 | 1664.831909 |
| 16380 | 536.809692  | 1022.029724 | 840.287720  | 671.095215 | 738.908997 | 2118.984863 | 807.995789 | 2624.343262 | 552.687012 | 479.768372 | ... | 1661.331055 | 3190.809570 | 1770.193970 | 2081.854248 | 2164.372803 | 2295.646729 | 1846.683594 | 2038.272339 | 2222.708252 | 2122.753662 |
| 16381 | 570.668884  | 989.891235  | 839.180298  | 689.863586 | 695.739136 | 2033.681885 | 786.547852 | 3528.163574 | 572.166077 | 484.491211 | ... | 1643.470337 | 2564.206787 | 2025.219971 | 2104.706787 | 1792.828613 | 2106.199463 | 2087.914062 | 1457.817871 | 1874.736938 | 1683.072021 |
| 16382 | 562.505310  | 977.029785  | 1005.927673 | 683.250183 | 661.608337 | 2566.123047 | 805.594116 | 3731.086426 | 566.710571 | 489.289673 | ... | 1556.492188 | 2783.619385 | 1312.174561 | 2378.643311 | 2466.965576 | 2160.641357 | 1691.332520 | 2013.095093 | 1632.475708 | 1352.443237 |
| 16383 | 567.307373  | 1006.794067 | 982.376526  | 677.099854 | 657.040588 | 2545.080322 | 784.917969 | 3850.334717 | 570.241699 | 512.688232 | ... | 2127.414551 | 2448.062012 | 1398.359253 | 1665.321167 | 2241.687256 | 1823.699829 | 1340.686035 | 1972.661743 | 1550.770142 | 1727.808228 |

16384 rows × 1500 columns
     :END:

   - I implemented a simple control prediction algorithm:
     1. apply robust scaling to the fluorescence trace
     2. threshold it (trying some variations, )

     #+BEGIN_SRC jupyter-python
       threshold = 2

       sim_pred = pd.DataFrame()
       sim_corr = pd.DataFrame()
       sim_robust = pd.DataFrame()
       for i in range(len(sim_dirty.columns)):
           trace = sim_dirty.iloc[:, i].to_numpy()
           trace_robust = ppd.scale_trace(trace.reshape(-1, 1), 'robust')
           trace_pred = trace_robust.flatten() > threshold
           trace_corr = np.delete(trace, trace_pred)

           trace_corr = pd.DataFrame(trace_corr)
           trace_pred = pd.DataFrame(trace_pred)
           trace_robust = pd.DataFrame(trace_robust)
           sim_corr = pd.concat([sim_corr, trace_corr], axis='columns')
           sim_pred = pd.concat([sim_pred, trace_pred], axis='columns')
           sim_robust = pd.concat([sim_robust, trace_robust], axis='columns')
       sim_corr.columns = sim_dirty.columns
       sim_pred.columns = sim_dirty.columns
       sim_robust.columns = sim_dirty.columns


     #+END_SRC

     #+RESULTS:

     #+BEGIN_SRC jupyter-python

       for i in [4, 100, 200, 400]:
           fig = plt.figure(figsize=(16,9))
           ax1 = plt.subplot(311, title='Original trace')
           ax1.set_prop_cycle(color=[sns.color_palette()[0]])
           sns.lineplot(x=sim_dirty.iloc[:, i].index,
                        y=sim_dirty.iloc[:, i])
           ax1.set_ylabel(r'intensity $[a.u.]$')

           ax2 = plt.subplot(312, title='Scaled trace and prediction', sharex=ax1)
           ax2.set_prop_cycle(color=[sns.color_palette()[0]])
           sns.lineplot(x=sim_robust.iloc[:, i].index,
                        y=sim_robust.iloc[:, i], alpha=0.5)
           ax2.set_prop_cycle(color=[sns.color_palette()[1]])
           sns.lineplot(x=sim_pred.iloc[:, i].index,
                        y=sim_pred.iloc[:, i] * sim_robust.iloc[:, i].max())
           ax2.set_prop_cycle(color=[sns.color_palette()[2]])
           plt.hlines(y=2, xmin=0, xmax=sim_pred.index.max(),
                      ls='--', color=sns.color_palette()[2])
           ax2.set_ylabel(r'scaled intensity $[a.u.]$')

           ax3 = plt.subplot(313, title='Corrected trace by cutandshift', sharex=ax1)
           ax3.set_prop_cycle(color=[sns.color_palette()[0]])
           sns.lineplot(x=sim_corr.iloc[:, i].index,
                        y=sim_corr.iloc[:, i])
           ax3.set_ylabel(r'intensity $[a.u.]$')
           ax3.set_xlabel(r'time steps $[ms]$')

           fig.suptitle(f'Control of robust scaling and thresholding. Molecule speed = {sim_dirty.columns[i]}')
           fig.tight_layout()
           plt.show()
           plt.close('all')



     #+END_SRC

     #+RESULTS:


   - example for 0.069:
     [[file:./data/exp-220316-publication1/jupyter/plot3_0dot069_robust-control-pred.png]]
   - example for 0.2:
     [[file:./data/exp-220316-publication1/jupyter/plot3_0dot2_robust-control-pred.png]]
   - example for 1.0:
     [[file:./data/exp-220316-publication1/jupyter/plot3_1_robust-control-pred.png]]
   - example for 50.0
     [[file:./data/exp-220316-publication1/jupyter/plot3_50_robust-control-pred.png]]

   - now let's save correlations and fit them with FOCUSpoint
     #+begin_src sh
       mkdir /home/lex/Programme/drmed-git/data/exp-220316-publication1/220719_threshold-prediction
     #+end_src

     #+RESULTS:

     #+BEGIN_SRC jupyter-python
       out_path = Path('/home/lex/Programme/drmed-git/data/exp-220316-publication1/220719_threshold-prediction')
       threshold = 2

       for name in set(sim_dirty.columns):
           out_folder = out_path / f'{name}'
           out_txt = f'robust_thresh-{threshold}'
           %mkdir -p $out_folder
           ans.threshold_predict_correct_correlate_simulations(
               sim_dirty.loc[:, name],
               out_path=out_folder,
               out_txt=out_txt,
               threshold=threshold)
     #+END_SRC

     #+RESULTS:
     : 2022-07-21 16:48:51.311194: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
     : 2022-07-21 16:48:51.312823: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)
     : 2022-07-21 16:48:51.314823: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (Topialex): /proc/driver/nvidia/version does not exist
     : 2022-07-21 16:48:51.368332: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
     : To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.

   - another thing I didn't do before is correlate the clean traces, so here it comes
     #+BEGIN_SRC jupyter-python
       out_path = Path('/home/lex/Programme/drmed-git/data/exp-220316-publication1/220719_threshold-prediction/')
       for name in set(sim_clean.columns):
           out_folder = out_path / f'{name}-clean'
           out_txt = 'clean'
           %mkdir -p $out_folder
           correlate.correlate_timetrace_and_save(
               df=sim_clean.loc[:, name],
               out_path=out_folder,
               out_txt=out_txt)

     #+END_SRC

     #+RESULTS:

*** Exp: simexps - failed attempts: ~weight=np.nan correction~ and =modulation filtering=
   :PROPERTIES:
   :header-args:jupyter-python: :session /jpy:localhost#8888:a37e524a-8134-4d8f-b24a-367acaf1bdd3 :pandoc t
   :END:

   - at MAF2022, Thorsten Wohland mentioned my current implementation of
     ~weight=0~ correction might be problematic. Instead of setting the part of
     the trace to 0, I should try to set it to =nan=, and that most correlation
     algorithms would account for these missing values. Let's try that for
     =multipletau= and =tttr2xfcs=

     #+BEGIN_SRC jupyter-python
       %cd ~/Programme/drmed-git
     #+END_SRC

     #+RESULTS:
     : /home/lex/Programme/drmed-git

   - load modules
     #+BEGIN_SRC jupyter-python
       import datetime
       import logging
       import os
       import pdb
       import sys

       import matplotlib.pyplot as plt
       import numpy as np
       import pandas as pd
       import seaborn as sns

       from multipletau import autocorrelate
       from pathlib import Path
       from pprint import pprint

       FLUOTRACIFY_PATH = '/home/lex/Programme/drmed-git/src/'
       sys.path.append(FLUOTRACIFY_PATH)
       from fluotracify.applications import (corr_fit_object as cfo,
                                             correlate)
       from fluotracify.imports import ptu_utils as ptu
       from fluotracify.simulations import (
          import_simulation_from_csv as isfc,
          analyze_simulations as ans,
       )

       logging.basicConfig(filename="/home/lex/Programme/drmed-git/data/exp-220316-publication1/jupyter.log",
                           filemode='w', format='%(asctime)s - %(message)s',
                           force=True,
                           level=logging.DEBUG)

       sns.set_theme(style="whitegrid", font_scale=2, palette='colorblind',
                     context='paper')

     #+END_SRC

     #+RESULTS:

    #+BEGIN_SRC jupyter-python
      import importlib
      importlib.reload(correlate)
      importlib.reload(ans)
    #+END_SRC

    #+RESULTS:
    : <module 'fluotracify.simulations.analyze_simulations' from '/home/lex/Programme/drmed-git/src/fluotracify/simulations/analyze_simulations.py'>

   - load simulated data
     #+BEGIN_SRC jupyter-python :pandoc t
       col_per_example = 3
       lab_thresh = 0.04
       artifact = 0
       model_type = 1
       fwhm = 250
       sim_path = Path('/home/lex/Programme/drmed-collections/drmed-simexps/firstartifact_Nov2020_test')

       sim, _, nsamples, sim_params = isfc.import_from_csv(
           folder=sim_path,
           header=12,
           frac_train=1,
           col_per_example=col_per_example,
           dropindex=None,
           dropcolumns=None)

       diffrates = sim_params.loc['diffusion rate of molecules in micrometer^2 / s'].astype(np.float32)
       nmols = sim_params.loc['number of fast molecules'].astype(np.float32)
       clusters = sim_params.loc['diffusion rate of clusters in micrometer^2 / s'].astype(np.float32)
       sim_columns = [f'{d:.4}' for d, c in zip(
           np.repeat(diffrates, nsamples[0]),
           np.repeat(clusters, nsamples[0]))]

       sim_sep = isfc.separate_data_and_labels(array=sim,
                                               nsamples=nsamples,
                                               col_per_example=col_per_example)
       sim_dirty = sim_sep['0']
       sim_dirty.columns = sim_columns

       sim_labels = sim_sep['1']
       sim_labels.columns = sim_columns
       sim_labbool = sim_labels > lab_thresh
       sim_labbool.columns = sim_columns
       sim_clean = sim_sep['2']
       sim_clean.columns = sim_columns

       sim_dirty
     #+END_SRC

     #+RESULTS:
     :RESULTS:
     |       | 0.069       | 0.069       | 0.069       | 0.069      | 0.069      | 0.069       | 0.069      | 0.069       | 0.069      | 0.069      | ... | 50.0        | 50.0        | 50.0        | 50.0        | 50.0        | 50.0        | 50.0        | 50.0        | 50.0        | 50.0        |
|-------+-------------+-------------+-------------+------------+------------+-------------+------------+-------------+------------+------------+-----+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------|
| 0     | 1187.467896 | 907.677734  | 480.048798  | 454.669403 | 466.063232 | 384.734467  | 543.981323 | 640.921509  | 795.946167 | 410.471893 | ... | 1897.398193 | 2279.646484 | 3088.531006 | 2034.432495 | 2187.548584 | 2105.736084 | 1789.366577 | 2023.093994 | 2331.185791 | 2185.028076 |
| 1     | 1184.055176 | 945.760315  | 471.065216  | 458.392487 | 473.306152 | 395.165527  | 558.603088 | 622.421204  | 776.199402 | 409.149170 | ... | 1499.969849 | 2199.652100 | 3207.333008 | 1650.523926 | 2122.935791 | 2791.281006 | 1661.286377 | 1111.879761 | 1853.699585 | 1926.844971 |
| 2     | 1191.848877 | 980.117798  | 459.479706  | 426.087982 | 482.370209 | 425.123413  | 551.536072 | 624.498535  | 778.671265 | 400.971954 | ... | 1822.985229 | 2456.422607 | 2969.562500 | 1934.118286 | 1457.731812 | 2251.077393 | 1903.003662 | 2063.915527 | 2198.018066 | 2038.683105 |
| 3     | 1199.065918 | 974.313110  | 462.205566  | 444.041809 | 463.703125 | 434.186615  | 573.044128 | 626.252502  | 747.284058 | 393.178162 | ... | 1741.839355 | 2467.149414 | 2588.980957 | 2136.627686 | 1930.263672 | 2323.700684 | 2133.313721 | 1638.169312 | 1926.058716 | 1815.259521 |
| 4     | 1221.957397 | 968.779175  | 464.918030  | 455.205292 | 474.615540 | 437.029419  | 586.489136 | 619.319092  | 781.954102 | 406.468018 | ... | 2431.400879 | 2246.336670 | 3000.961182 | 1915.518066 | 2052.773682 | 2359.145508 | 1699.926147 | 1862.709595 | 2291.338379 | 1332.422241 |
| ...   | ...         | ...         | ...         | ...        | ...        | ...         | ...        | ...         | ...        | ...        | ... | ...         | ...         | ...         | ...         | ...         | ...         | ...         | ...         | ...         | ...         |
| 16379 | 506.409668  | 1012.403931 | 855.006226  | 674.470703 | 769.859192 | 2110.732178 | 799.565247 | 1981.221436 | 528.844604 | 483.055878 | ... | 1512.586548 | 3212.712891 | 1491.119995 | 1843.866943 | 1748.956665 | 2048.602051 | 1662.244385 | 2593.879639 | 1921.427856 | 1664.831909 |
| 16380 | 536.809692  | 1022.029724 | 840.287720  | 671.095215 | 738.908997 | 2118.984863 | 807.995789 | 2624.343262 | 552.687012 | 479.768372 | ... | 1661.331055 | 3190.809570 | 1770.193970 | 2081.854248 | 2164.372803 | 2295.646729 | 1846.683594 | 2038.272339 | 2222.708252 | 2122.753662 |
| 16381 | 570.668884  | 989.891235  | 839.180298  | 689.863586 | 695.739136 | 2033.681885 | 786.547852 | 3528.163574 | 572.166077 | 484.491211 | ... | 1643.470337 | 2564.206787 | 2025.219971 | 2104.706787 | 1792.828613 | 2106.199463 | 2087.914062 | 1457.817871 | 1874.736938 | 1683.072021 |
| 16382 | 562.505310  | 977.029785  | 1005.927673 | 683.250183 | 661.608337 | 2566.123047 | 805.594116 | 3731.086426 | 566.710571 | 489.289673 | ... | 1556.492188 | 2783.619385 | 1312.174561 | 2378.643311 | 2466.965576 | 2160.641357 | 1691.332520 | 2013.095093 | 1632.475708 | 1352.443237 |
| 16383 | 567.307373  | 1006.794067 | 982.376526  | 677.099854 | 657.040588 | 2545.080322 | 784.917969 | 3850.334717 | 570.241699 | 512.688232 | ... | 2127.414551 | 2448.062012 | 1398.359253 | 1665.321167 | 2241.687256 | 1823.699829 | 1340.686035 | 1972.661743 | 1550.770142 | 1727.808228 |

16384 rows × 1500 columns
     :END:


   #+BEGIN_SRC jupyter-python

     sim_nan = pd.DataFrame()
     for i in range(len(sim_dirty.columns)):

         # weight=nan
         sim_nan_trace = np.where(sim_labbool.iloc[:, i].values == 1, np.nan,
                                  sim_dirty.iloc[:, i].values)
         sim_nan_trace = pd.DataFrame(sim_nan_trace)
         sim_nan = pd.concat([sim_nan, sim_nan_trace], axis='columns')
     sim_nan.columns = sim_columns

     # log.debug('label_correct_correlate: Finished "cut and shift" correction.')

     # after correction
     # lab_str = f'{lab_thresh}'.replace(".", "dot")
     # cas_txt = f'labthresh-{lab_str}_cutandshift'
     # del_txt = f'labthresh-{lab_str}_delete'
     # correlate.correlate_timetrace_and_save(df=sim_cas, out_path=out_path, out_txt=cas_txt)
     # correlate.correlate_timetrace_and_save(df=sim_del, out_path=out_path, out_txt=del_txt)
     sim_nan
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   |       | 0.069       | 0.069       | 0.069       | 0.069      | 0.069      | 0.069      | 0.069      | 0.069      | 0.069      | 0.069      | ... | 50.0        | 50.0 | 50.0        | 50.0        | 50.0        | 50.0        | 50.0        | 50.0        | 50.0        | 50.0        |
|-------+-------------+-------------+-------------+------------+------------+------------+------------+------------+------------+------------+-----+-------------+------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------|
| 0     | 1187.467896 | 907.677734  | 480.048798  | 454.669403 | 466.063232 | 384.734467 | 543.981323 | 640.921509 | 795.946167 | 410.471893 | ... | 1897.398193 | NaN  | NaN         | 2034.432495 | 2187.548584 | 2105.736084 | 1789.366577 | 2023.093994 | 2331.185791 | 2185.028076 |
| 1     | 1184.055176 | 945.760315  | 471.065216  | 458.392487 | 473.306152 | 395.165527 | 558.603088 | 622.421204 | 776.199402 | 409.149170 | ... | 1499.969849 | NaN  | NaN         | 1650.523926 | 2122.935791 | 2791.281006 | 1661.286377 | 1111.879761 | 1853.699585 | 1926.844971 |
| 2     | 1191.848877 | 980.117798  | 459.479706  | 426.087982 | 482.370209 | 425.123413 | 551.536072 | 624.498535 | 778.671265 | 400.971954 | ... | 1822.985229 | NaN  | NaN         | 1934.118286 | NaN         | 2251.077393 | 1903.003662 | 2063.915527 | 2198.018066 | 2038.683105 |
| 3     | 1199.065918 | 974.313110  | 462.205566  | 444.041809 | 463.703125 | 434.186615 | 573.044128 | 626.252502 | 747.284058 | 393.178162 | ... | 1741.839355 | NaN  | NaN         | 2136.627686 | NaN         | 2323.700684 | 2133.313721 | 1638.169312 | 1926.058716 | 1815.259521 |
| 4     | 1221.957397 | 968.779175  | 464.918030  | 455.205292 | 474.615540 | 437.029419 | 586.489136 | 619.319092 | 781.954102 | 406.468018 | ... | 2431.400879 | NaN  | NaN         | 1915.518066 | NaN         | 2359.145508 | 1699.926147 | 1862.709595 | 2291.338379 | 1332.422241 |
| ...   | ...         | ...         | ...         | ...        | ...        | ...        | ...        | ...        | ...        | ...        | ... | ...         | ...  | ...         | ...         | ...         | ...         | ...         | ...         | ...         | ...         |
| 16379 | 506.409668  | 1012.403931 | 855.006226  | 674.470703 | 769.859192 | NaN        | 799.565247 | NaN        | 528.844604 | 483.055878 | ... | 1512.586548 | NaN  | 1491.119995 | 1843.866943 | NaN         | 2048.602051 | 1662.244385 | 2593.879639 | 1921.427856 | 1664.831909 |
| 16380 | 536.809692  | 1022.029724 | 840.287720  | 671.095215 | 738.908997 | NaN        | 807.995789 | NaN        | 552.687012 | 479.768372 | ... | 1661.331055 | NaN  | 1770.193970 | 2081.854248 | NaN         | 2295.646729 | 1846.683594 | 2038.272339 | 2222.708252 | 2122.753662 |
| 16381 | 570.668884  | 989.891235  | 839.180298  | 689.863586 | 695.739136 | NaN        | 786.547852 | NaN        | 572.166077 | 484.491211 | ... | 1643.470337 | NaN  | 2025.219971 | 2104.706787 | NaN         | 2106.199463 | 2087.914062 | 1457.817871 | 1874.736938 | 1683.072021 |
| 16382 | 562.505310  | 977.029785  | 1005.927673 | 683.250183 | 661.608337 | NaN        | 805.594116 | NaN        | 566.710571 | 489.289673 | ... | 1556.492188 | NaN  | 1312.174561 | 2378.643311 | NaN         | 2160.641357 | 1691.332520 | 2013.095093 | 1632.475708 | 1352.443237 |
| 16383 | 567.307373  | 1006.794067 | 982.376526  | 677.099854 | 657.040588 | NaN        | 784.917969 | NaN        | 570.241699 | 512.688232 | ... | 2127.414551 | NaN  | 1398.359253 | 1665.321167 | NaN         | 1823.699829 | 1340.686035 | 1972.661743 | 1550.770142 | 1727.808228 |

16384 rows × 1500 columns
   :END:

   #+BEGIN_SRC jupyter-python


     out_path = '/home/lex/Programme/drmed-git/data/exp-220316-publication1/220914_nan-correction'
     # correlate.correlate_timetrace_and_save(df=sim_nan, out_path=out_path, out_txt='nan')

     for idx, col in enumerate(sim_nan.columns):
         trace = sim_nan.iloc[:, idx]
         corr_fn = autocorrelate(
             a=trace,
             m=16,
             deltat=1,
             normalize=True,
             compress='first')

         # try:
         #     corr_fn = autocorrelate(
         #         a=trace,
         #         m=16,
         #         deltat=1,
         #         normalize=True)
             # autotime = corr_fn[1:, 0]
             # autonorm = corr_fn[1:, 1]
             # out_file_txt = f'nan_{col.replace(".", "dot")}'
             # out_file = Path(f'{datetime.date.today()}_multipletau_'
             #                 f'{out_file_txt}_{idx:04}_correlation.csv')
             # out_file = out_path / out_file
             #
             # with open(out_file, 'w', encoding='utf-8') as out:
             #     out.write('version,3.0\n')
             #     out.write('numOfCh,1\n')
             #     out.write('type,point\n')
             #     out.write(f'parent_name,nan\n')
             #     out.write('ch_type,1_1\n')
             #     out.write('kcount,1\n')  # arbitrary value
             #     out.write('numberNandB,1\n')  # arbitrary value
             #     out.write('brightnessNandB,1\n')  # arbitrary value
             #     out.write('carpet pos,0\n')
             #     out.write('pc,0\n')
             #     out.write('Time (ms),CH1 Auto-Correlation\n')
             #     for i in range(autotime.shape[0]):
             #         out.write(f'{autotime[i]},{autonorm[i]}\n')
             #     out.write('end\n')
             # logging.debug('predict_correct_correlate: Finished saving of file %s',
             #           out_file)

         # except KeyError:
         #     logging.debug(f'skipped {col}-{idx}')
         #     continue
         break




   #+END_SRC

   #+RESULTS:


   - *the Python multipletau library does not seem able to handle np.nan*
   - let's quickly try experimental data

   #+BEGIN_SRC jupyter-python
     weight = np.nan
     myweight = float(0) if weight is None else float(weight)

     print(weight, myweight)
   #+END_SRC

   #+RESULTS:
   : nan nan

   - modulation filtering (@persson_modulation_2009). The algorithmic approach
     is to divide the  correlation function of the registered intensity trace
     with that of a modulation (a square wave pattern, in this case
     representing the peak artifacts)
   - since I have time constraints in my project, I will try if the straight
     forward approach
     1. take the dirty trace and the given mask from the simulations
     2. correlate dirty trace and mask trace (0 = no artifact, 1 = artifact)
     3. divide correlation of dirty trace by correlation of mask


   #+BEGIN_SRC jupyter-python


     out_path = '/home/lex/Programme/drmed-git/data/exp-220316-publication1/230102_modulation-filtering'
     # correlate.correlate_timetrace_and_save(df=sim_nan, out_path=out_path, out_txt='nan')

     for idx, col in enumerate(sim_labbool.columns):
         mod_trace = sim_labbool.iloc[:, idx+1].astype(np.float64)
         dirty_trace = sim_dirty.iloc[:, idx+1].astype(np.float64)
         mod_corr = autocorrelate(
             a=mod_trace,
             m=16,
             deltat=1,
             normalize=True)
         dirty_corr = autocorrelate(
             a=dirty_trace,
             m=16,
             deltat=1,
             normalize=False)
         mod_corr[:, 1] += 1
         dirty_corr[:, 1] += 1
         filt_corr_to1 = dirty_corr[1:, 1] / mod_corr[1:, 1]
         filt_corr_to0 = filt_corr_to1 - 1
         fig, ax = plt.subplots(2, 2, figsize=(16, 9))
         sns.lineplot(mod_corr[1:, 0], filt_corr_to1, ax=ax[1, 0]).set(title='filtered, offset=1')
         sns.lineplot(mod_corr[1:, 0], filt_corr_to0, ax=ax[1, 1]).set(title='filtered, offset=0')
         sns.lineplot(mod_corr[1:, 0], mod_corr[1:, 1], ax=ax[0, 1]).set(title='modulation')
         sns.lineplot(dirty_corr[1:, 0], dirty_corr[1:, 1], ax=ax[0, 0]).set(title='dirty')
         plt.setp(ax, xscale='log')
         # plt.setp(ax[1, :], ylim=[-1, 5])
         plt.tight_layout()
         plt.show()

         break




   #+END_SRC

   #+RESULTS:
   :RESULTS:
   : /home/lex/Programme/miniconda3/envs/tf/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.
   :   warnings.warn(
   : /home/lex/Programme/miniconda3/envs/tf/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.
   :   warnings.warn(
   : /home/lex/Programme/miniconda3/envs/tf/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.
   :   warnings.warn(
   : /home/lex/Programme/miniconda3/envs/tf/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.
   :   warnings.warn(
   [[file:./.ob-jupyter/0e82b92ededa286a61e214b31634c4f7d93263e0.png]]
   :END:
   #+RESULTS:

   - it seems like this simple approach does not work since it introduces at
     least huge instabilities in the tail of the correlation curve (and the
     start doesn't look tidy as well). In the publications by persson et al they
     also spoke of an analytical solution to the correlation of the modulation -
     but I have not time to figure this out at the moment

*** Exp: simexps - =correlation averaging=
   :PROPERTIES:
   :header-args:jupyter-python: :session /jpy:localhost#8888:a37e524a-8134-4d8f-b24a-367acaf1bdd3 :pandoc t
   :END:

   - After re-reading a lot of literature and putting it into text
     (<2023-01-02 Mo>), I decided to add the averaging method:
     1. segment trace in artifactual and non-artifactual (here: given by
        simulations)
     2. correlate all non-artifactual segments
     3. average correlations and fit the average

     #+BEGIN_SRC jupyter-python
       %cd ~/Programme/drmed-git
     #+END_SRC

     #+RESULTS:
     : /home/lex/Programme/drmed-git

   - load modules
     #+BEGIN_SRC jupyter-python
       import datetime
       import logging
       import os
       import pdb
       import scipy
       import sys

       import matplotlib.pyplot as plt
       import numpy as np
       import pandas as pd
       import seaborn as sns

       from multipletau import autocorrelate
       from pathlib import Path
       from pprint import pprint

       FLUOTRACIFY_PATH = '/home/lex/Programme/drmed-git/src/'
       sys.path.append(FLUOTRACIFY_PATH)
       from fluotracify.applications import (corr_fit_object as cfo,
                                             correlate)
       from fluotracify.imports import ptu_utils as ptu
       from fluotracify.simulations import (
          import_simulation_from_csv as isfc,
          analyze_simulations as ans,
       )

       logging.basicConfig(filename="/home/lex/Programme/drmed-git/data/exp-220316-publication1/jupyter.log",
                           filemode='w', format='%(asctime)s - %(message)s',
                           force=True,
                           level=logging.DEBUG)

       sns.set_theme(style="whitegrid", font_scale=2, palette='colorblind',
                     context='paper')

     #+END_SRC

     #+RESULTS:

    #+BEGIN_SRC jupyter-python
      import importlib
      importlib.reload(correlate)
      importlib.reload(ans)
    #+END_SRC

    #+RESULTS:
    : <module 'fluotracify.simulations.analyze_simulations' from '/home/lex/Programme/drmed-git/src/fluotracify/simulations/analyze_simulations.py'>

   - load simulated data
     #+BEGIN_SRC jupyter-python :pandoc t
       col_per_example = 3
       lab_thresh = 0.04
       artifact = 0
       model_type = 1
       fwhm = 250
       sim_path = Path('/home/lex/Programme/drmed-collections/drmed-simexps/firstartifact_Nov2020_test')

       sim, _, nsamples, sim_params = isfc.import_from_csv(
           folder=sim_path,
           header=12,
           frac_train=1,
           col_per_example=col_per_example,
           dropindex=None,
           dropcolumns=None)

       diffrates = sim_params.loc['diffusion rate of molecules in micrometer^2 / s'].astype(np.float32)
       nmols = sim_params.loc['number of fast molecules'].astype(np.float32)
       clusters = sim_params.loc['diffusion rate of clusters in micrometer^2 / s'].astype(np.float32)
       sim_columns = [f'{d:.4}' for d, c in zip(
           np.repeat(diffrates, nsamples[0]),
           np.repeat(clusters, nsamples[0]))]

       sim_sep = isfc.separate_data_and_labels(array=sim,
                                               nsamples=nsamples,
                                               col_per_example=col_per_example)
       sim_dirty = sim_sep['0']
       sim_dirty.columns = sim_columns

       sim_labels = sim_sep['1']
       sim_labels.columns = sim_columns
       sim_labbool = sim_labels > lab_thresh
       sim_labbool.columns = sim_columns
       sim_clean = sim_sep['2']
       sim_clean.columns = sim_columns

       sim_dirty
     #+END_SRC

     #+RESULTS:
     :RESULTS:
     |       | 0.069       | 0.069       | 0.069       | 0.069      | 0.069      | 0.069       | 0.069      | 0.069       | 0.069      | 0.069      | ... | 50.0        | 50.0        | 50.0        | 50.0        | 50.0        | 50.0        | 50.0        | 50.0        | 50.0        | 50.0        |
|-------+-------------+-------------+-------------+------------+------------+-------------+------------+-------------+------------+------------+-----+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------|
| 0     | 1187.467896 | 907.677734  | 480.048798  | 454.669403 | 466.063232 | 384.734467  | 543.981323 | 640.921509  | 795.946167 | 410.471893 | ... | 1897.398193 | 2279.646484 | 3088.531006 | 2034.432495 | 2187.548584 | 2105.736084 | 1789.366577 | 2023.093994 | 2331.185791 | 2185.028076 |
| 1     | 1184.055176 | 945.760315  | 471.065216  | 458.392487 | 473.306152 | 395.165527  | 558.603088 | 622.421204  | 776.199402 | 409.149170 | ... | 1499.969849 | 2199.652100 | 3207.333008 | 1650.523926 | 2122.935791 | 2791.281006 | 1661.286377 | 1111.879761 | 1853.699585 | 1926.844971 |
| 2     | 1191.848877 | 980.117798  | 459.479706  | 426.087982 | 482.370209 | 425.123413  | 551.536072 | 624.498535  | 778.671265 | 400.971954 | ... | 1822.985229 | 2456.422607 | 2969.562500 | 1934.118286 | 1457.731812 | 2251.077393 | 1903.003662 | 2063.915527 | 2198.018066 | 2038.683105 |
| 3     | 1199.065918 | 974.313110  | 462.205566  | 444.041809 | 463.703125 | 434.186615  | 573.044128 | 626.252502  | 747.284058 | 393.178162 | ... | 1741.839355 | 2467.149414 | 2588.980957 | 2136.627686 | 1930.263672 | 2323.700684 | 2133.313721 | 1638.169312 | 1926.058716 | 1815.259521 |
| 4     | 1221.957397 | 968.779175  | 464.918030  | 455.205292 | 474.615540 | 437.029419  | 586.489136 | 619.319092  | 781.954102 | 406.468018 | ... | 2431.400879 | 2246.336670 | 3000.961182 | 1915.518066 | 2052.773682 | 2359.145508 | 1699.926147 | 1862.709595 | 2291.338379 | 1332.422241 |
| ...   | ...         | ...         | ...         | ...        | ...        | ...         | ...        | ...         | ...        | ...        | ... | ...         | ...         | ...         | ...         | ...         | ...         | ...         | ...         | ...         | ...         |
| 16379 | 506.409668  | 1012.403931 | 855.006226  | 674.470703 | 769.859192 | 2110.732178 | 799.565247 | 1981.221436 | 528.844604 | 483.055878 | ... | 1512.586548 | 3212.712891 | 1491.119995 | 1843.866943 | 1748.956665 | 2048.602051 | 1662.244385 | 2593.879639 | 1921.427856 | 1664.831909 |
| 16380 | 536.809692  | 1022.029724 | 840.287720  | 671.095215 | 738.908997 | 2118.984863 | 807.995789 | 2624.343262 | 552.687012 | 479.768372 | ... | 1661.331055 | 3190.809570 | 1770.193970 | 2081.854248 | 2164.372803 | 2295.646729 | 1846.683594 | 2038.272339 | 2222.708252 | 2122.753662 |
| 16381 | 570.668884  | 989.891235  | 839.180298  | 689.863586 | 695.739136 | 2033.681885 | 786.547852 | 3528.163574 | 572.166077 | 484.491211 | ... | 1643.470337 | 2564.206787 | 2025.219971 | 2104.706787 | 1792.828613 | 2106.199463 | 2087.914062 | 1457.817871 | 1874.736938 | 1683.072021 |
| 16382 | 562.505310  | 977.029785  | 1005.927673 | 683.250183 | 661.608337 | 2566.123047 | 805.594116 | 3731.086426 | 566.710571 | 489.289673 | ... | 1556.492188 | 2783.619385 | 1312.174561 | 2378.643311 | 2466.965576 | 2160.641357 | 1691.332520 | 2013.095093 | 1632.475708 | 1352.443237 |
| 16383 | 567.307373  | 1006.794067 | 982.376526  | 677.099854 | 657.040588 | 2545.080322 | 784.917969 | 3850.334717 | 570.241699 | 512.688232 | ... | 2127.414551 | 2448.062012 | 1398.359253 | 1665.321167 | 2241.687256 | 1823.699829 | 1340.686035 | 1972.661743 | 1550.770142 | 1727.808228 |

16384 rows × 1500 columns
     :END:

   - let's start implementing the averaging
     #+BEGIN_SRC jupyter-python
       testtrace = [5, 6, 7, 8, 3, 4, 5, 6, 7, 8, 9]
       testsegme = [0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1]
       testlabel = scipy.ndimage.label(testsegme)
       testparts = []
       for u in np.unique(testlabel[0]):
           if u == 0:
               continue
           part = np.where(testlabel[0] == u, testtrace, np.nan)
           part = part[~np.isnan(part)]
           testparts.append(part)
       print(testparts)
     #+END_SRC

     #+RESULTS:
     : [array([7., 8., 3.]), array([6., 7.]), array([9.])]


   #+BEGIN_SRC jupyter-python
     sim_avg = pd.DataFrame()
     for i in range(len(sim_dirty.columns)):
         # sim_labbool gives False (or 0) for parts deemed non-artifactual and
         # True (or 1) for parts deemed artifactual. Take all connected segments
         # with value 0 and give them a distinct label
         sim_segments = scipy.ndimage.label(~sim_labbool.iloc[:, i])
         time_and_corrs = []
         for u in np.unique(sim_segments[0]):
             if u == 0:
                 continue
             # now get intensities for each label and correlate the parts
             part = np.where(sim_segments[0] == u, sim_dirty.iloc[:, i], np.nan)
             part = part[~np.isnan(part)]
             if len(part) > 32:
                 corr_fn = autocorrelate(
                     a=part,
                     m=16,
                     deltat=1,
                     normalize=True)
                 time_and_corrs.append(corr_fn)
         max_autotime = max([len(c[1:, 0]) for c in time_and_corrs])
         autotime = [c[1:, 0] for c in time_and_corrs if len(c[1:, 0]) == max_autotime][0]
         corrs = [c[1:, 1] for c in time_and_corrs]
         # convert to pandas dataframe to easily compute the mean
         corr_df = pd.DataFrame(corrs)
         corr_df.columns = autotime
         sim_avg = pd.concat([sim_avg, corr_df.mean()], axis='columns')
     sim_avg.columns = sim_dirty.columns
   #+END_SRC

   #+RESULTS:

   #+BEGIN_SRC jupyter-python

   #+END_SRC

   #+RESULTS:
   :RESULTS:
   |         | 0.069    | 0.069    | 0.069    | 0.069    | 0.069    | 0.069    | 0.069    | 0.069    | 0.069    | 0.069    | ... | 50.0      | 50.0     | 50.0     | 50.0     | 50.0     | 50.0     | 50.0     | 50.0     | 50.0     | 50.0     |
|---------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+-----+-----------+----------+----------+----------+----------+----------+----------+----------+----------+----------|
| 1.0     | 0.020698 | 0.019622 | 0.017920 | 0.032244 | 0.030221 | 0.025694 | 0.031892 | 0.025930 | 0.016783 | 0.039689 | ... | 0.003613  | 0.004836 | 0.004365 | 0.006933 | 0.003830 | 0.004863 | 0.005000 | 0.003529 | 0.004348 | 0.003922 |
| 2.0     | 0.019363 | 0.018849 | 0.016405 | 0.030617 | 0.029151 | 0.023944 | 0.030052 | 0.024905 | 0.015626 | 0.039011 | ... | 0.001848  | 0.002731 | 0.002170 | 0.004099 | 0.000872 | 0.002357 | 0.001167 | 0.002220 | 0.002760 | 0.001808 |
| 3.0     | 0.018039 | 0.018200 | 0.015337 | 0.029285 | 0.028322 | 0.022457 | 0.028391 | 0.023805 | 0.014568 | 0.038457 | ... | 0.000374  | 0.001714 | 0.002105 | 0.003783 | 0.000867 | 0.002107 | 0.000400 | 0.000521 | 0.002419 | 0.001642 |
| 4.0     | 0.017254 | 0.017494 | 0.014444 | 0.028055 | 0.027430 | 0.021223 | 0.027185 | 0.022827 | 0.013678 | 0.037895 | ... | -0.000046 | 0.001012 | 0.001043 | 0.003233 | 0.000772 | 0.001636 | 0.000235 | 0.001065 | 0.001524 | 0.001051 |
| 5.0     | 0.016354 | 0.016801 | 0.013631 | 0.027047 | 0.026551 | 0.020070 | 0.025930 | 0.021968 | 0.013003 | 0.037378 | ... | -0.000957 | 0.001453 | 0.001591 | 0.001492 | 0.000014 | 0.001743 | 0.000496 | 0.001033 | 0.000598 | 0.000294 |
| ...     | ...      | ...      | ...      | ...      | ...      | ...      | ...      | ...      | ...      | ...      | ... | ...       | ...      | ...      | ...      | ...      | ...      | ...      | ...      | ...      | ...      |
| 10240.0 | NaN      | NaN      | NaN      | NaN      | NaN      | NaN      | NaN      | NaN      | NaN      | NaN      | ... | NaN       | NaN      | NaN      | NaN      | NaN      | NaN      | NaN      | NaN      | NaN      | NaN      |
| 11264.0 | NaN      | NaN      | NaN      | NaN      | NaN      | NaN      | NaN      | NaN      | NaN      | NaN      | ... | NaN       | NaN      | NaN      | NaN      | NaN      | NaN      | NaN      | NaN      | NaN      | NaN      |
| 12288.0 | NaN      | NaN      | NaN      | NaN      | NaN      | NaN      | NaN      | NaN      | NaN      | NaN      | ... | NaN       | NaN      | NaN      | NaN      | NaN      | NaN      | NaN      | NaN      | NaN      | NaN      |
| 13312.0 | NaN      | NaN      | NaN      | NaN      | NaN      | NaN      | NaN      | NaN      | NaN      | NaN      | ... | NaN       | NaN      | NaN      | NaN      | NaN      | NaN      | NaN      | NaN      | NaN      | NaN      |
| 14336.0 | NaN      | NaN      | NaN      | NaN      | NaN      | NaN      | NaN      | NaN      | NaN      | NaN      | ... | NaN       | NaN      | NaN      | NaN      | NaN      | NaN      | NaN      | NaN      | NaN      | NaN      |

94 rows × 1500 columns
   :END:


   #+BEGIN_SRC jupyter-python




   #+END_SRC

   #+RESULTS:

   #+BEGIN_SRC jupyter-python
     x
   #+END_SRC

   #+RESULTS:
   #+begin_example
     1.0       0.020698
     2.0       0.019363
     3.0       0.018039
     4.0       0.017254
     5.0       0.016354
                 ...
     3072.0    0.007225
     3328.0    0.002897
     3584.0   -0.004941
     3840.0    0.007093
     4096.0    0.005923
     Name: 0.069, Length: 80, dtype: float64
   #+end_example

   #+BEGIN_SRC jupyter-python
     plt.figure(figsize=(16, 16))
     for i, _ in enumerate(sim_avg.columns):
         plt.subplot(5, 4, i+1).semilogx(sim_avg.index, sim_avg.iloc[:, i])
         if i > 18:
             break

   #+END_SRC

   #+RESULTS:
   [[file:./.ob-jupyter/54100c1f50753698a7e7a3fe916bfc0c861a06d1.png]]




   #+BEGIN_SRC jupyter-python


     ax = sns.lineplot(x=corr_df.columns, y=corr_df.mean())
     plt.setp(ax, xscale='log')
     ax.fill_between(corr_df.columns, (corr_df.mean() - corr_df.std()).values, (corr_df.mean() + corr_df.std()).values, alpha=0.3)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   : <matplotlib.collections.PolyCollection at 0x7f64d076c5b0>
   [[file:./.ob-jupyter/4f2647201789b4317b735291d3873b994b13439b.png]]
   :END:


   #+begin_src jupyter-python


     out_path = '/home/lex/Programme/drmed-git/data/exp-220316-publication1/230102_modulation-filtering'
     # correlate.correlate_timetrace_and_save(df=sim_nan, out_path=out_path, out_txt='nan')

     for idx, col in enumerate(sim_labbool.columns):
         mod_trace = sim_labbool.iloc[:, idx+2].astype(np.float64)
         dirty_trace = sim_dirty.iloc[:, idx+2].astype(np.float64)
         mod_corr = autocorrelate(
             a=mod_trace,
             m=16,
             deltat=1,
             normalize=True)
         dirty_corr = autocorrelate(
             a=dirty_trace,
             m=16,
             deltat=1,
             normalize=True)
         filt_corr = dirty_corr[1:, 1] / mod_corr[1:, 1]
         fig, ax = plt.subplots(1, 3, figsize=(16, 9))
         sns.lineplot(mod_corr[1:60, 0], filt_corr[:59], ax=ax[0])
         sns.lineplot(mod_corr[1:, 0], mod_corr[1:, 1], ax=ax[1])
         sns.lineplot(dirty_corr[1:, 0], dirty_corr[1:, 1], ax=ax[2])
         plt.setp(ax, xscale='log')
         plt.show()

         break
   #+end_src


   #+RESULTS:

   #+BEGIN_SRC jupyter-python
     ~sim_labbool.iloc[:, i]
   #+END_SRC

   #+RESULTS:
   #+begin_example
     0        True
     1        True
     2        True
     3        True
     4        True
              ...
     16379    True
     16380    True
     16381    True
     16382    True
     16383    True
     Name: 0.069, Length: 16384, dtype: bool
   #+end_example

*** Exp: bioexps - prediction by threshold
    :PROPERTIES:
    :header-args:jupyter-python: :session /jpy:localhost#8889:a37e524a-8134-4d8f-b24a-367acaf1bde3 :pandoc t
    :END:

    #+BEGIN_SRC jupyter-python
      import importlib
      importlib.reload(cfo)
    #+END_SRC

    #+RESULTS:
    : <module 'fluotracify.applications.corr_fit_object' from '/beegfs/ye53nis/drmed-git/src/fluotracify/applications/corr_fit_object.py'>

    #+NAME: prepare-jupyter
    #+BEGIN_SRC jupyter-python :var data_path="/beegfs/ye53nis/data/1911DD_atto+LUVs" :var output_path="/beegfs/ye53nis/drmed-git/data/exp-220316-publication1/220726_bioexps/"
      %cd /beegfs/ye53nis/drmed-git
      import logging
      import os
      import sys

      import matplotlib.pyplot as plt
      import numpy as np
      import pandas as pd
      import seaborn as sns

      from pathlib import Path
      from pprint import pprint

      FLUOTRACIFY_PATH = '/beegfs/ye53nis/drmed-git/src/'
      sys.path.append(FLUOTRACIFY_PATH)
      from fluotracify.applications import corr_fit_object as cfo

      data_path = Path(data_path)
      output_path = Path(output_path)
      %mkdir -p output_path

      log_path = output_path.parent / f'{output_path.name}.log'

      logging.basicConfig(filename=log_path,
                          filemode='w', format='%(asctime)s - %(message)s',
                          force=True)

      log = logging.getLogger(__name__)
      log.setLevel(logging.DEBUG)

      sns.set_theme(style="whitegrid", font_scale=2, palette='colorblind',
                    context='paper')
      class ParameterClass():
          """Stores parameters for correlation """
          def __init__(self):
              # Where the data is stored.
              self.data = []
              self.objectRef = []
              self.subObjectRef = []
              self.colors = ['blue', 'green', 'red', 'cyan', 'magenta',
                             'yellow', 'black']
              self.numOfLoaded = 0
              # very fast from Ncasc ~ 14 onwards
              self.NcascStart = 0
              self.NcascEnd = 30  # 25
              self.Nsub = 6  # 6
              self.photonLifetimeBin = 10  # used for photon decay
              self.photonCountBin = 1  # used for time series

      par_obj = ParameterClass()

      if data_path.name == "1911DD_atto+LUVs":
          ylim_clean = [-0.01, 0.08]
          ylim_dirty = [-0.01, 0.07]
          path_clean1 = data_path / 'clean_ptu_part1/'
          path_clean2 = data_path / 'clean_ptu_part2/'
          path_dirty1 = data_path / 'dirty_ptu_part1/'
          path_dirty2 = data_path / 'dirty_ptu_part2/'
          files_clean1 = [path_clean1 / f for f in os.listdir(path_clean1) if f.endswith('.ptu')]
          files_clean2 = [path_clean2 / f for f in os.listdir(path_clean2) if f.endswith('.ptu')]
          files_dirty1 = [path_dirty1 / f for f in os.listdir(path_dirty1) if f.endswith('.ptu')]
          files_dirty2 = [path_dirty2 / f for f in os.listdir(path_dirty2) if f.endswith('.ptu')]

      if data_path.name == "191113_Pex5_2_structured":
          ylim_clean = [-0.1, 0.8]
          ylim_dirty = [-0.1, 1.2]
          path_clean3 = data_path / 'HsPEX5EGFP 1-100001'
          path_dirty3 = data_path / 'TbPEX5EGFP 1-10002'
          files_clean3 = [path_clean3 / f for f in os.listdir(path_clean3) if f.endswith('.ptu')]
          files_dirty3 = [path_dirty3 / f for f in os.listdir(path_dirty3) if f.endswith('.ptu')]

      def threshold_predict_correct_correlate_ptu(files, pred_method, pred_threshold, correction_method, out_path):
          scaler = 'robust'
          if correction_method == 'delete_and_shift':
              method_str = 'DELSHIFT'
          elif correction_method == 'delete':
              method_str = 'DEL'
          for idx, myfile in enumerate(files):
              ptufile = cfo.PicoObject(myfile, par_obj)
              ptufile.predictTimeSeries(method=pred_method,
                                        scaler=scaler,
                                        threshold=pred_threshold)
              ptufile.correctTCSPC(method=correction_method)
              for key in list(ptufile.trueTimeArr.keys()):
                  if method_str in key:
                      ptufile.get_autocorrelation(method='tttr2xfcs', name=key)

              for m in ['multipletau', 'tttr2xfcs', 'tttr2xfcs_with_weights']:
                  if m in list(ptufile.autoNorm.keys()):
                      for key, item in list(ptufile.autoNorm[m].items()):
                          if method_str in key:
                              ptufile.save_autocorrelation(name=key, method=m,
                                                           output_path=out_path)
              # only for plot_threshold_predict_correct_ptu()
              # return ptufile

      def plot_threshold_predict_correct_ptu(files, pred_threshold, out_dir, ylim):
          ptufile = threshold_predict_correct_correlate_ptu(
              files=files,
              pred_method='threshold',
              pred_threshold=pred_threshold,
              correction_method='delete_and_shift',
              out_path=out_dir)

          fig = plt.figure(figsize=(10, 15), constrained_layout=True, facecolor='white')
          gs = fig.add_gridspec(6, 2)
          ax1 = fig.add_subplot(gs[0, :])
          ax1.plot(ptufile.timeSeries[f'{ptufile.name}']['CH2_BIN1.0'])
          ax1.set_title('original trace')
          ax2 = fig.add_subplot(gs[1, :], sharex=ax1)
          ax2.plot(ptufile.timeSeries[f'{ptufile.name}']['CH2_BIN1.0_PREPRO'])
          ax2.set_title('preprocessed trace after robust scaling')
          ax3 = fig.add_subplot(gs[2, :], sharex=ax1)
          ax3.plot(ptufile.predictions[f'{ptufile.name}']['CH2_BIN1.0'])
          ax3.set_title(f'predictions after threshold={pred_threshold} on preprocessed trace')
          ax4 = fig.add_subplot(gs[3, :], sharex=ax1)
          ax4.plot(ptufile.timeSeries[f'{ptufile.name}_DELSHIFT']['CH2_BIN1.0'])
          ax4.set_title('trace after "cut and shift" correction')
          ax4.set_xlabel(r'timesteps in $[ms]$')
          ax5 = fig.add_subplot(gs[4:, 0])
          ax5.plot(ptufile.autotime['tttr2xfcs'][f'CH2_BIN1.0_{ptufile.name}_DELSHIFT'].flatten(),
                   ptufile.autoNorm['tttr2xfcs'][f'CH2_BIN1.0_{ptufile.name}_DELSHIFT'].flatten(), ls=':', lw=3)
          ax5.set_xlim([0.001, 1000])
          ax5.set_ylim(ylim)
          plt.setp(ax5, xscale='log', title='correlation after correction',
                   xlabel=r'$\tau [ms]$', ylabel=r'Correlation $G(\tau)$')
          plt.setp([ax1.get_xticklabels(), ax2.get_xticklabels(), ax3.get_xticklabels()],
                   visible=False)
          plt.setp([ax1, ax2, ax3, ax4], ylabel=r'intensity $[a.u.]$')
          fig.align_labels()
          plt.show()
    #+END_SRC

    #+RESULTS: prepare-jupyter
    : /beegfs/ye53nis/drmed-git

    #+NAME: kill-jupyter
    #+BEGIN_SRC jupyter-python
      os._exit(00)
    #+END_SRC

    #+RESULTS: kill-jupyter
    : 5d59a92a-218e-4749-87b0-fed81557d851

    #+RESULTS:
    : 7a752b61-60c1-4322-8105-e8dbd705caa0

    #+CALL: prepare-jupyter()

    #+RESULTS:
    : /beegfs/ye53nis/drmed-git
    : 2022-07-26 15:34:25.807464: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
    : 2022-07-26 15:34:25.807515: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.

    - first, plot an example of each experimental dataset
      #+begin_src jupyter-python
        out_dir = output_path / f'af488+luvs'
        pred_method = 'threshold'
        correction_method = 'delete_and_shift'
        threshold_ls = [0.8, 1, 1.5, 2, 2.5]
        os.makedirs(out_dir, exist_ok=True)
        for thr in threshold_ls:
            plot_threshold_predict_correct_ptu(files=files_dirty1, pred_threshold=thr,
                                               out_dir=out_dir, ylim=ylim_dirty)

      #+end_src

      #+RESULTS:
      [[file:./.ob-jupyter/63ddc3edeabec8e80d517b09498bce72fa60a4f4.png]]
      [[file:./.ob-jupyter/f1110c313fbec347c280e131b5a9cdcc6568c373.png]]
      [[file:./.ob-jupyter/76d679e591e1d48c72216c2e4737f4a16a3ee80e.png]]
      [[file:./.ob-jupyter/05a836c7ec65e3890e18b8306e3d1600953c527f.png]]
      [[file:./.ob-jupyter/09ba93ddbd1c679e46e9058bced71eebf2721bdf.png]]

      #+BEGIN_SRC jupyter-python
        for thr in threshold_ls:
            plot_threshold_predict_correct_ptu(files=files_clean1, pred_threshold=thr,
                                               out_dir=out_dir, ylim=ylim_clean)
      #+END_SRC

      #+RESULTS:
      [[file:./.ob-jupyter/63cd549f8af483d37b073ab07468667458749ce3.png]]
      [[file:./.ob-jupyter/0690dbefb5b5ee9c800e70dd804900d0d4d74627.png]]
      [[file:./.ob-jupyter/2437c5e7376e2c42974f7e3b2d30e4876a9c788b.png]]
      [[file:./.ob-jupyter/5038e620d42638046ce0838bda60b8429465c971.png]]
      [[file:./.ob-jupyter/37cf9c3a5a73239128e1a6367c038e5067c1d65d.png]]

   - now, let's predict, correct, and correlate all the experimental traces. I
     used 3 different compute nodes to make the process faster. Because we have
     some memory allocation issue, I restart the jupyter kernel after each new
     threshold for each dataset. First, we start with the =AlexaFluor488+LUVs=
     data
     #+BEGIN_SRC jupyter-python
       # threshold[0]: node1
       # threshold[1]: node2
       # threshold[2]: node3
       # threshold[3]: node2
       # threshold[4]: node3
       for thr in threshold_ls:
           out_folder = out_dir / f'robust_thresh-{thr}'
           %mkdir -p $out_folder
           threshold_predict_correct_correlate_ptu(
               files=files_dirty1,
               pred_method=pred_method,
               pred_threshold=thr,
               correction_method=correction_method,
               out_path=out_folder)
           break
     #+END_SRC

     #+RESULTS:
     : 2022-07-26 15:38:12.573339: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
     : 2022-07-26 15:38:12.573387: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)
     : 2022-07-26 15:38:12.573411: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (node095): /proc/driver/nvidia/version does not exist
     : 2022-07-26 15:38:12.574336: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
     : To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
   - =#+CALL: kill-jupyter()=
     #+CALL: kill-jupyter()

   - =#+CALL: prepare-jupyter()=
     #+CALL: prepare-jupyter()

     #+RESULTS:
     : /beegfs/ye53nis/drmed-git
     : 2022-07-27 10:44:51.957570: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
     : 2022-07-27 10:44:51.957621: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.

     #+BEGIN_SRC jupyter-python
       # threshold[0]: node1
       # threshold[1]: node1
       # threshold[2]: node2
       # threshold[3]: node3
       # threshold[4]: node1  # just 431
       out_dir = output_path / f'af488+luvs'
       pred_method = 'threshold'
       correction_method = 'delete_and_shift'
       threshold_ls = [0.8, 1, 1.5, 2, 2.5]

       for thr in threshold_ls[4:]:
           out_folder = out_dir / f'robust_thresh-{thr}'
           # %mkdir -p $out_folder
           threshold_predict_correct_correlate_ptu(
               files=files_dirty2,
               pred_method=pred_method,
               pred_threshold=thr,
               correction_method=correction_method,
               out_path=out_folder)
           break
     #+END_SRC

     #+RESULTS:
     : 2022-07-27 00:18:27.343506: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
     : 2022-07-27 00:18:27.343564: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)
     : 2022-07-27 00:18:27.343589: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (node095): /proc/driver/nvidia/version does not exist
     : 2022-07-27 00:18:27.344160: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
     : To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.

   - =#+CALL: kill-jupyter()=
     #+CALL: kill-jupyter()

   - now, let's do =AlexaFluor488= (clean) data
   - =#+CALL: prepare-jupyter()=
     #+CALL: prepare-jupyter()

     #+BEGIN_SRC jupyter-python
       # threshold[0]: node2
       # threshold[1]: node3
       # threshold[2]: node2
       # threshold[3]: node3
       # threshold[4]: node1
       out_dir = output_path / f'af488'
       pred_method = 'threshold'
       correction_method = 'delete_and_shift'
       threshold_ls = [0.8, 1, 1.5, 2, 2.5]

       for thr in threshold_ls[4:]:
           out_folder = out_dir / f'robust_thresh-{thr}'
           %mkdir -p $out_folder
           threshold_predict_correct_correlate_ptu(
               files=files_clean1,
               pred_method=pred_method,
               pred_threshold=thr,
               correction_method=correction_method,
               out_path=out_folder)
           break
     #+END_SRC

     #+RESULTS:
     : 2022-07-27 03:54:07.972324: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
     : 2022-07-27 03:54:07.972364: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)
     : 2022-07-27 03:54:07.972390: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (node095): /proc/driver/nvidia/version does not exist
     : 2022-07-27 03:54:07.972933: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
     : To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.

   - =#+CALL: kill-jupyter()=
     #+CALL: kill-jupyter()

   - =#+CALL: prepare-jupyter()=
     #+CALL: prepare-jupyter()

     #+BEGIN_SRC jupyter-python
       # threshold[0]: node2  # just 420
       # threshold[1]: node3
       # threshold[2]: node1  # just 422
       # threshold[3]: node3
       # threshold[4]: node3
       out_dir = output_path / f'af488'
       pred_method = 'threshold'
       correction_method = 'delete_and_shift'
       threshold_ls = [0.8, 1, 1.5, 2, 2.5]

       for thr in threshold_ls[2:]:
           out_folder = out_dir / f'robust_thresh-{thr}'
           # %mkdir -p $out_folder
           threshold_predict_correct_correlate_ptu(
               files=files_clean2,
               pred_method=pred_method,
               pred_threshold=thr,
               correction_method=correction_method,
               out_path=out_folder)
           break
     #+END_SRC

     #+RESULTS:
     : 2022-07-27 10:45:14.050433: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
     : 2022-07-27 10:45:14.050526: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)
     : 2022-07-27 10:45:14.050552: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (node095): /proc/driver/nvidia/version does not exist
     : 2022-07-27 10:45:14.051164: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
     : To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.

   - =#+CALL: kill-jupyter()=
     #+CALL: kill-jupyter()

     #+RESULTS:
     :RESULTS:
     # [goto error]
     : ---------------------------------------------------------------------------
     : NameError                                 Traceback (most recent call last)
     : Input In [1], in <cell line: 1>()
     : ----> 1 os._exit(00)
     :
     : NameError: name 'os' is not defined
     :END:

   - now, let's do =human pex5= data. First, we plot again example plots
   - ~#+CALL: prepare-jupyter(data_path="/beegfs/ye53nis/data/191113_Pex5_2_structured")~
     #+CALL: prepare-jupyter(data_path="/beegfs/ye53nis/data/191113_Pex5_2_structured")

     #+RESULTS:
     : /beegfs/ye53nis/drmed-git
     : 2022-07-27 12:50:42.718757: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
     : 2022-07-27 12:50:42.718819: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.

     #+BEGIN_SRC jupyter-python
       threshold_ls = [5, 7, 10]
       for thr in threshold_ls:
           plot_threshold_predict_correct_ptu(files=files_clean3, pred_threshold=thr, out_dir=out_dir, ylim=ylim_clean)
     #+END_SRC

     #+RESULTS:
     file:./.ob-jupyter/96e1a3c007c7ba2d018e150f010b93d1baa441a4.png]]
     file:./.ob-jupyter/d2c86ef12fdc2ed934b289802fa11bc9204e53cc.png]]
     file:./.ob-jupyter/2a46b71a87306551dfb3484f5682fd3ea6a9b28f.png]]

     #+BEGIN_SRC jupyter-python
       threshold_ls = [5, 7, 10]
       for thr in threshold_ls:
           plot_threshold_predict_correct_ptu(files=files_dirty3[5:], pred_threshold=thr, out_dir=out_dir, ylim=ylim_dirty)
     #+END_SRC

     #+RESULTS:
     file:./.ob-jupyter/4442aa7245756ab4ed4f5c49688cb540b3a4a0e0.png]]
     file:./.ob-jupyter/2b473a234641c2cc76d53895fab83408dec95aad.png]]
     file:./.ob-jupyter/5d0b5f676cf73c2df7e72bc211be1429088ba01a.png]]

     #+BEGIN_SRC jupyter-python
       # threshold[0]:
       # threshold[1]:
       # threshold[2]: node1
       out_dir = output_path / 'Hs-PEX5-eGFP'
       pred_method = 'threshold'
       correction_method = 'delete_and_shift'
       threshold_ls = [5, 7, 10]

       for thr in threshold_ls[:2]:
           out_folder = out_dir / f'robust_thresh-{thr}'
           %mkdir -p $out_folder
           threshold_predict_correct_correlate_ptu(
               files=files_clean3,
               pred_method=pred_method,
               pred_threshold=thr,
               correction_method=correction_method,
               out_path=out_folder)
     #+END_SRC

     #+RESULTS:


     #+BEGIN_SRC jupyter-python
       # threshold[0]: node2
       # threshold[1]: node2
       # threshold[2]: node2
       out_dir = output_path / 'Tb-PEX5-eGFP'
       pred_method = 'threshold'
       correction_method = 'delete_and_shift'
       threshold_ls = [5, 7, 10]

       for thr in threshold_ls[2:]:
           out_folder = out_dir / f'robust_thresh-{thr}'
           %mkdir -p $out_folder
           threshold_predict_correct_correlate_ptu(
               files=files_dirty3,
               pred_method=pred_method,
               pred_threshold=thr,
               correction_method=correction_method,
               out_path=out_folder)
     #+END_SRC
*** Exp: bioexps - example traces
   :PROPERTIES:
   :header-args:jupyter-python: :session /jpy:localhost#8888:a37e524a-8134-4d8f-b24a-367acaf1bdd3 :pandoc t
   :END:
   - to interprete the correlations correctly, let's plot the underlying
     experimental data.
     #+BEGIN_SRC jupyter-python
       %cd ~/Programme/drmed-git
     #+END_SRC

     #+RESULTS:
     : /home/lex/Programme/drmed-git

   - load modules
     #+BEGIN_SRC jupyter-python
       import logging
       import os
       import sys

       import matplotlib.pyplot as plt
       import numpy as np
       import pandas as pd
       import seaborn as sns

       from pathlib import Path
       from pprint import pprint
       from tensorflow.keras.optimizers import Adam
       from mlflow.keras import load_model

       FLUOTRACIFY_PATH = '/home/lex/Programme/drmed-git/src/'
       sys.path.append(FLUOTRACIFY_PATH)
       from fluotracify.applications import corr_fit_object as cfo
       from fluotracify.imports import ptu_utils as ptu
       from fluotracify.training import (build_model as bm,
                                         preprocess_data as ppd)
       from fluotracify.simulations import (
          import_simulation_from_csv as isfc,
          analyze_simulations as ans,
       )

       logging.basicConfig(filename="/home/lex/Programme/drmed-git/data/exp-220316-publication1/jupyter.log",
                           filemode='w', format='%(asctime)s - %(message)s',
                           force=True,
                           level=logging.DEBUG)

       sns.set_theme(style="whitegrid", font_scale=2, palette='colorblind',
                     context='paper')

       model_ls = ['ff67be0b68e540a9a29a36a2d0c7a5be', '347669d050f344ad9fb9e480c814f727',
                   '714af8cd12c1441eac4ca980e8c20070', '34a6d207ac594035b1009c330fb67a65']

       model_name_ls = ['ff67b', '34766', '714af', '34a6d']

       scaler_ls = ['minmax', 'robust', 'maxabs', 'l2']


     #+END_SRC

     #+RESULTS:

    #+BEGIN_SRC jupyter-python
      import importlib
      importlib.reload(ppd)
      importlib.reload(isfc)
      importlib.reload(cfo)
    #+END_SRC

    #+RESULTS:
    : <module 'fluotracify.applications.corr_fit_object' from '/home/lex/Programme/drmed-git/src/fluotracify/applications/corr_fit_object.py'>

   - first, we prepare our correction functions as we did before
     #+BEGIN_SRC jupyter-python
       data_path = Path("/home/lex/Programme/drmed-collections/drmed-bioexps/brightbursts/")
       af488_path = data_path / '1911DD_alexafluor488+LUVs/clean_subsample/'
       af488luv_path = data_path / '1911DD_alexafluor488+LUVs/dirty_subsample/'
       hspex5_path = data_path / '191113_Pex5_2_structured/HsPEX5EGFP_1-100001_3of250'
       tbpex5_path = data_path / '191113_Pex5_2_structured/TbPEX5EGFP_1-10002_3of250'
       output_path = Path("data/exp-220316-publication1/220323_bioexps")

       def get_traces_and_predictions_from_ptu(path, model_id, output_path, pred_method='unet'):
           class ParameterClass():
               """Stores parameters for correlation """
               def __init__(self):
                   self.data = []
                   self.objectRef = []
                   self.numOfLoaded = 0
                   self.colors = ['blue', 'green', 'red', 'cyan', 'magenta',
                       'yellow', 'black']
                   # very fast from Ncasc ~ 14 onwards
                   self.NcascStart = 0
                   self.NcascEnd = 30  # 25
                   self.Nsub = 6  # 6
                   self.photonLifetimeBin = 10  # used for photon decay
                   self.photonCountBin = 1  # used for time series

           par_obj = ParameterClass()

           if pred_method == 'unet':
               scaler = scaler_ls[model_id]
               logged_model = Path(f'/home/lex/Programme/drmed-git/data/mlruns/10/{model_ls[model_id]}/artifacts/model')
               loaded_model = load_model(logged_model, compile=False)
               loaded_model.compile(loss=bm.binary_ce_dice_loss(),
                                    optimizer=Adam(),
                                    metrics = bm.unet_metrics([0.1, 0.3, 0.5, 0.7, 0.9]))
           elif pred_method == 'threshold':
               scaler = 'robust'
               threshold = 7
           else:
               raise ValueError('pred_method has to be "unet" or "threshold"')

           files = [path / f for f in os.listdir(path) if f.endswith('.ptu')]
           traces = pd.DataFrame()
           predtraces = pd.DataFrame()
           preds = pd.DataFrame()
           corrtraces = pd.DataFrame()

           for myfile in (files):
               ptufile = cfo.PicoObject(myfile, par_obj)
               if pred_method == 'unet':
                   ptufile.predictTimeSeries(method=pred_method,
                                             scaler=scaler,
                                             model=loaded_model)
               elif pred_method == 'threshold':
                   ptufile.predictTimeSeries(method=pred_method,
                                             scaler=scaler,
                                             threshold=threshold)
               ptufile.correctTCSPC(method='delete_and_shift')

               for key in list(ptufile.trueTimeArr.keys()):
                   ptufile.get_autocorrelation(method='tttr2xfcs', name=key)
               for key in list(ptufile.timeSeries.keys()):
                   if "DELSHIFT" in key:
                       for k, i in ptufile.timeSeries[key].items():
                           if "1.0" in k:
                               corrtraces = pd.concat([corrtraces, pd.DataFrame(
                                   i, columns=[f'{key}_{k}'])],
                                                      axis='columns')
                   else:
                       for k, i in ptufile.timeSeries[key].items():
                           if "PREPRO" in k:
                               if "1.0" in k:
                                   predtraces = pd.concat([predtraces, pd.DataFrame(
                                       i, columns=[f'{key}_{k}'])],
                                                          axis='columns')
                           elif "1.0" in k:
                               traces = pd.concat([traces, pd.DataFrame(
                                   i, columns=[f'{key}_{k}'])],
                                                  axis='columns')
                               preds = pd.concat([preds, pd.DataFrame(
                                   data=ptufile.predictions[key][k],
                                   columns=[f'{key}_{k}'])],
                                                 axis='columns')

               for m in ['multipletau', 'tttr2xfcs', 'tttr2xfcs_with_weights']:
                   if m in list(ptufile.autoNorm.keys()):
                       for key, item in list(ptufile.autoNorm[m].items()):
                           ptufile.save_autocorrelation(name=key, method=m,
                                                        output_path=output_path)
           predtraces.to_csv(Path(output_path) / f'{path.name}_predtraces.csv')
           traces.to_csv(Path(output_path) / f'{path.name}_traces.csv')
           preds.to_csv(Path(output_path) / f'{path.name}_preds.csv')
           corrtraces.to_csv(Path(output_path) / f'{path.name}_corrtraces.csv')

     #+END_SRC

     #+RESULTS:

     #+BEGIN_SRC jupyter-python
       model_id = 0
       ptufile = get_traces_and_predictions_from_ptu(af488luv_path, model_id, output_path)
     #+END_SRC

     #+RESULTS:
     : WARNING:tensorflow:Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.

     #+BEGIN_SRC jupyter-python
       model_id = 0
       get_traces_and_predictions_from_ptu(af488_path, model_id, output_path)
     #+END_SRC

     #+RESULTS:
     : WARNING:tensorflow:Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.

     #+BEGIN_SRC jupyter-python
       pred_method = 'threshold'
       get_traces_and_predictions_from_ptu(hspex5_path, model_id, output_path, pred_method=pred_method)

     #+END_SRC

     #+RESULTS:
     : WARNING:tensorflow:Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.

     #+BEGIN_SRC jupyter-python
       pred_method = 'threshold'
       get_traces_and_predictions_from_ptu(tbpex5_path, model_id, output_path, pred_method=pred_method)
     #+END_SRC

     #+RESULTS:
     : WARNING:tensorflow:Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.


     #+BEGIN_SRC jupyter-python
       key = 'DiO LUV 10uM in 20 nM AF48889_T1067s_1'
       key2 = 'DiO LUV 10uM in 20 nM AF48889_T1067s_1_DELSHIFT'
       tt_key = 'CH2_DiO LUV 10uM in 20 nM AF48889_T1067s_1_CORRECTED'
       ts_key = 'CH2_BIN1.0'
       ts = ptufile.timeSeries[key2][ts_key]
       # ptufile.predictions['20 nM AF48897_T1160s_1']['CH2_BIN1.0']
       ptufile.timeSeriesSize[key2][ts_key]
       pprint(ptufile.trueTimeArr.keys())
       pprint(ptufile.kcount.keys())
       pprint(ptufile.timeSeries.keys())
       pprint(ptufile.autoNorm['tttr2xfcs'].keys())
       for n in ptufile.trueTimeArr.keys():
           if 'DiO LUV 10uM in 20 nM AF48889_T1067s_1_DELSHIFT' in n:
               print(n)
     #+END_SRC

     #+RESULTS:
     : dict_keys(['DiO LUV 10uM in 20 nM AF48889_T1067s_1', 'CH2_DiO LUV 10uM in 20 nM AF48889_T1067s_1_DELSHIFT'])
     : dict_keys(['DiO LUV 10uM in 20 nM AF48889_T1067s_1', 'DiO LUV 10uM in 20 nM AF48889_T1067s_1_DELSHIFT'])
     : dict_keys(['DiO LUV 10uM in 20 nM AF48889_T1067s_1', 'DiO LUV 10uM in 20 nM AF48889_T1067s_1_DELSHIFT'])
     : dict_keys(['CH2_BIN1.0_DiO LUV 10uM in 20 nM AF48889_T1067s_1', 'CH2_BIN1.0_DiO LUV 10uM in 20 nM AF48889_T1067s_1_DELSHIFT'])
     : CH2_DiO LUV 10uM in 20 nM AF48889_T1067s_1_DELSHIFT
     'CH2_DiO LUV 10uM in 20 nM AF48889_T1067s_1_CORRECTED'])
     : dict_keys(['DiO LUV 10uM in 20 nM AF48889_T1067s_1',
      'DiO LUV 10uM in 20 nM AF48889_T1067s_1_DELSHIFT'])

*** Plot 1: Pipeline
   - Done with inkscape
*** Plot 2: Simulations
   :PROPERTIES:
   :header-args:jupyter-python: :session /jpy:localhost#8888:a37e524a-8134-4d8f-b24a-367acaf1bdd3 :pandoc t
   :END:

   - load modules
     #+NAME: simulations-prepare-modules
     #+BEGIN_SRC jupyter-python
       %cd ~/Programme/drmed-git

       import logging
       import os
       import sys

       import matplotlib.pyplot as plt
       import numpy as np
       import pandas as pd
       import seaborn as sns

       from mlflow.keras import load_model
       from pathlib import Path
       from pprint import pprint
       from sklearn.preprocessing import MaxAbsScaler
       from tensorflow.keras.optimizers import Adam



       FLUOTRACIFY_PATH = '/home/lex/Programme/drmed-git/src/'
       sys.path.append(FLUOTRACIFY_PATH)
       from fluotracify.applications import corr_fit_object as cfo
       from fluotracify.imports import ptu_utils as ptu
       from fluotracify.training import (build_model as bm,
                                         preprocess_data as ppd)
       from fluotracify.simulations import (
          import_simulation_from_csv as isfc,
          analyze_simulations as ans,
       )

       logging.basicConfig(filename="/home/lex/Programme/drmed-git/data/exp-220316-publication1/jupyter.log",
                           filemode='w', format='%(asctime)s - %(message)s',
                           force=True,
                           level=logging.DEBUG)

       sns.set_theme(style="whitegrid", font_scale=2, palette='colorblind',
                     context='paper')

       model_ls = ['ff67be0b68e540a9a29a36a2d0c7a5be', '347669d050f344ad9fb9e480c814f727',
                   '714af8cd12c1441eac4ca980e8c20070', '34a6d207ac594035b1009c330fb67a65',
                   '484af471c61943fa90e5f78e78a229f0', '0cd2023eeaf745aca0d3e8ad5e1fc653',
                   'fe81d71c52404ed790b3a32051258da9', '19e3e786e1bc4e2b93856f5dc9de8216',
                   'c1204e3a8a1e4c40a35b5b7b1922d1ce']

       model_name_ls = [f'{s:.5}' for s in model_ls]

       pred_thresh = 0.5

     #+END_SRC

     #+RESULTS: simulations-prepare-modules
     : /home/lex/Programme/drmed-git
     : 2023-02-10 12:42:18.096925: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
     : 2023-02-10 12:42:18.096952: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.

     #+RESULTS:

   - load simulated data
     #+NAME: simulations-prepare-data
     #+BEGIN_SRC jupyter-python :pandoc t
       col_per_example = 3
       lab_thresh = 0.04
       artifact = 0
       model_type = 1
       fwhm = 250
       sim_path = Path('/home/lex/Programme/drmed-collections/drmed-simexps/firstartifact_Nov2020_test')

       sim, _, nsamples, sim_params = isfc.import_from_csv(
           folder=sim_path,
           header=12,
           frac_train=1,
           col_per_example=col_per_example,
           dropindex=None,
           dropcolumns=None)

       diffrates = sim_params.loc['diffusion rate of molecules in micrometer^2 / s'].astype(np.float32)
       nmols = sim_params.loc['number of fast molecules'].astype(np.float32)
       clusters = sim_params.loc['diffusion rate of clusters in micrometer^2 / s'].astype(np.float32)
       sim_columns = [f'{d:.4}-{c:.4}' for d, c in zip(
           np.repeat(diffrates, nsamples[0]),
           np.repeat(clusters, nsamples[0]))]

       sim_sep = isfc.separate_data_and_labels(array=sim,
                                               nsamples=nsamples,
                                               col_per_example=col_per_example)
       sim_dirty = sim_sep['0']
       sim_dirty.columns = sim_columns

       sim_labels = sim_sep['1']
       sim_labels.columns = sim_columns
       sim_labbool = sim_labels > lab_thresh
       sim_labbool.columns = sim_columns
       sim_clean = sim_sep['2']
       sim_clean.columns = sim_columns

       sim_dirty
     #+END_SRC

     #+RESULTS:
     :RESULTS:
     |       | 0.069-0.1   | 0.069-0.1   | 0.069-0.1   | 0.069-0.1  | 0.069-0.1  | 0.069-0.1   | 0.069-0.1  | 0.069-0.1   | 0.069-0.1  | 0.069-0.1  | ... | 50.0-0.01   | 50.0-0.01   | 50.0-0.01   | 50.0-0.01   | 50.0-0.01   | 50.0-0.01   | 50.0-0.01   | 50.0-0.01   | 50.0-0.01   | 50.0-0.01   |
|-------+-------------+-------------+-------------+------------+------------+-------------+------------+-------------+------------+------------+-----+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------|
| 0     | 1187.467896 | 907.677734  | 480.048798  | 454.669403 | 466.063232 | 384.734467  | 543.981323 | 640.921509  | 795.946167 | 410.471893 | ... | 1897.398193 | 2279.646484 | 3088.531006 | 2034.432495 | 2187.548584 | 2105.736084 | 1789.366577 | 2023.093994 | 2331.185791 | 2185.028076 |
| 1     | 1184.055176 | 945.760315  | 471.065216  | 458.392487 | 473.306152 | 395.165527  | 558.603088 | 622.421204  | 776.199402 | 409.149170 | ... | 1499.969849 | 2199.652100 | 3207.333008 | 1650.523926 | 2122.935791 | 2791.281006 | 1661.286377 | 1111.879761 | 1853.699585 | 1926.844971 |
| 2     | 1191.848877 | 980.117798  | 459.479706  | 426.087982 | 482.370209 | 425.123413  | 551.536072 | 624.498535  | 778.671265 | 400.971954 | ... | 1822.985229 | 2456.422607 | 2969.562500 | 1934.118286 | 1457.731812 | 2251.077393 | 1903.003662 | 2063.915527 | 2198.018066 | 2038.683105 |
| 3     | 1199.065918 | 974.313110  | 462.205566  | 444.041809 | 463.703125 | 434.186615  | 573.044128 | 626.252502  | 747.284058 | 393.178162 | ... | 1741.839355 | 2467.149414 | 2588.980957 | 2136.627686 | 1930.263672 | 2323.700684 | 2133.313721 | 1638.169312 | 1926.058716 | 1815.259521 |
| 4     | 1221.957397 | 968.779175  | 464.918030  | 455.205292 | 474.615540 | 437.029419  | 586.489136 | 619.319092  | 781.954102 | 406.468018 | ... | 2431.400879 | 2246.336670 | 3000.961182 | 1915.518066 | 2052.773682 | 2359.145508 | 1699.926147 | 1862.709595 | 2291.338379 | 1332.422241 |
| ...   | ...         | ...         | ...         | ...        | ...        | ...         | ...        | ...         | ...        | ...        | ... | ...         | ...         | ...         | ...         | ...         | ...         | ...         | ...         | ...         | ...         |
| 16379 | 506.409668  | 1012.403931 | 855.006226  | 674.470703 | 769.859192 | 2110.732178 | 799.565247 | 1981.221436 | 528.844604 | 483.055878 | ... | 1512.586548 | 3212.712891 | 1491.119995 | 1843.866943 | 1748.956665 | 2048.602051 | 1662.244385 | 2593.879639 | 1921.427856 | 1664.831909 |
| 16380 | 536.809692  | 1022.029724 | 840.287720  | 671.095215 | 738.908997 | 2118.984863 | 807.995789 | 2624.343262 | 552.687012 | 479.768372 | ... | 1661.331055 | 3190.809570 | 1770.193970 | 2081.854248 | 2164.372803 | 2295.646729 | 1846.683594 | 2038.272339 | 2222.708252 | 2122.753662 |
| 16381 | 570.668884  | 989.891235  | 839.180298  | 689.863586 | 695.739136 | 2033.681885 | 786.547852 | 3528.163574 | 572.166077 | 484.491211 | ... | 1643.470337 | 2564.206787 | 2025.219971 | 2104.706787 | 1792.828613 | 2106.199463 | 2087.914062 | 1457.817871 | 1874.736938 | 1683.072021 |
| 16382 | 562.505310  | 977.029785  | 1005.927673 | 683.250183 | 661.608337 | 2566.123047 | 805.594116 | 3731.086426 | 566.710571 | 489.289673 | ... | 1556.492188 | 2783.619385 | 1312.174561 | 2378.643311 | 2466.965576 | 2160.641357 | 1691.332520 | 2013.095093 | 1632.475708 | 1352.443237 |
| 16383 | 567.307373  | 1006.794067 | 982.376526  | 677.099854 | 657.040588 | 2545.080322 | 784.917969 | 3850.334717 | 570.241699 | 512.688232 | ... | 2127.414551 | 2448.062012 | 1398.359253 | 1665.321167 | 2241.687256 | 1823.699829 | 1340.686035 | 1972.661743 | 1550.770142 | 1727.808228 |

16384 rows × 1500 columns
     :END:

    #+BEGIN_SRC jupyter-python
      import importlib
      importlib.reload(ppd)
      importlib.reload(isfc)
      importlib.reload(cfo)
    #+END_SRC

    #+RESULTS:
    : <module 'fluotracify.applications.corr_fit_object' from '/home/lex/Programme/drmed-git/src/fluotracify/applications/corr_fit_object.py'>
**** plot traces with labels
   :PROPERTIES:
   :header-args:jupyter-python: :session /jpy:localhost#8888:a37e524a-8134-4d8f-b24a-367acaf1bdd3 :pandoc t
   :END:

   - call =jupyter-set-output-directory=, =simulations-prepare-modules= and
     =simulations-prepare-data=
     #+CALL: jupyter-set-output-directory()

     #+RESULTS:
     : ./data/exp-220316-publication1/jupyter

     #+CALL: simulations-prepare-modules()

     #+RESULTS:
     :RESULTS:
     : /home/lex/Programme/drmed-git
     : 2023-01-16 15:28:48.365303: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
     : 2023-01-16 15:28:48.365357: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
     :END:

     #+CALL: simulations-prepare-data()

     #+RESULTS:
     :RESULTS:
     |       | 0.069-0.1   | 0.069-0.1   | 0.069-0.1   | 0.069-0.1  | 0.069-0.1  | 0.069-0.1   | 0.069-0.1  | 0.069-0.1   | 0.069-0.1  | 0.069-0.1  | ... | 50.0-0.01   | 50.0-0.01   | 50.0-0.01   | 50.0-0.01   | 50.0-0.01   | 50.0-0.01   | 50.0-0.01   | 50.0-0.01   | 50.0-0.01   | 50.0-0.01   |
|-------+-------------+-------------+-------------+------------+------------+-------------+------------+-------------+------------+------------+-----+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------|
| 0     | 1187.467896 | 907.677734  | 480.048798  | 454.669403 | 466.063232 | 384.734467  | 543.981323 | 640.921509  | 795.946167 | 410.471893 | ... | 1897.398193 | 2279.646484 | 3088.531006 | 2034.432495 | 2187.548584 | 2105.736084 | 1789.366577 | 2023.093994 | 2331.185791 | 2185.028076 |
| 1     | 1184.055176 | 945.760315  | 471.065216  | 458.392487 | 473.306152 | 395.165527  | 558.603088 | 622.421204  | 776.199402 | 409.149170 | ... | 1499.969849 | 2199.652100 | 3207.333008 | 1650.523926 | 2122.935791 | 2791.281006 | 1661.286377 | 1111.879761 | 1853.699585 | 1926.844971 |
| 2     | 1191.848877 | 980.117798  | 459.479706  | 426.087982 | 482.370209 | 425.123413  | 551.536072 | 624.498535  | 778.671265 | 400.971954 | ... | 1822.985229 | 2456.422607 | 2969.562500 | 1934.118286 | 1457.731812 | 2251.077393 | 1903.003662 | 2063.915527 | 2198.018066 | 2038.683105 |
| 3     | 1199.065918 | 974.313110  | 462.205566  | 444.041809 | 463.703125 | 434.186615  | 573.044128 | 626.252502  | 747.284058 | 393.178162 | ... | 1741.839355 | 2467.149414 | 2588.980957 | 2136.627686 | 1930.263672 | 2323.700684 | 2133.313721 | 1638.169312 | 1926.058716 | 1815.259521 |
| 4     | 1221.957397 | 968.779175  | 464.918030  | 455.205292 | 474.615540 | 437.029419  | 586.489136 | 619.319092  | 781.954102 | 406.468018 | ... | 2431.400879 | 2246.336670 | 3000.961182 | 1915.518066 | 2052.773682 | 2359.145508 | 1699.926147 | 1862.709595 | 2291.338379 | 1332.422241 |
| ...   | ...         | ...         | ...         | ...        | ...        | ...         | ...        | ...         | ...        | ...        | ... | ...         | ...         | ...         | ...         | ...         | ...         | ...         | ...         | ...         | ...         |
| 16379 | 506.409668  | 1012.403931 | 855.006226  | 674.470703 | 769.859192 | 2110.732178 | 799.565247 | 1981.221436 | 528.844604 | 483.055878 | ... | 1512.586548 | 3212.712891 | 1491.119995 | 1843.866943 | 1748.956665 | 2048.602051 | 1662.244385 | 2593.879639 | 1921.427856 | 1664.831909 |
| 16380 | 536.809692  | 1022.029724 | 840.287720  | 671.095215 | 738.908997 | 2118.984863 | 807.995789 | 2624.343262 | 552.687012 | 479.768372 | ... | 1661.331055 | 3190.809570 | 1770.193970 | 2081.854248 | 2164.372803 | 2295.646729 | 1846.683594 | 2038.272339 | 2222.708252 | 2122.753662 |
| 16381 | 570.668884  | 989.891235  | 839.180298  | 689.863586 | 695.739136 | 2033.681885 | 786.547852 | 3528.163574 | 572.166077 | 484.491211 | ... | 1643.470337 | 2564.206787 | 2025.219971 | 2104.706787 | 1792.828613 | 2106.199463 | 2087.914062 | 1457.817871 | 1874.736938 | 1683.072021 |
| 16382 | 562.505310  | 977.029785  | 1005.927673 | 683.250183 | 661.608337 | 2566.123047 | 805.594116 | 3731.086426 | 566.710571 | 489.289673 | ... | 1556.492188 | 2783.619385 | 1312.174561 | 2378.643311 | 2466.965576 | 2160.641357 | 1691.332520 | 2013.095093 | 1632.475708 | 1352.443237 |
| 16383 | 567.307373  | 1006.794067 | 982.376526  | 677.099854 | 657.040588 | 2545.080322 | 784.917969 | 3850.334717 | 570.241699 | 512.688232 | ... | 2127.414551 | 2448.062012 | 1398.359253 | 1665.321167 | 2241.687256 | 1823.699829 | 1340.686035 | 1972.661743 | 1550.770142 | 1727.808228 |

16384 rows × 1500 columns
     :END:

   - define plotting functions
     #+BEGIN_SRC jupyter-python
       plot1_index = ['3.0-0.01', '3.0-0.1', '3.0-1.0',
                      '0.2-0.01', '0.2-0.1', '0.2-1.0',
                      '0.069-0.01', '0.069-0.1', '0.069-1.0']

       plot1_traceno = [1, 1, 0,
                        5, 0, 0,
                        1, 1, 0]

       def get_tt(dr):
           dr = dr.removesuffix('-0.01').removesuffix('-0.1').removesuffix('-1.0')
           dr = float(dr)
           tt, _ = ans.convert_diffcoeff_to_transittimes(dr, 250)
           return f'\nsimulated trace\n$\\tau_{{sim}}={tt:.2f}ms$'

       def save_plot(filename, txt):
           plot_file = f'{filename}_{txt}'.replace(' ', '_').replace(
               '\n', '_').replace('"', '').replace('{', '').replace(
               '}', '').replace('$', '').replace('=', '-').replace('\\', '')
           plt.savefig(f'{plot_file}.pdf', bbox_inches='tight', dpi=300)
           os.system(f'pdf2svg {plot_file}.pdf {plot_file}.svg')
           os.system(f'rm {plot_file}.pdf')

       def plot_label_based_cut_and_shift_correction(filename):
           for i, (idx, t) in enumerate(zip(plot1_index, plot1_traceno)):
               fig = plt.figure()
               ax = plt.subplot(111)
               txt = get_tt(idx)
               ax.set_prop_cycle(color=[sns.color_palette()[4]])
               sim_labbool_scaled = sim_dirty.loc[:, idx].iloc[
                   :, t].max() * sim_labbool.loc[:, idx].iloc[:, t]
               sns.lineplot(data=sim_labbool_scaled, alpha=0.5)
               plt.fill_between(x=sim_labbool.loc[:, idx].iloc[:, t].index,
                                y1=sim_labbool_scaled,
                                y2=0, alpha=0.5, label='label:\npeak artifacts')

               ax.set_prop_cycle(color=[sns.color_palette()[2]])
               sim_invbool_scaled = sim_dirty.loc[:, idx].iloc[
                   :, t].max() * ~sim_labbool.loc[:, idx].iloc[:, t]
               plt.fill_between(x=sim_labbool.loc[:, idx].iloc[:, t].index,
                                y1=sim_invbool_scaled,
                                y2=0, alpha=0.5, label='\nlabel:\nno artifacts')
               ax.set_prop_cycle(color=[sns.color_palette()[0]])
               sns.lineplot(data=sim_dirty.loc[:, idx].iloc[:, t], label=txt)
               plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)
               plt.setp(ax, xlabel=r'Time [$ms$]', ylabel=r'Intensity [a.u.]', title='')
               save_plot(filename, f'{txt}_{i}')
           plt.close('all')

     #+END_SRC

     #+RESULTS:




     #+BEGIN_SRC jupyter-python
       plot_label_based_cut_and_shift_correction('data/exp-220316-publication1/jupyter/plot2_labseg')
     #+END_SRC

     #+RESULTS:

**** plot transit times vs random cuts in =clean= trace
   :PROPERTIES:
   :header-args:jupyter-python: :session /jpy:localhost#8888:a37e524a-8134-4d8f-b24a-367acaf1bdd3 :pandoc t
   :END:

   - call =jupyter-set-output-directory= and =simulations-prepare-modules=
     #+CALL: jupyter-set-output-directory()

     #+RESULTS:
     : ./data/exp-220316-publication1/jupyter

     #+CALL: simulations-prepare-modules()

     #+RESULTS:
     :RESULTS:
     : /home/lex/Programme/drmed-git
     : 2023-01-16 15:28:48.365303: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
     : 2023-01-16 15:28:48.365357: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
     :END:

   - let's load and plot cuts vs transit times from 3 different base molecule
     speeds (varying molecule numbers): 0.069, 0.2, and 1.0 um^2/s respectively.
     #+BEGIN_SRC jupyter-python
       path = Path('data/exp-220316-publication1/220714_sim-cutandshift')

       # slowest molecules
       odot069_0cuts_path = path / '0.069/0.069-all-results/0dot069_0-cuts_outputParam.csv'
       odot069_1cuts_path = path / '0.069/0.069-all-results/0dot069_1-cuts_outputParam.csv'
       odot069_2cuts_path = path / '0.069/0.069-all-results/0dot069_2-cuts_outputParam.csv'
       odot069_4cuts_path = path / '0.069/0.069-all-results/0dot069_4-cuts_outputParam.csv'
       odot069_8cuts_path = path / '0.069/0.069-all-results/0dot069_8-cuts_outputParam.csv'
       odot069_10cuts_path = path / '0.069/0.069-all-results/0dot069_10-cuts_outputParam.csv'
       odot069_20cuts_path = path / '0.069/0.069-all-results/0dot069_20-cuts_outputParam.csv'
       odot069_40cuts_path = path / '0.069/0.069-all-results/0dot069_40-cuts_outputParam.csv'
       odot069_80cuts_path = path / '0.069/0.069-all-results/0dot069_80-cuts_outputParam.csv'
       odot069_100cuts_path = path / '0.069/0.069-all-results/0dot069_100-cuts_outputParam.csv'
       odot069_200cuts_path = path / '0.069/0.069-all-results/0dot069_200-cuts_outputParam.csv'
       odot069_400cuts_path = path / '0.069/0.069-all-results/0dot069_400-cuts_outputParam.csv'
       odot069_800cuts_path = path / '0.069/0.069-all-results/0dot069_800-cuts_outputParam.csv'
       odot069_1000cuts_path = path / '0.069/0.069-all-results/0dot069_1000-cuts_outputParam.csv'
       odot069_10000cuts_path = path / '0.069/0.069-all-results/0dot069_10000-cuts_outputParam.csv'

       # medium molecules
       odot2_0cuts_path = path / '0.2/0.2-all-results/0dot2_0-cuts_outputParam.csv'
       odot2_1cuts_path = path / '0.2/0.2-all-results/0dot2_1-cuts_outputParam.csv'
       odot2_2cuts_path = path / '0.2/0.2-all-results/0dot2_2-cuts_outputParam.csv'
       odot2_4cuts_path = path / '0.2/0.2-all-results/0dot2_4-cuts_outputParam.csv'
       odot2_8cuts_path = path / '0.2/0.2-all-results/0dot2_8-cuts_outputParam.csv'
       odot2_10cuts_path = path / '0.2/0.2-all-results/0dot2_10-cuts_outputParam.csv'
       odot2_20cuts_path = path / '0.2/0.2-all-results/0dot2_20-cuts_outputParam.csv'
       odot2_40cuts_path = path / '0.2/0.2-all-results/0dot2_40-cuts_outputParam.csv'
       odot2_80cuts_path = path / '0.2/0.2-all-results/0dot2_80-cuts_outputParam.csv'
       odot2_100cuts_path = path / '0.2/0.2-all-results/0dot2_100-cuts_outputParam.csv'
       odot2_200cuts_path = path / '0.2/0.2-all-results/0dot2_200-cuts_outputParam.csv'
       odot2_400cuts_path = path / '0.2/0.2-all-results/0dot2_400-cuts_outputParam.csv'
       odot2_800cuts_path = path / '0.2/0.2-all-results/0dot2_800-cuts_outputParam.csv'
       odot2_1000cuts_path = path / '0.2/0.2-all-results/0dot2_1000-cuts_outputParam.csv'
       odot2_10000cuts_path = path / '0.2/0.2-all-results/0dot2_10000-cuts_outputParam.csv'

       # fast molecules
       # one_0cuts_path = path / '1.0/1.0-all-results/1dot0_0-cuts_outputParam.csv'
       # one_1cuts_path = path / '1.0/1.0-all-results/1dot0_1-cuts_outputParam.csv'
       # one_2cuts_path = path / '1.0/1.0-all-results/1dot0_2-cuts_outputParam.csv'
       # one_4cuts_path = path / '1.0/1.0-all-results/1dot0_4-cuts_outputParam.csv'
       # one_8cuts_path = path / '1.0/1.0-all-results/1dot0_8-cuts_outputParam.csv'
       # one_10cuts_path = path / '1.0/1.0-all-results/1dot0_10-cuts_outputParam.csv'
       # one_20cuts_path = path / '1.0/1.0-all-results/1dot0_20-cuts_outputParam.csv'
       # one_40cuts_path = path / '1.0/1.0-all-results/1dot0_40-cuts_outputParam.csv'
       # one_80cuts_path = path / '1.0/1.0-all-results/1dot0_80-cuts_outputParam.csv'
       # one_100cuts_path = path / '1.0/1.0-all-results/1dot0_100-cuts_outputParam.csv'
       # one_200cuts_path = path / '1.0/1.0-all-results/1dot0_200-cuts_outputParam.csv'
       # one_400cuts_path = path / '1.0/1.0-all-results/1dot0_400-cuts_outputParam.csv'
       # one_800cuts_path = path / '1.0/1.0-all-results/1dot0_800-cuts_outputParam.csv'
       # one_1000cuts_path = path / '1.0/1.0-all-results/1dot0_1000-cuts_outputParam.csv'
       # one_10000cuts_path = path / '1.0/1.0-all-results/1dot0_10000-cuts_outputParam.csv'

       # fastest molecules
       three_0cuts_path = path / '3.0/3.0-all-results/3dot0_0-cuts_outputParam.csv'
       three_1cuts_path = path / '3.0/3.0-all-results/3dot0_1-cuts_outputParam.csv'
       three_2cuts_path = path / '3.0/3.0-all-results/3dot0_2-cuts_outputParam.csv'
       three_4cuts_path = path / '3.0/3.0-all-results/3dot0_4-cuts_outputParam.csv'
       three_8cuts_path = path / '3.0/3.0-all-results/3dot0_8-cuts_outputParam.csv'
       three_10cuts_path = path / '3.0/3.0-all-results/3dot0_10-cuts_outputParam.csv'
       three_20cuts_path = path / '3.0/3.0-all-results/3dot0_20-cuts_outputParam.csv'
       three_40cuts_path = path / '3.0/3.0-all-results/3dot0_40-cuts_outputParam.csv'
       three_80cuts_path = path / '3.0/3.0-all-results/3dot0_80-cuts_outputParam.csv'
       three_100cuts_path = path / '3.0/3.0-all-results/3dot0_100-cuts_outputParam.csv'
       three_200cuts_path = path / '3.0/3.0-all-results/3dot0_200-cuts_outputParam.csv'
       three_400cuts_path = path / '3.0/3.0-all-results/3dot0_400-cuts_outputParam.csv'
       three_800cuts_path = path / '3.0/3.0-all-results/3dot0_800-cuts_outputParam.csv'
       three_1000cuts_path = path / '3.0/3.0-all-results/3dot0_1000-cuts_outputParam.csv'
       three_10000cuts_path = path / '3.0/3.0-all-results/3dot0_10000-cuts_outputParam.csv'


       # slowest molecules
       odot069_0cuts = pd.read_csv(odot069_0cuts_path, sep=',').assign(speed=300*[0.069,])
       odot069_1cuts = pd.read_csv(odot069_1cuts_path, sep=',').assign(speed=300*[0.069,])
       odot069_2cuts = pd.read_csv(odot069_2cuts_path, sep=',').assign(speed=300*[0.069,])
       odot069_4cuts = pd.read_csv(odot069_4cuts_path, sep=',').assign(speed=300*[0.069,])
       odot069_8cuts = pd.read_csv(odot069_8cuts_path, sep=',').assign(speed=300*[0.069,])
       odot069_10cuts = pd.read_csv(odot069_10cuts_path, sep=',').assign(speed=300*[0.069,])
       odot069_20cuts = pd.read_csv(odot069_20cuts_path, sep=',').assign(speed=300*[0.069,])
       odot069_40cuts = pd.read_csv(odot069_40cuts_path, sep=',').assign(speed=300*[0.069,])
       odot069_80cuts = pd.read_csv(odot069_80cuts_path, sep=',').assign(speed=300*[0.069,])
       odot069_100cuts = pd.read_csv(odot069_100cuts_path, sep=',').assign(speed=300*[0.069,])
       odot069_200cuts = pd.read_csv(odot069_200cuts_path, sep=',').assign(speed=300*[0.069,])
       odot069_400cuts = pd.read_csv(odot069_400cuts_path, sep=',').assign(speed=300*[0.069,])
       odot069_800cuts = pd.read_csv(odot069_800cuts_path, sep=',').assign(speed=300*[0.069,])
       odot069_1000cuts = pd.read_csv(odot069_1000cuts_path, sep=',').assign(speed=300*[0.069,])
       odot069_10000cuts = pd.read_csv(odot069_10000cuts_path, sep=',').assign(speed=300*[0.069,])

       # medium speed molecules
       odot2_0cuts = pd.read_csv(odot2_0cuts_path, sep=',').assign(speed=300*[0.2,])
       odot2_1cuts = pd.read_csv(odot2_1cuts_path, sep=',').assign(speed=300*[0.2,])
       odot2_2cuts = pd.read_csv(odot2_2cuts_path, sep=',').assign(speed=300*[0.2,])
       odot2_4cuts = pd.read_csv(odot2_4cuts_path, sep=',').assign(speed=300*[0.2,])
       odot2_8cuts = pd.read_csv(odot2_8cuts_path, sep=',').assign(speed=300*[0.2,])
       odot2_10cuts = pd.read_csv(odot2_10cuts_path, sep=',').assign(speed=300*[0.2,])
       odot2_20cuts = pd.read_csv(odot2_20cuts_path, sep=',').assign(speed=300*[0.2,])
       odot2_40cuts = pd.read_csv(odot2_40cuts_path, sep=',').assign(speed=300*[0.2,])
       odot2_80cuts = pd.read_csv(odot2_80cuts_path, sep=',').assign(speed=300*[0.2,])
       odot2_100cuts = pd.read_csv(odot2_100cuts_path, sep=',').assign(speed=300*[0.2,])
       odot2_200cuts = pd.read_csv(odot2_200cuts_path, sep=',').assign(speed=300*[0.2,])
       odot2_400cuts = pd.read_csv(odot2_400cuts_path, sep=',').assign(speed=300*[0.2,])
       odot2_800cuts = pd.read_csv(odot2_800cuts_path, sep=',').assign(speed=300*[0.2,])
       odot2_1000cuts = pd.read_csv(odot2_1000cuts_path, sep=',').assign(speed=300*[0.2,])
       odot2_10000cuts = pd.read_csv(odot2_10000cuts_path, sep=',').assign(speed=300*[0.2,])


       # faster molecules
       # one_0cuts = pd.read_csv(one_0cuts_path, sep=',').assign(speed=300*[1.0,])
       # one_1cuts = pd.read_csv(one_1cuts_path, sep=',').assign(speed=300*[1.0,])
       # one_2cuts = pd.read_csv(one_2cuts_path, sep=',').assign(speed=300*[1.0,])
       # one_4cuts = pd.read_csv(one_4cuts_path, sep=',').assign(speed=300*[1.0,])
       # one_8cuts = pd.read_csv(one_8cuts_path, sep=',').assign(speed=300*[1.0,])
       # one_10cuts = pd.read_csv(one_10cuts_path, sep=',').assign(speed=300*[1.0,])
       # one_20cuts = pd.read_csv(one_20cuts_path, sep=',').assign(speed=300*[1.0,])
       # one_40cuts = pd.read_csv(one_40cuts_path, sep=',').assign(speed=300*[1.0,])
       # one_80cuts = pd.read_csv(one_80cuts_path, sep=',').assign(speed=300*[1.0,])
       # one_100cuts = pd.read_csv(one_100cuts_path, sep=',').assign(speed=300*[1.0,])
       # one_200cuts = pd.read_csv(one_200cuts_path, sep=',').assign(speed=300*[1.0,])
       # one_400cuts = pd.read_csv(one_400cuts_path, sep=',').assign(speed=300*[1.0,])
       # one_800cuts = pd.read_csv(one_800cuts_path, sep=',').assign(speed=300*[1.0,])
       # one_1000cuts = pd.read_csv(one_1000cuts_path, sep=',').assign(speed=300*[1.0,])
       # one_10000cuts = pd.read_csv(one_10000cuts_path, sep=',').assign(speed=300*[1.0,])

       # fastest molecules
       three_0cuts = pd.read_csv(three_0cuts_path, sep=',').assign(speed=300*[3.0,])
       three_1cuts = pd.read_csv(three_1cuts_path, sep=',').assign(speed=300*[3.0,])
       three_2cuts = pd.read_csv(three_2cuts_path, sep=',').assign(speed=300*[3.0,])
       three_4cuts = pd.read_csv(three_4cuts_path, sep=',').assign(speed=300*[3.0,])
       three_8cuts = pd.read_csv(three_8cuts_path, sep=',').assign(speed=300*[3.0,])
       three_10cuts = pd.read_csv(three_10cuts_path, sep=',').assign(speed=300*[3.0,])
       three_20cuts = pd.read_csv(three_20cuts_path, sep=',').assign(speed=300*[3.0,])
       three_40cuts = pd.read_csv(three_40cuts_path, sep=',').assign(speed=300*[3.0,])
       three_80cuts = pd.read_csv(three_80cuts_path, sep=',').assign(speed=300*[3.0,])
       three_100cuts = pd.read_csv(three_100cuts_path, sep=',').assign(speed=300*[3.0,])
       three_200cuts = pd.read_csv(three_200cuts_path, sep=',').assign(speed=300*[3.0,])
       three_400cuts = pd.read_csv(three_400cuts_path, sep=',').assign(speed=300*[3.0,])
       three_800cuts = pd.read_csv(three_800cuts_path, sep=',').assign(speed=300*[3.0,])
       three_1000cuts = pd.read_csv(three_1000cuts_path, sep=',').assign(speed=300*[3.0,])
       three_10000cuts = pd.read_csv(three_10000cuts_path, sep=',').assign(speed=300*[3.0,])


       all_param = pd.concat([odot069_0cuts, odot069_1cuts, odot069_2cuts,
                              odot069_4cuts, odot069_8cuts, odot069_10cuts,
                              odot069_20cuts, odot069_40cuts, odot069_80cuts,
                              odot069_100cuts, odot069_200cuts, odot069_400cuts,
                              odot069_800cuts, odot069_1000cuts, # odot069_10000cuts,
                              odot2_0cuts, odot2_1cuts, odot2_2cuts,
                              odot2_4cuts, odot2_8cuts, odot2_10cuts,
                              odot2_20cuts, odot2_40cuts, odot2_80cuts,
                              odot2_100cuts, odot2_200cuts, odot2_400cuts,
                              odot2_800cuts, odot2_1000cuts, # odot2_10000cuts,
       #                        one_0cuts, one_1cuts, one_2cuts,
       #                        one_4cuts, one_8cuts, one_10cuts,
       #                        one_20cuts, one_40cuts, one_80cuts,
       #                        one_100cuts, one_200cuts, one_400cuts,
       #                        one_800cuts, one_1000cuts, one_10000cuts,
                              three_0cuts, three_1cuts, three_2cuts,
                              three_4cuts, three_8cuts, three_10cuts,
                              three_20cuts, three_40cuts, three_80cuts,
                              three_100cuts, three_200cuts, three_400cuts,
                              three_800cuts, three_1000cuts]) # , three_10000cuts])
       def cuts_to_integer(param_ls):
           cuts = param_ls.replace('-cuts', '')
           return int(cuts)

       def add_transit_time_legend(param_ls):
           tt, _ = ans.convert_diffcoeff_to_transittimes(param_ls, fwhm=250)
           return tt


       all_param['cuts'] = all_param['parent_name'].apply(
           lambda x: cuts_to_integer(x))
       all_param['legend'] = all_param['speed'].apply(
           lambda x: add_transit_time_legend(x))
       all_param
     #+END_SRC

     #+RESULTS:
     :RESULTS:
     |     | name_of_plot                                      | master_file | parent_name | parent_uqid | time of fit              | Diff_eq     | Diff_species | Triplet_eq | Triplet_species | Dimen | ... | txy1       | stdev(txy1) | alpha1 | stdev(alpha1) | N (mom) | bri (kHz) | above zero | speed | cuts | legend     |
|-----+---------------------------------------------------+-------------+-------------+-------------+--------------------------+-------------+--------------+------------+-----------------+-------+-----+------------+-------------+--------+---------------+---------+-----------+------------+-------+------+------------|
| 0   | 2022-07-15_multipletau_0-cuts_0dot069_0000_cor... | Not known   | 0-cuts      | 0           | Fri Jul 15 11:59:09 2022 | Equation 1A | 1            | no triplet | 1               | 2D    | ... | 260.674853 | 17.960765   | 1.0    | 0.0           | 1.0     | 1.0       | 0.829787   | 0.069 | 0    | 163.348623 |
| 1   | 2022-07-15_multipletau_0-cuts_0dot069_0001_cor... | Not known   | 0-cuts      | 0           | Fri Jul 15 11:59:09 2022 | Equation 1A | 1            | no triplet | 1               | 2D    | ... | 243.225061 | 27.450702   | 1.0    | 0.0           | 1.0     | 1.0       | 0.808511   | 0.069 | 0    | 163.348623 |
| 2   | 2022-07-15_multipletau_0-cuts_0dot069_0002_cor... | Not known   | 0-cuts      | 0           | Fri Jul 15 11:59:09 2022 | Equation 1A | 1            | no triplet | 1               | 2D    | ... | 203.668625 | 16.326836   | 1.0    | 0.0           | 1.0     | 1.0       | 0.765957   | 0.069 | 0    | 163.348623 |
| 3   | 2022-07-15_multipletau_0-cuts_0dot069_0003_cor... | Not known   | 0-cuts      | 0           | Fri Jul 15 11:59:09 2022 | Equation 1A | 1            | no triplet | 1               | 2D    | ... | 146.365708 | 15.385068   | 1.0    | 0.0           | 1.0     | 1.0       | 0.808511   | 0.069 | 0    | 163.348623 |
| 4   | 2022-07-15_multipletau_0-cuts_0dot069_0004_cor... | Not known   | 0-cuts      | 0           | Fri Jul 15 11:59:09 2022 | Equation 1A | 1            | no triplet | 1               | 2D    | ... | 95.248179  | 5.913751    | 1.0    | 0.0           | 1.0     | 1.0       | 0.829787   | 0.069 | 0    | 163.348623 |
| ... | ...                                               | ...         | ...         | ...         | ...                      | ...         | ...          | ...        | ...             | ...   | ... | ...        | ...         | ...    | ...           | ...     | ...       | ...        | ...   | ...  | ...        |
| 295 | 2022-08-11_multipletau_1000-cuts_3dot0_0295_co... | Not known   | 1000-cuts   | 0           | Thu Aug 11 13:48:19 2022 | Equation 1A | 1            | no triplet | 1               | 2D    | ... | 1.394099   | 0.119116    | 1.0    | 0.0           | 1.0     | 1.0       | 0.627660   | 3.000 | 1000 | 3.757018   |
| 296 | 2022-08-11_multipletau_1000-cuts_3dot0_0296_co... | Not known   | 1000-cuts   | 0           | Thu Aug 11 13:48:19 2022 | Equation 1A | 1            | no triplet | 1               | 2D    | ... | 1.089558   | 0.146448    | 1.0    | 0.0           | 1.0     | 1.0       | 0.521277   | 3.000 | 1000 | 3.757018   |
| 297 | 2022-08-11_multipletau_1000-cuts_3dot0_0297_co... | Not known   | 1000-cuts   | 0           | Thu Aug 11 13:48:19 2022 | Equation 1A | 1            | no triplet | 1               | 2D    | ... | 1.077039   | 0.158026    | 1.0    | 0.0           | 1.0     | 1.0       | 0.521277   | 3.000 | 1000 | 3.757018   |
| 298 | 2022-08-11_multipletau_1000-cuts_3dot0_0298_co... | Not known   | 1000-cuts   | 0           | Thu Aug 11 13:48:19 2022 | Equation 1A | 1            | no triplet | 1               | 2D    | ... | 1.030778   | 0.112158    | 1.0    | 0.0           | 1.0     | 1.0       | 0.627660   | 3.000 | 1000 | 3.757018   |
| 299 | 2022-08-11_multipletau_1000-cuts_3dot0_0299_co... | Not known   | 1000-cuts   | 0           | Thu Aug 11 13:48:19 2022 | Equation 1A | 1            | no triplet | 1               | 2D    | ... | 1.383809   | 0.108761    | 1.0    | 0.0           | 1.0     | 1.0       | 0.648936   | 3.000 | 1000 | 3.757018   |

12600 rows × 30 columns
     :END:

     #+BEGIN_SRC jupyter-python
       def simcuts(data):
           g = sns.catplot(data=all_param,
                           y='txy1',
                           x='cuts',
                           row='legend',
                           height=5,
                           aspect=2,
                           legend_out=True,
                           kind='boxen',
                           showfliers=False,
                           sharey=False)
           styles = ['--', ':', '-', '-.']
           for i, ax in enumerate(g.axes):
               tt = str(ax[0].title).split('= ')
               tt = tt[1].strip("')")
               tt = float(tt)
               hline = ax[0].axhline(tt, lw=3, label='', ls=styles[i])
               hline_legend = {f'\n$\\tau_{{sim}}={tt:.2f}ms$' : hline}
               g._legend_data.update(hline_legend)
           g.add_legend(g._legend_data)
           g.map_dataframe(sns.stripplot,
                 y='txy1',
                 x='cuts',
                 dodge=True,
                 palette=sns.color_palette(['0.3']),
                 size=4,
                 jitter=0.2)
           g.fig.suptitle('', size=25)
           for ax in g.axes:
               # ax[0].set_title('')
               tt = str(ax[0].title).split('= ')
               tt = tt[1].strip("')")
               tt = float(tt)
               ax[0].set_title(f'sim. trace $\\tau_{{sim}}={tt:.2f}ms$,\nno artifacts, $n=300$, $\\tau_D$ from 1 sp. fit')

           # g._legend.set_title('')
           # new_labels = ['424 traces of\nAlexaFluor488\n(no artifacts)\n$\\tau_D$ from\n1 species fit',
           #               '440 traces of\nAF488 + DiO LUVs\n(peak artifacts)\n$\\tau_D$ from\nfast sp. of 2 sp. fit']
           # for t, l in zip(g._legend.texts, new_labels):
           #     t.set_text(l)
           for i, axes in enumerate(g.axes.flat):
              _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=45)

           plt.setp(g.axes, yscale='log', xlabel='number of cuts',
                    ylabel=r'log transit time $\tau_{D}$ $[ms]$')
           g.fig.align_ylabels()
           g.tight_layout()

           savefig = './data/exp-220316-publication1/jupyter/plot2_transit-times-vs-cuts'
           plt.savefig(f'{savefig}.pdf', bbox_inches='tight', dpi=300)
           os.system(f'pdf2svg {savefig}.pdf {savefig}.svg')
           plt.close('all')
     #+END_SRC

     #+RESULTS:

     #+BEGIN_SRC jupyter-python
       sns.set_theme(style="whitegrid", font_scale=2.4, palette='colorblind',
                     context='paper')
       simcuts(all_param)
     #+END_SRC

     #+RESULTS:

     #+BEGIN_SRC jupyter-python

     #+END_SRC

**** comparison correction methods - fit outcomes
   :PROPERTIES:
   :header-args:jupyter-python: :session /jpy:localhost#8888:a37e524a-8134-4d8f-b24a-367acaf1bdd3 :pandoc t
   :END:

   - call =jupyter-set-output-directory= and =simulations-prepare-modules=
     #+CALL: jupyter-set-output-directory()

     #+RESULTS:
     : ./data/exp-220316-publication1/jupyter

     #+CALL: simulations-prepare-modules()

     #+RESULTS:
     : /home/lex/Programme/drmed-git

   - let's *compare correction methods* on 3 different simulated base molecule
     speeds (0.069, 0.2, and 3.0 um^2/s; varying molecule numbers) and af488 vs
     af488+LUV data and hs-pex5 vs tb-pex5 data
     #+BEGIN_SRC jupyter-python
       # dirty correlations - check out from  branch exp-220227-unet
       path1 = Path('/home/lex/Programme/drmed-git/data/exp-220227-unet/2022-04-25_simulations/dirty')
       odot069_dirty_1comp_path = path1 / '0.069_results/dirty_0dot069-all_1comp_outputParam.csv'
       odot069_dirty_2comp_path = path1 / '0.069_results/dirty_0dot069-all_2comp_outputParam.csv'
       odot2_dirty_1comp_path = path1 / '0.2_results/dirty_0dot2-all_1comp_outputParam.csv'
       odot2_dirty_2comp_path = path1 / '0.2_results/dirty_0dot2-all_2comp_outputParam.csv'
       # one_dirty_1comp_path = path1 / '1.0_results/dirty_1dot0-all_1comp_outputParam.csv'
       # one_dirty_2comp_path = path1 / '1.0_results/dirty_1dot0-all_2comp_outputParam.csv'
       three_dirty_1comp_path = path1 / '3.0_results/dirty_3dot0-all_1comp_outputParam.csv'
       three_dirty_2comp_path = path1 / '3.0_results/dirty_3dot0-all_2comp_outputParam.csv'

       # clean correlations
       path2 = Path('/home/lex/Programme/drmed-git/data/exp-220316-publication1/220719_threshold-prediction/threshold-all-results')
       odot069_clean_1comp_path = path2 / '0dot069_clean_1comp_outputParam.csv'
       odot2_clean_1comp_path = path2 / '0dot2_clean_1comp_outputParam.csv'
       # one_clean_1comp_path = path2 / '1dot0_clean_1comp_outputParam.csv'
       three_clean_1comp_path = path2 / '3dot0_clean_1comp_outputParam.csv'
       #
       # # control with prediction threshold
       # odot069_thresh2_1comp_path = path2 / '0dot069_robust_thresh2_1comp_outputParam.csv'
       # odot069_thresh2_2comp_path = path2 / '0dot069_robust_thresh2_2comp_outputParam.csv'
       # odot2_thresh2_1comp_path = path2 / '0dot2_robust_thresh2_1comp_outputParam.csv'
       # odot2_thresh2_2comp_path = path2 / '0dot2_robust_thresh2_2comp_outputParam.csv'
       # # one_thresh2_1comp_path = path2 / '1dot0_robust_thresh2_1comp_outputParam.csv'
       # # one_thresh2_2comp_path = path2 / '1dot0_robust_thresh2_2comp_outputParam.csv'
       # three_thresh2_1comp_path = path2 / '3dot0_robust_thresh2_1comp_outputParam.csv'
       # three_thresh2_2comp_path = path2 / '3dot0_robust_thresh2_2comp_outputParam.csv'
       # # fifty_thresh2_1comp_path = path2 / '50dot0_robust_thresh2_1comp_outputParam.csv'


       # load correction by label information as baseline
       path4 = Path('/home/lex/Programme/drmed-git/data/exp-220316-publication1/220517_simulations/')
       odot069_labcas_1comp_path = path4 / '0.069-all-results/0dot069_lab_cas_1comp_outputParam.csv'
       odot069_labcas_2comp_path = path4 / '0.069-all-results/0dot069_lab_cas_2comp_outputParam.csv'
       odot2_labcas_1comp_path = path4 / '0.2-all-results/0dot2_lab_cas_1comp_outputParam.csv'
       odot2_labcas_2comp_path = path4 / '0.2-all-results/0dot2_lab_cas_2comp_outputParam.csv'
       three_labcas_1comp_path = path4 / '3.0-all-results/3dot0_lab_cas_1comp_outputParam.csv'
       three_labcas_2comp_path = path4 / '3.0-all-results/3dot0_lab_cas_2comp_outputParam.csv'
       odot069_labdel_1comp_path = path4 / '0.069-all-results/0dot069_lab_del_1comp_outputParam.csv'
       odot069_labdel_2comp_path = path4 / '0.069-all-results/0dot069_lab_del_2comp_outputParam.csv'
       odot2_labdel_1comp_path = path4 / '0.2-all-results/0dot2_lab_del_1comp_outputParam.csv'
       odot2_labdel_2comp_path = path4 / '0.2-all-results/0dot2_lab_del_2comp_outputParam.csv'
       three_labdel_1comp_path = path4 / '3.0-all-results/3dot0_lab_del_1comp_outputParam.csv'
       three_labdel_2comp_path = path4 / '3.0-all-results/3dot0_lab_del_2comp_outputParam.csv'

       path5 = Path('/home/lex/Programme/drmed-git/data/exp-220316-publication1/230103_avg-correction/')
       odot069_labavg_1comp_path = path5 / 'all-results/0dot069_lab_avg_1comp_outputParam.csv'
       odot069_labavg_2comp_path = path5 / 'all-results/0dot069_lab_avg_2comp_outputParam.csv'
       odot2_labavg_1comp_path = path5 / 'all-results/0dot2_lab_avg_1comp_outputParam.csv'
       odot2_labavg_2comp_path = path5 / 'all-results/0dot2_lab_avg_2comp_outputParam.csv'
       three_labavg_1comp_path = path5 / 'all-results/three_lab_avg_1comp_outputParam.csv'
       three_labavg_2comp_path = path5 / 'all-results/three_lab_avg_2comp_outputParam.csv'


       # prediction by best unet 0cd20 - check out from branch exp-220227-unet
       # path3 = Path('/home/lex/Programme/drmed-git/data/exp-220227-unet/2022-04-25_simulations/0cd20')
       # odot069_0cd20_1comp_path = path3 / '0.069_results/0cd20_0dot069-all_1comp_outputParam.csv'
       # odot069_0cd20_2comp_path = path3 / '0.069_results/0cd20_0dot069-all_2comp_outputParam.csv'
       # odot2_0cd20_1comp_path = path3 / '0.2_results/0cd20_0dot2-all_1comp_outputParam.csv'
       # odot2_0cd20_2comp_path = path3 / '0.2_results/0cd20_0dot2-all_2comp_outputParam.csv'
       # # one_0cd20_1comp_path = path3 / '1.0_results/0cd20_1dot0-all_1comp_outputParam.csv'
       # # one_0cd20_2comp_path = path3 / '1.0_results/0cd20_1dot0-all_2comp_outputParam.csv'
       # three_0cd20_1comp_path = path3 / '3.0_results/0cd20_3dot0-all_1comp_outputParam.csv'
       # three_0cd20_2comp_path = path3 / '3.0_results/0cd20_3dot0-all_2comp_outputParam.csv'

       # biological data - clean, dirty, cut and stitch
       path6 = Path('/home/lex/Programme/drmed-git/data/exp-220227-unet/2022-05-22_experimental-af488')
       af488_noc_1comp_path = path6 / 'clean-all-results/clean_no-correction_1comp_outputParam.csv'
       # af488_0cd20cas_1comp_path = path6 / 'clean-all-results/clean_0cd20_1comp_outputParam.csv'

       af488luv_noc_1comp_path = path6 / 'dirty-all-results/dirty_no-correction_1comp_outputParam.csv'
       af488luv_noc_2comp_path = path6 / 'dirty-all-results/dirty_no-correction_2comp_outputParam.csv'
       af488luv_0cd20cas_1comp_path = path6 / 'dirty-all-results/dirty_0cd20_1comp_outputParam.csv'
       af488luv_0cd20cas_2comp_path = path6 / 'dirty-all-results/dirty_0cd20_2comp_outputParam.csv'

       path7 = Path('/home/lex/Programme/drmed-git/data/exp-220227-unet/2022-06-02_experimental-pex5')
       hspex5_noc_1comp_path = path7 / 'clean-all-results/Hs-PEX5-eGFP_no-correction_triplet-0.04_1comp_outputParam.csv'
       # hspex5_0cd20cas_1comp_path = path7 / 'clean-all-results/Hs-PEX5-eGFP_no-correction_triplet-0.04_1comp_outputParam.csv'

       tbpex5_noc_1comp_path = path7 / 'dirty-all-results/Tb-PEX5-eGFP_no-correction_triplet-0.04_1comp_outputParam.csv'
       tbpex5_noc_2comp_path = path7 / 'dirty-all-results/Tb-PEX5-eGFP_no-correction_triplet-0.04_2comp_outputParam.csv'
       tbpex5_0cd20cas_1comp_path = path7 / 'dirty-all-results/Tb-PEX5-eGFP_0cd20_triplet-0.04_1comp_outputParam.csv'
       tbpex5_0cd20cas_2comp_path = path7 / 'dirty-all-results/Tb-PEX5-eGFP_0cd20_triplet-0.04_2comp_outputParam.csv'

       # biological data - averaging, set to zero
       path8 = Path('/home/lex/Programme/drmed-git/data/exp-220227-unet/2023-01-10_experimental-averaging-delete/all-results')
       # af488_0cd20avg_1comp_path = path8 / 'af488_0cd20_averaging_1comp_outputParam.csv'
       # af488_0cd20del_1comp_path = path8 / 'af488_0cd20_delete_1comp_outputParam.csv'

       af488luv_0cd20avg_1comp_path = path8 / 'af488luv_0cd20_averaging_1comp_outputParam.csv'
       af488luv_0cd20avg_2comp_path = path8 / 'af488luv_0cd20_averaging_2comp_outputParam.csv'
       af488luv_0cd20del_1comp_path = path8 / 'af488luv_0cd20_delete_1comp_outputParam.csv'
       af488luv_0cd20del_2comp_path = path8 / 'af488luv_0cd20_delete_2comp_outputParam.csv'

       # hspex5_0cd20avg_1comp_path = path8 / 'hspex5_0cd20_averaging_1comp_outputParam.csv'
       # hspex5_0cd20del_1comp_path = path8 / 'hspex5_0cd20_delete_1comp_outputParam.csv'

       tbpex5_0cd20avg_1comp_path = path8 / 'tbpex5_0cd20_averaging_1comp_outputParam.csv'
       tbpex5_0cd20avg_2comp_path = path8 / 'tbpex5_0cd20_averaging_2comp_outputParam.csv'
       tbpex5_0cd20del_1comp_path = path8 / 'tbpex5_0cd20_delete_1comp_outputParam.csv'
       tbpex5_0cd20del_2comp_path = path8 / 'tbpex5_0cd20_delete_2comp_outputParam.csv'

       # load data
       odot069_clean_1comp = pd.read_csv(odot069_clean_1comp_path, sep=',').assign(sim=300*[0.069,], processing=300*['clean noc',])
       odot2_clean_1comp = pd.read_csv(odot2_clean_1comp_path, sep=',').assign(sim=300*[0.2,], processing=300*['clean noc',])
       # one_clean_1comp = pd.read_csv(one_clean_1comp_path, sep=',').assign(sim=300*[1,], processing=300*['clean noc',])
       three_clean_1comp = pd.read_csv(three_clean_1comp_path, sep=',').assign(sim=300*[3,], processing=300*['clean noc',])
       af488_1comp = pd.read_csv(af488_noc_1comp_path, sep=',').assign(sim=424*[280,], processing=424*['clean noc',])
       hspex5_1comp = pd.read_csv(hspex5_noc_1comp_path, sep=',').assign(sim=250*[31,], processing=250*['clean noc',])


       odot069_dirty_1comp = pd.read_csv(odot069_dirty_1comp_path, sep=',').assign(sim=300*[0.069,], processing=300*['dirty noc',])
       odot069_dirty_2comp = pd.read_csv(odot069_dirty_2comp_path, sep=',').assign(sim=300*[0.069,], processing=300*['dirty noc',])
       odot2_dirty_1comp = pd.read_csv(odot2_dirty_1comp_path, sep=',').assign(sim=300*[0.2,], processing=300*['dirty noc',])
       odot2_dirty_2comp = pd.read_csv(odot2_dirty_2comp_path, sep=',').assign(sim=300*[0.2,], processing=300*['dirty noc',])
       # one_dirty_1comp = pd.read_csv(one_dirty_1comp_path, sep=',').assign(sim=300*[1,], processing=300*['dirty noc',])
       # one_dirty_2comp = pd.read_csv(one_dirty_2comp_path, sep=',').assign(sim=300*[1,], processing=300*['dirty noc',])
       three_dirty_1comp = pd.read_csv(three_dirty_1comp_path, sep=',').assign(sim=300*[3,], processing=300*['dirty noc',])
       three_dirty_2comp = pd.read_csv(three_dirty_2comp_path, sep=',').assign(sim=300*[3,], processing=300*['dirty noc',])
       af488luv_1comp = pd.read_csv(af488luv_noc_1comp_path, sep=',').assign(sim=440*[280,], processing=440*['dirty noc',])
       af488luv_2comp = pd.read_csv(af488luv_noc_2comp_path, sep=',').assign(sim=440*[280,], processing=440*['dirty noc',])
       tbpex5_1comp = pd.read_csv(tbpex5_noc_1comp_path, sep=',').assign(sim=250*[31,], processing=250*['dirty noc',])
       tbpex5_2comp = pd.read_csv(tbpex5_noc_2comp_path, sep=',').assign(sim=250*[31,], processing=250*['dirty noc',])


       # load correction by label information as baseline
       odot069_labdel_1comp = pd.read_csv(odot069_labdel_1comp_path, sep=',').assign(sim=300*[0.069,], processing=300*['dirty del',])
       odot069_labdel_2comp = pd.read_csv(odot069_labdel_2comp_path, sep=',').assign(sim=300*[0.069,], processing=300*['dirty del',])
       odot2_labdel_1comp = pd.read_csv(odot2_labdel_1comp_path, sep=',').assign(sim=300*[0.2,], processing=300*['dirty del',])
       odot2_labdel_2comp = pd.read_csv(odot2_labdel_2comp_path, sep=',').assign(sim=300*[0.2,], processing=300*['dirty del',])
       three_labdel_1comp = pd.read_csv(three_labdel_1comp_path, sep=',').assign(sim=300*[3,], processing=300*['dirty del',])
       three_labdel_2comp = pd.read_csv(three_labdel_2comp_path, sep=',').assign(sim=300*[3,], processing=300*['dirty del',])
       af488luv_0cd20del_1comp = pd.read_csv(af488luv_0cd20del_1comp_path, sep=',').assign(sim=440*[280,], processing=440*['dirty del'])
       af488luv_0cd20del_2comp = pd.read_csv(af488luv_0cd20del_2comp_path, sep=',').assign(sim=440*[280,], processing=440*['dirty del'])
       tbpex5_0cd20del_1comp = pd.read_csv(tbpex5_0cd20del_1comp_path, sep=',').assign(sim=250*[31,], processing=250*['dirty del'])
       tbpex5_0cd20del_2comp = pd.read_csv(tbpex5_0cd20del_2comp_path, sep=',').assign(sim=250*[31,], processing=250*['dirty del'])

       odot069_labcas_1comp = pd.read_csv(odot069_labcas_1comp_path, sep=',').assign(sim=300*[0.069,], processing=300*['dirty cas',])
       odot069_labcas_2comp = pd.read_csv(odot069_labcas_2comp_path, sep=',').assign(sim=300*[0.069,], processing=300*['dirty cas',])
       odot2_labcas_1comp = pd.read_csv(odot2_labcas_1comp_path, sep=',').assign(sim=300*[0.2,], processing=300*['dirty cas',])
       odot2_labcas_2comp = pd.read_csv(odot2_labcas_2comp_path, sep=',').assign(sim=300*[0.2,], processing=300*['dirty cas',])
       three_labcas_1comp = pd.read_csv(three_labcas_1comp_path, sep=',').assign(sim=300*[3,], processing=300*['dirty cas',])
       three_labcas_2comp = pd.read_csv(three_labcas_2comp_path, sep=',').assign(sim=300*[3,], processing=300*['dirty cas',])
       af488luv_0cd20cas_1comp = pd.read_csv(af488luv_0cd20cas_1comp_path, sep=',').assign(sim=440*[280,], processing=440*['dirty cas'])
       af488luv_0cd20cas_2comp = pd.read_csv(af488luv_0cd20cas_2comp_path, sep=',').assign(sim=440*[280,], processing=440*['dirty cas'])
       tbpex5_0cd20cas_1comp = pd.read_csv(tbpex5_0cd20cas_1comp_path, sep=',').assign(sim=250*[31,], processing=250*['dirty cas'])
       tbpex5_0cd20cas_2comp = pd.read_csv(tbpex5_0cd20cas_2comp_path, sep=',').assign(sim=250*[31,], processing=250*['dirty cas'])


       odot069_labavg_1comp = pd.read_csv(odot069_labavg_1comp_path, sep=',').assign(sim=300*[0.069,], processing=300*['dirty avg',])
       odot069_labavg_2comp = pd.read_csv(odot069_labavg_2comp_path, sep=',').assign(sim=300*[0.069,], processing=300*['dirty avg',])
       odot2_labavg_1comp = pd.read_csv(odot2_labavg_1comp_path, sep=',').assign(sim=300*[0.2,], processing=300*['dirty avg',])
       odot2_labavg_2comp = pd.read_csv(odot2_labavg_2comp_path, sep=',').assign(sim=300*[0.2,], processing=300*['dirty avg',])
       three_labavg_1comp = pd.read_csv(three_labavg_1comp_path, sep=',').assign(sim=300*[3,], processing=300*['dirty avg',])
       three_labavg_2comp = pd.read_csv(three_labavg_2comp_path, sep=',').assign(sim=300*[3,], processing=300*['dirty avg',])
       af488luv_0cd20avg_1comp = pd.read_csv(af488luv_0cd20avg_1comp_path, sep=',').assign(sim=440*[280,], processing=440*['dirty avg'])
       af488luv_0cd20avg_2comp = pd.read_csv(af488luv_0cd20avg_2comp_path, sep=',').assign(sim=440*[280,], processing=440*['dirty avg'])
       tbpex5_0cd20avg_1comp = pd.read_csv(tbpex5_0cd20avg_1comp_path, sep=',').assign(sim=250*[31,], processing=250*['dirty avg'])
       tbpex5_0cd20avg_2comp = pd.read_csv(tbpex5_0cd20avg_2comp_path, sep=',').assign(sim=250*[31,], processing=250*['dirty avg'])


       # # control with prediction threshold
       # odot069_thresh2_1comp = pd.read_csv(odot069_thresh2_1comp_path, sep=',').assign(sim=300*[0.069,], processing=300*['Peak artifacts,\nmanual thresh. pred.\n+ cut and shift corr.:\nscaler=robust\nthreshold=2',])
       # odot069_thresh2_2comp = pd.read_csv(odot069_thresh2_2comp_path, sep=',').assign(sim=300*[0.069,], processing=300*['Peak artifacts,\nmanual thresh. pred.\n+ cut and shift corr.:\nscaler=robust\nthreshold=2',])
       # odot2_thresh2_1comp = pd.read_csv(odot2_thresh2_1comp_path, sep=',').assign(sim=300*[0.2,], processing=300*['Peak artifacts,\nmanual thresh. pred.\n+ cut and shift corr.:\nscaler=robust\nthreshold=2',])
       # odot2_thresh2_2comp = pd.read_csv(odot2_thresh2_2comp_path, sep=',').assign(sim=300*[0.2,], processing=300*['Peak artifacts,\nmanual thresh. pred.\n+ cut and shift corr.:\nscaler=robust\nthreshold=2',])
       # # one_thresh2_1comp = pd.read_csv(one_thresh2_1comp_path, sep=',').assign(sim=300*[1,], processing=300*['Peak artifacts,\nmanual thresh. pred.\n+ cut and shift corr.:\nscaler=robust\nthreshold=2',])
       # # one_thresh2_2comp = pd.read_csv(one_thresh2_2comp_path, sep=',').assign(sim=300*[1,], processing=300*['Peak artifacts,\nmanual thresh. pred.\n+ cut and shift corr.:\nscaler=robust\nthreshold=2',])
       # three_thresh2_1comp = pd.read_csv(three_thresh2_1comp_path, sep=',').assign(sim=300*[3,], processing=300*['Peak artifacts,\nmanual thresh. pred.\n+ cut and shift corr.:\nscaler=robust\nthreshold=2',])
       # three_thresh2_2comp = pd.read_csv(three_thresh2_2comp_path, sep=',').assign(sim=300*[3,], processing=300*['Peak artifacts,\nmanual thresh. pred.\n+ cut and shift corr.:\nscaler=robust\nthreshold=2',])
       #
       # # pred. by best unet 0cd20 - check out from branch exp-220227-unet
       # odot069_0cd20_1comp = pd.read_csv(odot069_0cd20_1comp_path, sep=',').assign(sim=300*[0.069,], processing=300*['Peak artifacts,\nautomatic U-Net pred.\n+ cut and shift corr.:\nscaler=quantile transf.,\nlarge model (200MB)',])
       # odot069_0cd20_2comp = pd.read_csv(odot069_0cd20_2comp_path, sep=',').assign(sim=300*[0.069,], processing=300*['Peak artifacts,\nautomatic U-Net pred.\n+ cut and shift corr.:\nscaler=quantile transf.,\nlarge model (200MB)',])
       # odot2_0cd20_1comp = pd.read_csv(odot2_0cd20_1comp_path, sep=',').assign(sim=300*[0.2,], processing=300*['Peak artifacts,\nautomatic U-Net pred.\n+ cut and shift corr.:\nscaler=quantile transf.,\nlarge model (200MB)',])
       # odot2_0cd20_2comp = pd.read_csv(odot2_0cd20_2comp_path, sep=',').assign(sim=300*[0.2,], processing=300*['Peak artifacts,\nautomatic U-Net pred.\n+ cut and shift corr.:\nscaler=quantile transf.,\nlarge model (200MB)',])
       # # one_0cd20_1comp = pd.read_csv(one_0cd20_1comp_path, sep=',').assign(sim=300*[1,], processing=300*['Peak artifacts,\nautomatic U-Net pred.\n+ cut and shift corr.:\nscaler=quantile transf.,\nlarge model (200MB)',])
       # # one_0cd20_2comp = pd.read_csv(one_0cd20_2comp_path, sep=',').assign(sim=300*[1,], processing=300*['Peak artifacts,\nautomatic U-Net pred.\n+ cut and shift corr.:\nscaler=quantile transf.,\nlarge model (200MB)',])
       # three_0cd20_1comp = pd.read_csv(three_0cd20_1comp_path, sep=',').assign(sim=300*[3,], processing=300*['Peak artifacts,\nautomatic U-Net pred.\n+ cut and shift corr.:\nscaler=quantile transf.,\nlarge model (200MB)',])
       # three_0cd20_2comp = pd.read_csv(three_0cd20_2comp_path, sep=',').assign(sim=300*[3,], processing=300*['Peak artifacts,\nautomatic U-Net pred.\n+ cut and shift corr.:\nscaler=quantile transf.,\nlarge model (200MB)',])



       all_param = pd.concat([odot069_clean_1comp, odot2_clean_1comp, three_clean_1comp,
                              af488_1comp, hspex5_1comp,
                              odot069_dirty_1comp, odot2_dirty_1comp, three_dirty_1comp,
                              odot069_dirty_2comp, odot2_dirty_2comp, three_dirty_2comp,
                              af488luv_1comp, tbpex5_1comp,
                              af488luv_2comp, tbpex5_2comp,
                              odot069_labdel_1comp, odot2_labdel_1comp, three_labdel_1comp,
                              odot069_labdel_2comp, odot2_labdel_2comp, three_labdel_2comp,
                              af488luv_0cd20del_1comp, tbpex5_0cd20del_1comp,
                              af488luv_0cd20del_2comp, tbpex5_0cd20del_2comp,
                              odot069_labcas_1comp, odot2_labcas_1comp, three_labcas_1comp,
                              odot069_labcas_2comp, odot2_labcas_2comp, three_labcas_2comp,
                              af488luv_0cd20cas_1comp, tbpex5_0cd20cas_1comp,
                              af488luv_0cd20cas_2comp, tbpex5_0cd20cas_2comp,
                              odot069_labavg_1comp, odot2_labavg_1comp, three_labavg_1comp,
                              odot069_labavg_2comp, odot2_labavg_2comp, three_labavg_2comp,
                              af488luv_0cd20avg_1comp, tbpex5_0cd20avg_1comp,
                              af488luv_0cd20avg_2comp, tbpex5_0cd20avg_2comp])


       #                        odot069_thresh2_1comp, odot069_thresh2_2comp, odot2_thresh2_1comp,
       #                        odot2_thresh2_2comp, three_thresh2_1comp, three_thresh2_2comp,
       #                        odot069_0cd20_1comp, odot069_0cd20_2comp, odot2_0cd20_1comp,
       #                        odot2_0cd20_2comp, three_0cd20_1comp, three_0cd20_2comp])

       # assert the following fit parameters
       assert set(all_param['Dimen']) == {'2D', '3D'}
       assert set(all_param[all_param['Dimen'] == '2D']['sim']) == {0.069, 0.2, 3.0}
       assert set(all_param[all_param['Dimen'] == '3D']['sim']) == {31.0, 280.0}
       assert set(all_param[(all_param['Dimen'] == '3D') & (all_param['sim'] == 31.0)]['AR1']) == {6.0}
       assert set(all_param[(all_param['Dimen'] == '3D') & (all_param['sim'] == 280.0)]['AR1']) == {5.0}
       assert set(all_param['Diff_eq']) == {'Equation 1A', 'Equation 1B'}
       assert set(all_param[all_param['Diff_eq'] == 'Equation 1A']['sim']) == {0.069, 0.2, 3.0}
       assert set(all_param[all_param['Diff_eq'] == 'Equation 1B']['sim']) == {31.0, 280.0}
       assert set(all_param['Triplet_eq']) == {'Triplet Eq 2B', 'no triplet'}
       assert set(all_param[all_param['Triplet_eq'] == 'no triplet']['sim']) == {0.069, 0.2, 3.0, 280.0}
       assert set(all_param[all_param['Triplet_eq'] == 'Triplet Eq 2B']['sim']) == {31.0}
       assert set(all_param['alpha1']) == {1.0}
       assert set(all_param['xmin']) == {0.001018, 1.0}
       assert set(all_param[all_param['xmin'] == 1.0]['sim']) == {0.069, 0.2, 3.0}
       assert set(all_param[all_param['xmin'] == 0.001018]['sim']) == {31.0, 280.0}
       assert set(all_param['xmax']) == {0.524282, 100.66329, 469.762042, 512.0, 896.0, 939.52409, 1024.0, 2048.0, 4096.0, 7168.0, 8192.0}
       # biological af488 correlations were fitted with xmax=500 for peak artifacts, except xmax=0.5 for peak artifacts with averaging correction,
       # and xmax=100 for correlations without peak artifacts,
       assert set(all_param[all_param['xmax'] == 0.524282]['sim']) == {31.0, 280.0}
       assert set(all_param[all_param['xmax'] == 0.524282]['processing']) == {'dirty avg'}
       assert set(all_param[all_param['xmax'].isin([100.66329, 469.762042])]['sim']) == {280.0}
       assert set(all_param[all_param['xmax'] == 100.66329]['processing']) == {'clean noc'}
       assert set(all_param[all_param['xmax'] == 469.762042]['processing']) == {'dirty cas', 'dirty del', 'dirty noc'}
       assert set(all_param[all_param['xmax'].isin([512.0, 896.0, 1024.0, 2048.0, 4096.0, 7168.0, 8192.0])]['sim']) == {0.069, 0.2, 3.0}
       # simulated correlations with and without peak artifacts were fitted with xmax=8192 (except see below)
       assert set(all_param[all_param['xmax'] == 8192.0]['processing']) == {'clean noc', 'dirty cas', 'dirty del', 'dirty noc'}
       # 294 of 300 correlations with peak artifacts and averaging correction were fitted with xmax=1024,
       # this failed for 6 correlations which were too short and thus automatically got xmax=512 or xmax=896.0
       assert len(set(all_param[(all_param['xmax'] == 1024.0) & (all_param['processing'] == 'dirty avg')].index)) == 294
       assert set(all_param[all_param['xmax'].isin([512.0, 896.0])]['processing']) == {'dirty avg'}
       assert len(set(all_param[all_param['xmax'].isin([512.0, 896.0])].index)) == 6
       # 279, 288, or 276 of 300 correlations with peak artifacts and cut and stitch correction were fitted with xmax=8192
       # (3 groups depending on simulated molecule speed)
       # this failed for 55 correlations which were too short and thus automatically got xmax={1024, 2048, 4096, 7168}
       assert len(set(all_param[(all_param['xmax'] == 8192.0) & (all_param['processing'] == 'dirty cas') & (all_param['sim'] == 0.069)].index))
       assert len(set(all_param[(all_param['xmax'] == 8192.0) & (all_param['processing'] == 'dirty cas') & (all_param['sim'] == 0.2)].index))
       assert len(set(all_param[(all_param['xmax'] == 8192.0) & (all_param['processing'] == 'dirty cas') & (all_param['sim'] == 3.0)].index))
       assert set(all_param[all_param['xmax'].isin([2048.0, 4096.0, 7168.0])]['processing']) == {'dirty cas'}
       assert len(set(all_param[(all_param['xmax'].isin([2048.0, 4096.0, 7168.0])) |
                                ((all_param['xmax'] == 1024) & (all_param['processing'] == 'dirty cas'))].index)) == 55

       pprint(all_param.keys())
       all_param = all_param[['name_of_plot', 'Diff_species', 'N (FCS)', 'A1', 'txy1', 'sim', 'processing', 'A2', 'txy2']]
       with pd.option_context("max_colwidth", 1000):
           display(all_param)
     #+END_SRC

     #+RESULTS:
     :RESULTS:
     : Index(['name_of_plot', 'master_file', 'parent_name', 'parent_uqid',
     :        'time of fit', 'Diff_eq', 'Diff_species', 'Triplet_eq',
     :        'Triplet_species', 'Dimen', 'xmin', 'xmax', 'offset', 'stdev(offset)',
     :        'GN0', 'stdev(GN0)', 'N (FCS)', 'cpm (kHz)', 'A1', 'stdev(A1)', 'txy1',
     :        'stdev(txy1)', 'alpha1', 'stdev(alpha1)', 'N (mom)', 'bri (kHz)',
     :        'above zero', 'sim', 'processing', 'AR1', 'stdev(AR1)', 'T1',
     :        'stdev(T1)', 'tauT1', 'stdev(tauT1)', 'A2', 'stdev(A2)', 'txy2',
     :        'stdev(txy2)', 'alpha2', 'stdev(alpha2)', 'AR2', 'stdev(AR2)'],
     :       dtype='object')
     |     | name_of_plot                                                                                     | Diff_species | N (FCS)   | A1       | txy1       | sim    | processing | A2       | txy2    |
|-----+--------------------------------------------------------------------------------------------------+--------------+-----------+----------+------------+--------+------------+----------+---------|
| 0   | 2022-07-21_multipletau_clean_0dot069_0000_correlation-CH1_1                                      | 1            | 13.940974 | 1.000000 | 150.165642 | 0.069  | clean noc  | NaN      | NaN     |
| 1   | 2022-07-21_multipletau_clean_0dot069_0001_correlation-CH1_1                                      | 1            | 16.272292 | 1.000000 | 133.758850 | 0.069  | clean noc  | NaN      | NaN     |
| 2   | 2022-07-21_multipletau_clean_0dot069_0002_correlation-CH1_1                                      | 1            | 17.590156 | 1.000000 | 109.396491 | 0.069  | clean noc  | NaN      | NaN     |
| 3   | 2022-07-21_multipletau_clean_0dot069_0003_correlation-CH1_1                                      | 1            | 13.739355 | 1.000000 | 140.176075 | 0.069  | clean noc  | NaN      | NaN     |
| 4   | 2022-07-21_multipletau_clean_0dot069_0004_correlation-CH1_1                                      | 1            | 10.985121 | 1.000000 | 520.784146 | 0.069  | clean noc  | NaN      | NaN     |
| ... | ...                                                                                              | ...          | ...       | ...      | ...        | ...    | ...        | ...      | ...     |
| 245 | 2023-01-18_tttr2xfcs_with_averaging_CH2_BIN1dot0_TbPEX5EGFP 1-1000246_T5364s_1_correlation-CH2_2 | 2            | 1.717779  | 0.554806 | 0.007325   | 31.000 | dirty avg  | 0.445194 | 0.34921 |
| 246 | 2023-01-18_tttr2xfcs_with_averaging_CH2_BIN1dot0_TbPEX5EGFP 1-1000247_T5386s_1_correlation-CH2_2 | 2            | 1.717779  | 0.554806 | 0.007325   | 31.000 | dirty avg  | 0.445194 | 0.34921 |
| 247 | 2023-01-18_tttr2xfcs_with_averaging_CH2_BIN1dot0_TbPEX5EGFP 1-1000248_T5408s_1_correlation-CH2_2 | 2            | 1.717779  | 0.554806 | 0.007325   | 31.000 | dirty avg  | 0.445194 | 0.34921 |
| 248 | 2023-01-18_tttr2xfcs_with_averaging_CH2_BIN1dot0_TbPEX5EGFP 1-1000249_T5430s_1_correlation-CH2_2 | 2            | 1.717779  | 0.554806 | 0.007325   | 31.000 | dirty avg  | 0.445194 | 0.34921 |
| 249 | 2023-01-18_tttr2xfcs_with_averaging_CH2_BIN1dot0_TbPEX5EGFP 1-1000250_T5451s_1_correlation-CH2_2 | 2            | 1.717779  | 0.554806 | 0.007325   | 31.000 | dirty avg  | 0.445194 | 0.34921 |

14294 rows × 9 columns
     :END:

     #+BEGIN_SRC jupyter-python
       def sort_fit(param_ls):
           sim = param_ls[-1]
           nfcs = param_ls[-2]

           tt, tt_low_high = ans.convert_diffcoeff_to_transittimes(sim, fwhm=250)

           array = np.array(list(param_ls)[:-2]).reshape((2, 2))
           # sort by transit times
           array = array[:, array[0, :].argsort()]
           A_fast = array[1, 0]
           A_slow = array[1, 1]
           N_fast = A_fast * nfcs
           N_slow = A_slow * nfcs
           t_fast = array[0, 0]
           t_slow = array[0, 1]
           if np.isnan(t_slow):
               # if tt_low_high[0] <= t_fast <= tt_low_high[1]:
               #     out =

               out = t_fast, nfcs, pd.NA, pd.NA, tt

           elif f'{A_fast:.0%}' == '100%':
               # if tt_low_high[0] <= t_fast <= tt_low_high[1]:
               #     out =

               out = t_fast, N_fast, pd.NA, pd.NA, tt
           elif f'{A_slow:.0%}' == '100%':
               # if tt_low_high[0] <= t_slow <= tt_low_high[1]:
               #     out =
               out = pd.NA, pd.NA, t_slow, N_slow, tt
           else:
               # if (tt_low_high[0] <= t_fast <= tt_low_high[1]) or (
               #     tt_low_high[0] <= t_slow <= tt_low_high[1]):
               #     out =
               out = t_fast, N_fast, t_slow, N_slow, tt

           return out

       def sort_fit_legend(param_ls):
           species = param_ls[0]
           component = param_ls[1]

           if species == 1:
               legend = '$\\tau_D$ from\n1 species fit'
           elif (species == 2) and (component == 'fast'):
               legend = '$\\tau_D$ from\nfast sp. of 2 sp. fit'
           elif (species == 2) and (component == 'slow'):
               legend = '$\\tau_D$ from\nslow sp. of 2 sp. fit'
           return legend

       all_param[['t_fast', 'N_fast', 't_slow', 'N_slow', 'expected transit time']] = all_param[['txy1', 'txy2', 'A1', 'A2', 'N (FCS)', 'sim']].apply(
           lambda x: sort_fit(x), axis=1, result_type='expand')
       all_param = pd.wide_to_long(all_param, stubnames=['t', 'N'],
                                   i=['name_of_plot', 'Diff_species'],
                                   j='fit component',
                                   sep='_', suffix=r'\w+')

       all_param = all_param.reset_index()
       # if Diff_species is 1, there is only 1 component
       all_param = all_param[~((all_param['fit component'] == 'slow') & (all_param['Diff_species'] == 1))]
       all_param = all_param.reset_index()

       all_param['legend'] = all_param[['Diff_species', 'fit component']].apply(
           lambda x: sort_fit_legend(x), axis=1)
       print('before dropping NaNs')
       print('1 species fit: {}'.format(len(all_param[all_param["legend"] == "$\\tau_D$ from\n1 species fit"])))
       print('slow sp of 2 sp fit: {}'.format(len(all_param[all_param["legend"] == "$\\tau_D$ from\nslow sp. of 2 sp. fit"])))
       print('fast sp of 2 sp fit: {}'.format(len(all_param[all_param["legend"] == "$\\tau_D$ from\nfast sp. of 2 sp. fit"])))

       all_param = all_param[~pd.isna(all_param['t'])]
       print('after dropping NaNs')
       print('1 species fit: {}'.format(len(all_param[all_param["legend"] == "$\\tau_D$ from\n1 species fit"])))
       print('slow sp of 2 sp fit: {}'.format(len(all_param[all_param["legend"] == "$\\tau_D$ from\nslow sp. of 2 sp. fit"])))
       print('fast sp of 2 sp fit: {}'.format(len(all_param[all_param["legend"] == "$\\tau_D$ from\nfast sp. of 2 sp. fit"])))

       all_param = all_param[['legend', 't', 'N', 'expected transit time', 'sim', 'processing']]
       all_param.loc[:, ['t', 'N']] = all_param.loc[:, ['t', 'N']].apply(pd.to_numeric)
       all_param.loc[:, 't'] = all_param.loc[:, 't'].apply(lambda x: np.log10(x))

       # with pd.option_context("max_colwidth", 1000):
       #     display(all_param[['legend', 't', 'A', 'expected transit time', 'sim', 'processing']])
       pub_cond1 = ((all_param['legend'] == '$\\tau_D$ from\n1 species fit') &
                    ~((all_param['processing'].isin(['dirty noc', 'dirty del', 'dirty cas'])) &
                      (all_param['expected transit time'] == 0.040253767881946526)) &
                    ~((all_param['processing'] == 'dirty avg') &
                      (all_param['expected transit time'] == 0.36358241957887183)))
       pub_cond2 = ((all_param['legend'] == '$\\tau_D$ from\nfast sp. of 2 sp. fit') &
                    ((all_param['processing'].isin(['dirty noc', 'dirty del', 'dirty cas'])) &
                     (all_param['expected transit time'] == 0.040253767881946526)))
       pub_cond3 = ((all_param['legend'] == "$\\tau_D$ from\nslow sp. of 2 sp. fit") &
                    ((all_param['processing'] == 'dirty avg') &
                     (all_param['expected transit time'] == 0.36358241957887183)))

       pub_param = all_param[pub_cond1 | pub_cond2 | pub_cond3]
       # compare_correction_param = all_param[(all_param['legend'] == '\nsim. trace\nn=300, $\\tau_D$ from\n1 species fit') &
       #                                      ~((all_param['processing'] == 'Peak artifacts,\nmanual thresh. pred.\n+ cut and shift corr.:\nscaler=robust\nthreshold=2') |
       #                                        (all_param['processing'] == 'Peak artifacts,\nautomatic U-Net pred.\n+ cut and shift corr.:\nscaler=quantile transf.,\nlarge model (200MB)'))]
       pub_param
     #+END_SRC

     #+RESULTS:
     :RESULTS:
     : before dropping NaNs
     : 1 species fit: 7934
     : slow sp of 2 sp fit: 6360
     : fast sp of 2 sp fit: 6360
     : after dropping NaNs
     : 1 species fit: 7934
     : slow sp of 2 sp fit: 5459
     : fast sp of 2 sp fit: 5812
     |       | legend                               | t         | N         | expected transit time | sim    | processing |
|-------+--------------------------------------+-----------+-----------+-----------------------+--------+------------|
| 0     | $\tau_D$ from\n1 species fit         | 2.176571  | 13.940974 | 163.348623            | 0.069  | clean noc  |
| 1     | $\tau_D$ from\n1 species fit         | 2.126323  | 16.272292 | 163.348623            | 0.069  | clean noc  |
| 2     | $\tau_D$ from\n1 species fit         | 2.039003  | 17.590156 | 163.348623            | 0.069  | clean noc  |
| 3     | $\tau_D$ from\n1 species fit         | 2.146674  | 13.739355 | 163.348623            | 0.069  | clean noc  |
| 4     | $\tau_D$ from\n1 species fit         | 2.716658  | 10.985121 | 163.348623            | 0.069  | clean noc  |
| ...   | ...                                  | ...       | ...       | ...                   | ...    | ...        |
| 20645 | $\tau_D$ from\nslow sp. of 2 sp. fit | -0.456913 | 0.764745  | 0.363582              | 31.000 | dirty avg  |
| 20647 | $\tau_D$ from\nslow sp. of 2 sp. fit | -0.456913 | 0.764745  | 0.363582              | 31.000 | dirty avg  |
| 20649 | $\tau_D$ from\nslow sp. of 2 sp. fit | -0.456913 | 0.764745  | 0.363582              | 31.000 | dirty avg  |
| 20651 | $\tau_D$ from\nslow sp. of 2 sp. fit | -0.456913 | 0.764745  | 0.363582              | 31.000 | dirty avg  |
| 20653 | $\tau_D$ from\nslow sp. of 2 sp. fit | -0.456913 | 0.764745  | 0.363582              | 31.000 | dirty avg  |

7934 rows × 6 columns
     :END:

     #+BEGIN_SRC jupyter-python
       list(set(all_param['expected transit time']))[::-1]
       all_param.loc[:, 't'].apply(lambda x: np.log10(x))
     #+END_SRC

     #+RESULTS:
     #+begin_example
       0        2.176571
       1        2.126323
       2        2.039003
       3        2.146674
       4        2.716658
                  ...
       20649   -0.456913
       20650   -2.135192
       20651   -0.456913
       20652   -2.135192
       20653   -0.456913
       Name: t, Length: 19205, dtype: float64
     #+end_example

     #+BEGIN_SRC jupyter-python
       def simplot(data, x, height, aspect, hue, xlim, add_text='other'):
           g = sns.catplot(data=data,
                           x=x,
                           y='processing',
                           order=['clean noc', 'dirty noc', 'dirty cas', 'dirty del', 'dirty avg'],
                           col='expected transit time',
                           col_wrap=3,
                           hue=hue,
                           height=height,
                           aspect=aspect,
                           legend_out=True,
                           kind='violin',
                           sharex=True,
                           showfliers=False,
                           scale='width')
           if hue is not None:
               g._legend.remove()
           styles = ['--', ':', '-', '-.', (0, (3, 1, 1, 1, 1, 1)), (0, (3, 1, 1, 1, 1, 1, 1, 1))]
           for i, ax in enumerate(g.axes):
               tt = str(ax.title).split('= ')
               tt = tt[1].strip("')")
               tt = float(tt)
               clean = data[(data['processing'] == 'clean noc') & (data['expected transit time'] == tt)]
               median = clean[x].median()
               if x == 'N':
                   line = ax.axvline(median, lw=4, label='', ls=styles[::-1][i])
                   line_legend = {f'\n$N{{exp}}={median:.2f}$' : line}
               else:
                   line = ax.axvline(median, lw=4, label='', ls=styles[i])
                   line_legend = {f'\n$\\tau_{{exp}}={median:.2f}ms$' : line}
               g._legend_data.update(line_legend)
           g.add_legend(g._legend_data)
           if hue is None:
               dodge = False
           else:
               dodge = True
           g.map_dataframe(sns.stripplot,
                 x=x,
                 y='processing',
                 order=['clean noc', 'dirty noc', 'dirty cas', 'dirty del', 'dirty avg'],
                 hue=hue,
                 dodge=dodge,
                 palette=sns.color_palette(['0.3']),
                 size=3,
                 jitter=0.1,
                 hue_order=['$\\tau_D$ from\n1 species fit', '$\\tau_D$ from\nfast sp. of 2 sp. fit', '$\\tau_D$ from\nslow sp. of 2 sp. fit'])

           g.fig.suptitle('', size=25)
           for ax in g.axes:
               # ax[0].set_title('')
               tt = str(ax.title).split('= ')
               tt = tt[1].strip("')")
               tt = float(tt)
               clean = data[(data['processing'] == 'clean noc') & (data['expected transit time'] == tt)]
               median = clean[x].median()
               if x == 'N':
                   ax.set_title(f'$N_{{exp}}={median:.2f}$')
               else:
                   ax.set_title(f'$\\tau_{{exp}}={median:.2f}ms$')

           if x == 't':
               xscale = 'linear'
           else:
               xscale = 'linear'

           plt.setp(g.axes, xscale=xscale, xlabel='log transit time $\tau_{D}$ $[ms]$',
                    ylabel='', xlim=xlim)
           for i, ax in enumerate(g.axes):
               xlab = ax.get_xticklabels()
               # because seaborns violinplot does not support kde calculation in log values,
               # I have to do this manually, by first log-transforming the data, now
               # extracting the xticklabels and manually transforming them back.
               xlab_power = [10**lab.get_position()[0] for lab in xlab]
               ax.set_xticklabels(xlab_power)
           g.tight_layout()
           savefig = f'./data/exp-220316-publication1/jupyter/plot2_{add_text}'
           plt.savefig(f'{savefig}.pdf', dpi=300)
           os.system(f'pdf2svg {savefig}.pdf {savefig}.svg')
           plt.close('all')
     #+END_SRC

     #+RESULTS:

   - the following constraints are given by the nanoletters template:
     - one column: up to 240 points wide (3.33 in.)
     - double-column: between 300 and 504 points (4.167 in. and 7 in.).
     - maximum depth: 660 points (9.167 in.) including the caption (allow 12 pts.
       For each line of caption text)
     - Lettering should be no smaller than 4.5 points in the final published
       format. The text should be legible when the graphic is viewed full-size.
       Helvetica or Arial fonts work well for lettering. Lines should be no
       thinner than 0.5 point.

   - plot simulated data - transit times
     #+BEGIN_SRC jupyter-python
       sns.set_theme(style="whitegrid", font_scale=1, palette='colorblind',
                     context='paper')
       # sns.set(rc={'figure.figsize':(3.33, 7)})
       simplot(pub_param[~(pub_param['expected transit time'].isin([0.040253767881946526, 0.36358241957887183]))],
               x='t', height=3, aspect=3/3, hue=None, xlim=[1, 10000], add_text='compare-correction-sim_transit-times')
     #+END_SRC

     #+RESULTS:

   - plot biological af488 data - transit times
     #+BEGIN_SRC jupyter-python
       sns.set_theme(style="whitegrid", font_scale=1, palette='colorblind',
                     context='paper')
       # sns.set(rc={'figure.figsize':(3.33, 7)})
       simplot(pub_param[(pub_param['expected transit time'] == 0.040253767881946526)],
               x='t', height=3, aspect=3/3, hue=None, xlim=None, add_text='compare-correction-af488_transit-times')
     #+END_SRC

     #+RESULTS:

   - plot biological af488 data - particle numbers
     #+BEGIN_SRC jupyter-python
       sns.set_theme(style="whitegrid", font_scale=1, palette='colorblind',
                     context='paper')
       # sns.set(rc={'figure.figsize':(3.33, 7)})
       simplot(pub_param[(pub_param['expected transit time'] == 0.040253767881946526)],
               x='N', height=3, aspect=3/3, hue=None, xlim=None, add_text='compare-correction-af488_particle-numbers')
     #+END_SRC

     #+RESULTS:

   - plot biological pex5 data - transit times
     #+BEGIN_SRC jupyter-python
       sns.set_theme(style="whitegrid", font_scale=1, palette='colorblind',
                     context='paper')
       # sns.set(rc={'figure.figsize':(3.33, 7)})
       simplot(pub_param[(pub_param['expected transit time'] == 0.36358241957887183)],
               x='t', height=3, aspect=3/3, hue=None, xlim=[0.1, 10], add_text='compare-correction-pex5_transit-times')
     #+END_SRC

     #+RESULTS:
   - plot biological pex5 data - particle numbers
     #+BEGIN_SRC jupyter-python
       sns.set_theme(style="whitegrid", font_scale=1, palette='colorblind',
                     context='paper')
       # sns.set(rc={'figure.figsize':(3.33, 7)})
       simplot(pub_param[(pub_param['expected transit time'] == 0.36358241957887183)],
               x='N', height=3, aspect=3/3, hue=None, xlim=[0, 2], add_text='compare-correction-pex5_particle-numbers')
     #+END_SRC

     #+RESULTS:

   - for supplementary, plot 1 and 2 species fits for every category for transit times
     #+BEGIN_SRC jupyter-python
       sns.set_theme(style="whitegrid", font_scale=1, palette='colorblind',
                     context='paper')
       # sns.set(rc={'figure.figsize':(3.33, 7)})
       simplot(all_param,
               x='t', height=4, aspect=3/4, hue='legend', xlim=None, add_text='compare-correction-sim-bio-allfits')
     #+END_SRC

     #+RESULTS:

   - for supplementary, plot 1 and 2 species fits for simulated data - transit times
     #+BEGIN_SRC jupyter-python
       sns.set_theme(style="whitegrid", font_scale=1, palette='colorblind',
                     context='paper')
       # sns.set(rc={'figure.figsize':(3.33, 7)})
       simplot(all_param[~(all_param['expected transit time'].isin([0.040253767881946526, 0.36358241957887183]))],
               x='t', height=4, aspect=3/4, hue='legend', xlim=None, add_text='compare-correction-sim_transit-times-allfits')
     #+END_SRC

     #+RESULTS:

   - for supplementary, plot 1 and 2 species fits for af488 particle numbers
     #+BEGIN_SRC jupyter-python
       sns.set_theme(style="whitegrid", font_scale=1, palette='colorblind',
                     context='paper')
       # sns.set(rc={'figure.figsize':(3.33, 7)})
       simplot(all_param[all_param['expected transit time'] == 0.040253767881946526],
               x='N', height=4, aspect=3/4, hue='legend', xlim=None, add_text='compare-correction-af488_particle-numbers-allfits')
     #+END_SRC

     #+RESULTS:

   - for supplementary, plot 1 and 2 species fits for af488 transit times
     #+BEGIN_SRC jupyter-python
       sns.set_theme(style="whitegrid", font_scale=1, palette='colorblind',
                     context='paper')
       # sns.set(rc={'figure.figsize':(3.33, 7)})
       simplot(all_param[all_param['expected transit time'] == 0.040253767881946526],
               x='t', height=4, aspect=3/4, hue='legend', xlim=None, add_text='compare-correction-af488_transit-times-allfits')
     #+END_SRC

     #+RESULTS:

   - for supplementary, plot 1 and 2 species fits for pex5 transit times
     #+BEGIN_SRC jupyter-python
       sns.set_theme(style="whitegrid", font_scale=1, palette='colorblind',
                     context='paper')
       # sns.set(rc={'figure.figsize':(3.33, 7)})
       simplot(all_param[(all_param['expected transit time'] == 0.36358241957887183)],
               x='t', height=4, aspect=3/4, hue='legend', xlim=None, add_text='compare-correction-pex5_transit-times-allfits')
     #+END_SRC

     #+RESULTS:

   - for supplementary, plot 1 and 2 species fits for pex5 particle numbers
     #+BEGIN_SRC jupyter-python
       sns.set_theme(style="whitegrid", font_scale=1, palette='colorblind',
                     context='paper')
       # sns.set(rc={'figure.figsize':(3.33, 7)})
       simplot(all_param[(all_param['expected transit time'] == 0.36358241957887183)],
               x='N', height=4, aspect=3/4, hue='legend', xlim=[0, 3], add_text='compare-correction-pex5_particle-numbers-allfits')
     #+END_SRC

     #+RESULTS:

   - try violin plots
     #+BEGIN_SRC jupyter-python
       pub_param.loc[:, ['t', 'N']].info()
     #+END_SRC

     #+RESULTS:
     : <class 'pandas.core.frame.DataFrame'>
     : Int64Index: 7934 entries, 0 to 20653
     : Data columns (total 2 columns):
     :  #   Column  Non-Null Count  Dtype
     : ---  ------  --------------  -----
     :  0   t       7934 non-null   float64
     :  1   N       7934 non-null   float64
     : dtypes: float64(2)
     : memory usage: 186.0 KB

     #+BEGIN_SRC jupyter-python
       sns.set_theme(style="whitegrid", font_scale=1, palette='colorblind',
                     context='paper')
       # sns.set(rc={'figure.figsize':(3.33, 7)})
       simplot(pub_param[~(pub_param['expected transit time'].isin([0.040253767881946526, 0.36358241957887183]))],
               x='t', height=3, aspect=3/3, hue=None, xlim=[-1, 5], add_text='violin-fliers')
     #+END_SRC

     #+RESULTS:
     : /tmp/ipykernel_54954/3269630507.py:74: UserWarning: FixedFormatter should only be used together with FixedLocator
     :   ax.set_xticklabels(xlab_power)

     #+BEGIN_SRC jupyter-python
       g = sns.violinplot(data=pub_param[(pub_param['expected transit time'].isin([0.040253767881946526]))],
                          x='t', y='processing', order=['clean noc', 'dirty noc', 'dirty cas', 'dirty del', 'dirty avg'],
                          scale='width', cut=0)
       plt.setp(g.axes, xscale='log', xlabel='log transit time $\tau_{D}$ $[ms]$',
                          ylabel='', xlim=None)
     #+END_SRC

     #+RESULTS:
     :RESULTS:
          | hline | Text | (0.5 0 log transit time $\tau_{D}$ $[ms]$) | Text | (0 0.5 ) | 0.003219436230485522 | 1.3012831561711415 |
     [[file:./data/exp-220316-publication1/jupyter/1507278416c74d578e0f3ab34027fff1c21786d0.png]]
     :END:

     #+BEGIN_SRC jupyter-python
       pub_param[(pub_param['expected transit time'].isin([0.040253767881946526]))].loc[:, 't']
     #+END_SRC

     #+RESULTS:
     #+begin_example
       900      0.038136
       901      0.039811
       902      0.040138
       903      0.039936
       904      0.039690
                  ...
       19019    0.047309
       19020    0.098125
       19021    0.052949
       19022    0.091594
       19023    0.096936
       Name: t, Length: 2184, dtype: float64
     #+end_example

     #+BEGIN_SRC jupyter-python

       plt.violinplot(dataset=pub_param[(pub_param['expected transit time'].isin([0.040253767881946526]))].loc[:, 't'])
       plt.yscale('log')

     #+END_SRC

     #+RESULTS:
     [[file:./data/exp-220316-publication1/jupyter/650ee6794d085196b2af7ab48d88cd6154621177.png]]

**** comparison correction methods - corr&fit examples
   :PROPERTIES:
   :header-args:jupyter-python: :session /jpy:localhost#8888:a37e524a-8134-4d8f-b24a-367acaf1bdd3 :pandoc t
   :END:

   - call =jupyter-set-output-directory= and =simulations-prepare-modules=
     #+CALL: jupyter-set-output-directory()

     #+RESULTS:
     : ./data/exp-220316-publication1/jupyter

     #+CALL: simulations-prepare-modules()

     #+RESULTS:
     : /home/lex/Programme/drmed-git
     : 2023-01-19 16:29:03.924662: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
     : 2023-01-19 16:29:03.924767: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.

   - let's load and plot cuts vs transit times from 3 different base molecule
     speeds (varying molecule numbers): 0.069, 0.2, and 3.0 um^2/s respectively.
   - this time for *supplementary plot of examples of individual correlations
     and fits*
     #+BEGIN_SRC jupyter-python
       # dirty correlations - check out from  branch exp-220227-unet
       path1 = Path('/home/lex/Programme/drmed-git/data/exp-220227-unet/2022-04-25_simulations/')
       odot069_0dot01_dirty_1comp_ex_path = path1 / '0.069-all-results/0dot069_dirty_1comp_example-0dot01-2657_rawFitData.csv'
       odot069_0dot01_dirty_1comp_av_path = path1 / '0.069-all-results/0dot069_dirty_1comp_100curves-avg-0dot01_rawFitData.csv'
       odot069_0dot1_dirty_1comp_ex_path = path1 / '0.069-all-results/0dot069_dirty_1comp_example-0dot1-1614_rawFitData.csv'
       odot069_0dot1_dirty_1comp_av_path = path1 / '0.069-all-results/0dot069_dirty_1comp_100curves-avg-0dot1_rawFitData.csv'
       odot069_1dot0_dirty_1comp_ex_path = path1 / '0.069-all-results/0dot069_dirty_1comp_example-1dot0-0024_rawFitData.csv'
       odot069_1dot0_dirty_1comp_av_path = path1 / '0.069-all-results/0dot069_dirty_1comp_100curves-avg-1dot0_rawFitData.csv'
       odot2_0dot01_dirty_1comp_ex_path = path1 / '0.2-all-results/0dot2_dirty_1comp_example-0dot01-2219_rawFitData.csv'
       odot2_0dot01_dirty_1comp_av_path = path1 / '0.2-all-results/0dot2_dirty_1comp_100curves-avg-0dot01_rawFitData.csv'
       odot2_0dot1_dirty_1comp_ex_path = path1 / '0.2-all-results/0dot2_dirty_1comp_example-0dot1-0312_rawFitData.csv'
       odot2_0dot1_dirty_1comp_av_path = path1 / '0.2-all-results/0dot2_dirty_1comp_100curves-avg-0dot1_rawFitData.csv'
       odot2_1dot0_dirty_1comp_ex_path = path1 / '0.2-all-results/0dot2_dirty_1comp_example-1dot0-0700_rawFitData.csv'
       odot2_1dot0_dirty_1comp_av_path = path1 / '0.2-all-results/0dot2_dirty_1comp_100curves-avg-1dot0_rawFitData.csv'
       three_0dot01_dirty_1comp_ex_path = path1 / '3.0-all-results/3dot0_dirty_1comp_example-0dot01-0501_rawFitData.csv'
       three_0dot01_dirty_1comp_av_path = path1 / '3.0-all-results/3dot0_dirty_1comp_100curves-avg-0dot01_rawFitData.csv'
       three_0dot1_dirty_1comp_ex_path = path1 / '3.0-all-results/3dot0_dirty_1comp_example-0dot1-2408_rawFitData.csv'
       three_0dot1_dirty_1comp_av_path = path1 / '3.0-all-results/3dot0_dirty_1comp_100curves-avg-0dot1_rawFitData.csv'
       three_1dot0_dirty_1comp_ex_path = path1 / '3.0-all-results/3dot0_dirty_1comp_example-1dot0-0400_rawFitData.csv'
       three_1dot0_dirty_1comp_av_path = path1 / '3.0-all-results/3dot0_dirty_1comp_100curves-avg-1dot0_rawFitData.csv'

       # clean correlations
       path2 = Path('/home/lex/Programme/drmed-git/data/exp-220316-publication1/220719_threshold-prediction/threshold-all-results')
       odot069_clean_1comp_ex_path = path2 / '0dot069_clean_1comp_example-0021_rawFitData.csv'
       odot069_clean_1comp_av_path = path2 / '0dot069_clean_1comp_300curves-avg_rawFitData.csv'
       odot2_clean_1comp_ex_path = path2 / '0dot2_clean_1comp_example-0001_rawFitData.csv'
       odot2_clean_1comp_av_path = path2 / '0dot2_clean_1comp_300curves-avg_rawFitData.csv'
       three_clean_1comp_ex_path = path2 / '3dot0_clean_1comp_example-0002_rawFitData.csv'
       three_clean_1comp_av_path = path2 / '3dot0_clean_1comp_300curves-avg_rawFitData.csv'

       # load correction methods for comparison - here segmentation is given by simulations
       path4 = Path('/home/lex/Programme/drmed-git/data/exp-220316-publication1/220517_simulations/')
       odot069_labcas_1comp_ex_path = path4 / '0.069-all-results/0dot069_lab_cas_1comp_example-1642_rawFitData.csv'
       odot069_labcas_1comp_av_path = path4 / '0.069-all-results/0dot069_lab_cas_1comp_300curves-avg_rawFitData.csv'
       odot2_labcas_1comp_ex_path = path4 / '0.2-all-results/0dot2_lab_cas_1comp_example-0304_rawFitData.csv'
       odot2_labcas_1comp_av_path = path4 / '0.2-all-results/0dot2_lab_cas_1comp_300curves-avg_rawFitData.csv'
       three_labcas_1comp_ex_path = path4 / '3.0-all-results/3dot0_lab_cas_1comp_example-0507_rawFitData.csv'
       three_labcas_1comp_av_path = path4 / '3.0-all-results/3dot0_lab_cas_1comp_300curves-avg_rawFitData.csv'

       odot069_labdel_1comp_ex_path = path4 / '0.069-all-results/0dot069_lab_del_1comp_example-2666_rawFitData.csv'
       odot069_labdel_1comp_av_path = path4 / '0.069-all-results/0dot069_lab_del_1comp_300curves-avg_rawFitData.csv'
       odot2_labdel_1comp_ex_path = path4 / '0.2-all-results/0dot2_lab_del_1comp_example-0304_rawFitData.csv'
       odot2_labdel_1comp_av_path = path4 / '0.2-all-results/0dot2_lab_del_1comp_300curves-avg_rawFitData.csv'
       three_labdel_1comp_ex_path = path4 / '3.0-all-results/3dot0_lab_del_1comp_example-2402_rawFitData.csv'
       three_labdel_1comp_av_path = path4 / '3.0-all-results/3dot0_lab_del_1comp_300curves-avg_rawFitData.csv'

       path5 = Path('/home/lex/Programme/drmed-git/data/exp-220316-publication1/230103_avg-correction/')
       odot069_labavg_1comp_ex_path = path5 / 'all-results/0dot069_lab_avg_1comp_example-0039_rawFitData.csv'
       odot069_labavg_1comp_av_path = path5 / 'all-results/0dot069_lab_avg_1comp_300curves-avg_rawFitData.csv'
       odot2_labavg_1comp_ex_path = path5 / 'all-results/0dot2_lab_avg_1comp_example-0006_rawFitData.csv'
       odot2_labavg_1comp_av_path = path5 / 'all-results/0dot2_lab_avg_1comp_300curves-avg_rawFitData.csv'
       three_labavg_1comp_ex_path = path5 / 'all-results/three_lab_avg_1comp_example-0002_rawFitData.csv'
       three_labavg_1comp_av_path = path5 / 'all-results/three_lab_avg_1comp_300curves-avg_rawFitData.csv'

       # load data - clean
       odot069_clean_1comp_ex = pd.read_csv(odot069_clean_1comp_ex_path, sep=',', usecols=[0, 1, 2], na_values=' ', header=0, names=['lag time [ms]', 'correlation', 'fit']).T.assign(sim=3*[0.069,], processing=3*['Ex: No artifacts,\nno correction',])
       odot069_clean_1comp_av = pd.read_csv(odot069_clean_1comp_av_path, sep=',', usecols=[0, 1, 2], na_values=' ', header=0, names=['lag time [ms]', 'correlation', 'fit']).T.assign(sim=3*[0.069,], processing=3*['Avg: No artifacts,\nno correction',])
       odot2_clean_1comp_ex = pd.read_csv(odot2_clean_1comp_ex_path, sep=',', usecols=[0, 1, 2], na_values=' ', header=0, names=['lag time [ms]', 'correlation', 'fit']).T.assign(sim=3*[0.2,], processing=3*['Ex: No artifacts,\nno correction',])
       odot2_clean_1comp_av = pd.read_csv(odot2_clean_1comp_av_path, sep=',', usecols=[0, 1, 2], na_values=' ', header=0, names=['lag time [ms]', 'correlation', 'fit']).T.assign(sim=3*[0.2,], processing=3*['Avg: No artifacts,\nno correction',])
       three_clean_1comp_ex = pd.read_csv(three_clean_1comp_ex_path, sep=',', usecols=[0, 1, 2], na_values=' ', header=0, names=['lag time [ms]', 'correlation', 'fit']).T.assign(sim=3*[3.0,], processing=3*['Ex: No artifacts,\nno correction',])
       three_clean_1comp_av = pd.read_csv(three_clean_1comp_av_path, sep=',', usecols=[0, 1, 2], na_values=' ', header=0, names=['lag time [ms]', 'correlation', 'fit']).T.assign(sim=3*[3.0,], processing=3*['Avg: No artifacts,\nno correction',])

       # load data - dirty
       odot069_0dot01_dirty_1comp_ex = pd.read_csv(odot069_0dot01_dirty_1comp_ex_path, sep=',', usecols=[0, 1, 2], na_values=' ', header=0, names=['lag time [ms]', 'correlation', 'fit']).T.assign(sim=3*[0.069,], processing=3*['Ex1: Peak artifacts,\nno correction',])
       odot069_0dot1_dirty_1comp_ex = pd.read_csv(odot069_0dot1_dirty_1comp_ex_path, sep=',', usecols=[0, 1, 2], na_values=' ', header=0, names=['lag time [ms]', 'correlation', 'fit']).T.assign(sim=3*[0.069,], processing=3*['Ex2: Peak artifacts,\nno correction',])
       odot069_1dot0_dirty_1comp_ex = pd.read_csv(odot069_1dot0_dirty_1comp_ex_path, sep=',', usecols=[0, 1, 2], na_values=' ', header=0, names=['lag time [ms]', 'correlation', 'fit']).T.assign(sim=3*[0.069,], processing=3*['Ex3: Peak artifacts,\nno correction',])
       odot069_0dot01_dirty_1comp_av = pd.read_csv(odot069_0dot01_dirty_1comp_av_path, sep=',', usecols=[0, 1, 2], na_values=' ', header=0, names=['lag time [ms]', 'correlation', 'fit']).T.assign(sim=3*[0.069,], processing=3*['Avg1: Peak artifacts,\nno correction',])
       odot069_0dot1_dirty_1comp_av = pd.read_csv(odot069_0dot1_dirty_1comp_av_path, sep=',', usecols=[0, 1, 2], na_values=' ', header=0, names=['lag time [ms]', 'correlation', 'fit']).T.assign(sim=3*[0.069,], processing=3*['Avg2: Peak artifacts,\nno correction',])
       odot069_1dot0_dirty_1comp_av = pd.read_csv(odot069_1dot0_dirty_1comp_av_path, sep=',', usecols=[0, 1, 2], na_values=' ', header=0, names=['lag time [ms]', 'correlation', 'fit']).T.assign(sim=3*[0.069,], processing=3*['Avg3: Peak artifacts,\nno correction',])

       odot2_0dot01_dirty_1comp_ex = pd.read_csv(odot2_0dot01_dirty_1comp_ex_path, sep=',', usecols=[0, 1, 2], na_values=' ', header=0, names=['lag time [ms]', 'correlation', 'fit']).T.assign(sim=3*[0.2,], processing=3*['Ex1: Peak artifacts,\nno correction',])
       odot2_0dot1_dirty_1comp_ex = pd.read_csv(odot2_0dot1_dirty_1comp_ex_path, sep=',', usecols=[0, 1, 2], na_values=' ', header=0, names=['lag time [ms]', 'correlation', 'fit']).T.assign(sim=3*[0.2,], processing=3*['Ex2: Peak artifacts,\nno correction',])
       odot2_1dot0_dirty_1comp_ex = pd.read_csv(odot2_1dot0_dirty_1comp_ex_path, sep=',', usecols=[0, 1, 2], na_values=' ', header=0, names=['lag time [ms]', 'correlation', 'fit']).T.assign(sim=3*[0.2,], processing=3*['Ex3: Peak artifacts,\nno correction',])
       odot2_0dot01_dirty_1comp_av = pd.read_csv(odot2_0dot01_dirty_1comp_av_path, sep=',', usecols=[0, 1, 2], na_values=' ', header=0, names=['lag time [ms]', 'correlation', 'fit']).T.assign(sim=3*[0.2,], processing=3*['Avg1: Peak artifacts,\nno correction',])
       odot2_0dot1_dirty_1comp_av = pd.read_csv(odot2_0dot1_dirty_1comp_av_path, sep=',', usecols=[0, 1, 2], na_values=' ', header=0, names=['lag time [ms]', 'correlation', 'fit']).T.assign(sim=3*[0.2,], processing=3*['Avg2: Peak artifacts,\nno correction',])
       odot2_1dot0_dirty_1comp_av = pd.read_csv(odot2_1dot0_dirty_1comp_av_path, sep=',', usecols=[0, 1, 2], na_values=' ', header=0, names=['lag time [ms]', 'correlation', 'fit']).T.assign(sim=3*[0.2,], processing=3*['Avg3: Peak artifacts,\nno correction',])

       three_0dot01_dirty_1comp_ex = pd.read_csv(three_0dot01_dirty_1comp_ex_path, sep=',', usecols=[0, 1, 2], na_values=' ', header=0, names=['lag time [ms]', 'correlation', 'fit']).T.assign(sim=3*[3.0,], processing=3*['Ex1: Peak artifacts,\nno correction',])
       three_0dot1_dirty_1comp_ex = pd.read_csv(three_0dot1_dirty_1comp_ex_path, sep=',', usecols=[0, 1, 2], na_values=' ', header=0, names=['lag time [ms]', 'correlation', 'fit']).T.assign(sim=3*[3.0,], processing=3*['Ex2: Peak artifacts,\nno correction',])
       three_1dot0_dirty_1comp_ex = pd.read_csv(three_1dot0_dirty_1comp_ex_path, sep=',', usecols=[0, 1, 2], na_values=' ', header=0, names=['lag time [ms]', 'correlation', 'fit']).T.assign(sim=3*[3.0,], processing=3*['Ex3: Peak artifacts,\nno correction',])
       three_0dot01_dirty_1comp_av = pd.read_csv(three_0dot01_dirty_1comp_av_path, sep=',', usecols=[0, 1, 2], na_values=' ', header=0, names=['lag time [ms]', 'correlation', 'fit']).T.assign(sim=3*[3.0,], processing=3*['Avg1: Peak artifacts,\nno correction',])
       three_0dot1_dirty_1comp_av = pd.read_csv(three_0dot1_dirty_1comp_av_path, sep=',', usecols=[0, 1, 2], na_values=' ', header=0, names=['lag time [ms]', 'correlation', 'fit']).T.assign(sim=3*[3.0,], processing=3*['Avg2: Peak artifacts,\nno correction',])
       three_1dot0_dirty_1comp_av = pd.read_csv(three_1dot0_dirty_1comp_av_path, sep=',', usecols=[0, 1, 2], na_values=' ', header=0, names=['lag time [ms]', 'correlation', 'fit']).T.assign(sim=3*[3.0,], processing=3*['Avg3: Peak artifacts,\nno correction',])

       # load correction methods
       odot069_labcas_1comp_ex = pd.read_csv(odot069_labcas_1comp_ex_path, sep=',', usecols=[0, 1, 2], na_values=' ', header=0, names=['lag time [ms]', 'correlation', 'fit']).T.assign(sim=3*[0.069,], processing=3*['Ex: Peak artifacts,\nsim.-based pred.\n+ cut and shift corr.',])
       odot069_labcas_1comp_av = pd.read_csv(odot069_labcas_1comp_av_path, sep=',', usecols=[0, 1, 2], na_values=' ', header=0, names=['lag time [ms]', 'correlation', 'fit']).T.assign(sim=3*[0.069,], processing=3*['Avg: Peak artifacts,\nsim.-based pred.\n+ cut and shift corr.',])
       odot2_labcas_1comp_ex = pd.read_csv(odot2_labcas_1comp_ex_path, sep=',', usecols=[0, 1, 2], na_values=' ', header=0, names=['lag time [ms]', 'correlation', 'fit']).T.assign(sim=3*[0.2,], processing=3*['Ex: Peak artifacts,\nsim.-based pred.\n+ cut and shift corr.',])
       odot2_labcas_1comp_av = pd.read_csv(odot2_labcas_1comp_av_path, sep=',', usecols=[0, 1, 2], na_values=' ', header=0, names=['lag time [ms]', 'correlation', 'fit']).T.assign(sim=3*[0.2,], processing=3*['Avg: Peak artifacts,\nsim.-based pred.\n+ cut and shift corr.',])
       three_labcas_1comp_ex = pd.read_csv(three_labcas_1comp_ex_path, sep=',', usecols=[0, 1, 2], na_values=' ', header=0, names=['lag time [ms]', 'correlation', 'fit']).T.assign(sim=3*[3,], processing=3*['Ex: Peak artifacts,\nsim.-based pred.\n+ cut and shift corr.',])
       three_labcas_1comp_av = pd.read_csv(three_labcas_1comp_av_path, sep=',', usecols=[0, 1, 2], na_values=' ', header=0, names=['lag time [ms]', 'correlation', 'fit']).T.assign(sim=3*[3,], processing=3*['Avg: Peak artifacts,\nsim.-based pred.\n+ cut and shift corr.',])

       odot069_labdel_1comp_ex = pd.read_csv(odot069_labdel_1comp_ex_path, sep=',', usecols=[0, 1, 2], na_values=' ', header=0, names=['lag time [ms]', 'correlation', 'fit']).T.assign(sim=3*[0.069,], processing=3*['Ex: Peak artifacts,\nsim.-based prediction\n+ weight=0 correction',])
       odot069_labdel_1comp_av = pd.read_csv(odot069_labdel_1comp_av_path, sep=',', usecols=[0, 1, 2], na_values=' ', header=0, names=['lag time [ms]', 'correlation', 'fit']).T.assign(sim=3*[0.069,], processing=3*['Avg: Peak artifacts,\nsim.-based prediction\n+ weight=0 correction',])
       odot2_labdel_1comp_ex = pd.read_csv(odot2_labdel_1comp_ex_path, sep=',', usecols=[0, 1, 2], na_values=' ', header=0, names=['lag time [ms]', 'correlation', 'fit']).T.assign(sim=3*[0.2,], processing=3*['Ex: Peak artifacts,\nsim.-based prediction\n+ weight=0 correction',])
       odot2_labdel_1comp_av = pd.read_csv(odot2_labdel_1comp_av_path, sep=',', usecols=[0, 1, 2], na_values=' ', header=0, names=['lag time [ms]', 'correlation', 'fit']).T.assign(sim=3*[0.2,], processing=3*['Avg: Peak artifacts,\nsim.-based prediction\n+ weight=0 correction',])
       three_labdel_1comp_ex = pd.read_csv(three_labdel_1comp_ex_path, sep=',', usecols=[0, 1, 2], na_values=' ', header=0, names=['lag time [ms]', 'correlation', 'fit']).T.assign(sim=3*[3,], processing=3*['Ex: Peak artifacts,\nsim.-based prediction\n+ weight=0 correction',])
       three_labdel_1comp_av = pd.read_csv(three_labdel_1comp_av_path, sep=',', usecols=[0, 1, 2], na_values=' ', header=0, names=['lag time [ms]', 'correlation', 'fit']).T.assign(sim=3*[3,], processing=3*['Avg: Peak artifacts,\nsim.-based prediction\n+ weight=0 correction',])

       odot069_labavg_1comp_ex = pd.read_csv(odot069_labavg_1comp_ex_path, sep=',', usecols=[0, 1, 2], na_values=' ', header=0, names=['lag time [ms]', 'correlation', 'fit']).T.assign(sim=3*[0.069,], processing=3*['Ex: Peak artifacts,\nsim.-based pred.\n+ averaging corr.',])
       odot069_labavg_1comp_av = pd.read_csv(odot069_labavg_1comp_av_path, sep=',', usecols=[0, 1, 2], na_values=' ', header=0, names=['lag time [ms]', 'correlation', 'fit']).T.assign(sim=3*[0.069,], processing=3*['Avg: Peak artifacts,\nsim.-based pred.\n+ averaging corr.',])
       odot2_labavg_1comp_ex = pd.read_csv(odot2_labavg_1comp_ex_path, sep=',', usecols=[0, 1, 2], na_values=' ', header=0, names=['lag time [ms]', 'correlation', 'fit']).T.assign(sim=3*[0.2,], processing=3*['Ex: Peak artifacts,\nsim.-based pred.\n+ averaging corr.',])
       odot2_labavg_1comp_av = pd.read_csv(odot2_labavg_1comp_av_path, sep=',', usecols=[0, 1, 2], na_values=' ', header=0, names=['lag time [ms]', 'correlation', 'fit']).T.assign(sim=3*[0.2,], processing=3*['Avg: Peak artifacts,\nsim.-based pred.\n+ averaging corr.',])
       three_labavg_1comp_ex = pd.read_csv(three_labavg_1comp_ex_path, sep=',', usecols=[0, 1, 2], na_values=' ', header=0, names=['lag time [ms]', 'correlation', 'fit']).T.assign(sim=3*[3,], processing=3*['Ex: Peak artifacts,\nsim.-based pred.\n+ averaging corr.',])
       three_labavg_1comp_av = pd.read_csv(three_labavg_1comp_av_path, sep=',', usecols=[0, 1, 2], na_values=' ', header=0, names=['lag time [ms]', 'correlation', 'fit']).T.assign(sim=3*[3,], processing=3*['Avg: Peak artifacts,\nsim.-based pred.\n+ averaging corr.',])


       all_fit = pd.concat([odot069_clean_1comp_ex, odot069_clean_1comp_av, odot2_clean_1comp_ex,
                              odot2_clean_1comp_av, three_clean_1comp_ex, three_clean_1comp_av,
                              odot069_0dot01_dirty_1comp_ex, odot069_0dot01_dirty_1comp_av, odot2_0dot01_dirty_1comp_ex,
                              odot2_0dot01_dirty_1comp_av, three_0dot01_dirty_1comp_ex, three_0dot01_dirty_1comp_av,
                              odot069_0dot1_dirty_1comp_ex, odot069_0dot1_dirty_1comp_av, odot2_0dot1_dirty_1comp_ex,
                              odot2_0dot1_dirty_1comp_av, three_0dot1_dirty_1comp_ex, three_0dot1_dirty_1comp_av,
                              odot069_1dot0_dirty_1comp_ex, odot069_1dot0_dirty_1comp_av, odot2_1dot0_dirty_1comp_ex,
                              odot2_1dot0_dirty_1comp_av, three_1dot0_dirty_1comp_ex, three_1dot0_dirty_1comp_av,
                              odot069_labdel_1comp_ex, odot069_labdel_1comp_av, odot2_labdel_1comp_ex,
                              odot2_labdel_1comp_av, three_labdel_1comp_ex, three_labdel_1comp_av,
                              odot069_labavg_1comp_ex, odot069_labavg_1comp_av, odot2_labavg_1comp_ex,
                              odot2_labavg_1comp_av, three_labavg_1comp_ex, three_labavg_1comp_av,
                              odot069_labcas_1comp_ex, odot069_labcas_1comp_av, odot2_labcas_1comp_ex,
                              odot2_labcas_1comp_av, three_labcas_1comp_ex, three_labcas_1comp_av])
       first = set(all_fit.index)
       second = set(all_fit['sim'])
       third = set(all_fit['processing'])

       third_avg = set(['Avg1: Peak artifacts,\nno correction',
                        'Avg2: Peak artifacts,\nno correction',
                        'Avg3: Peak artifacts,\nno correction',
                        'Avg: No artifacts,\nno correction',
                        'Avg: Peak artifacts,\nsim.-based pred.\n+ averaging corr.',
                        'Avg: Peak artifacts,\nsim.-based prediction\n+ weight=0 correction',
                        'Avg: Peak artifacts,\nsim.-based pred.\n+ cut and shift corr.'])
       new_index = pd.MultiIndex.from_product([first, second, third], names=['axis', 'sim', 'processing'])
       all_fit = all_fit.reset_index()
       all_fit = all_fit.set_index(['index', 'sim', 'processing'])

       all_fit = all_fit.reindex(new_index)
       # pprint(all_param.keys())
       # all_param = all_param[['name_of_plot', 'Diff_species', 'A1', 'txy1', 'sim', 'processing', 'A2', 'txy2']]
       with pd.option_context("max_colwidth", 1000):
           display(all_fit)


     #+END_SRC

     #+RESULTS:
     :RESULTS:
     |               |       |                                                               | 0        | 1        | 2        | 3        | 4        | 5        | 6        | 7        | 8        | 9         | ... | 84          | 85          | 86          | 87          | 88          | 89           | 90           | 91           | 92           | 93           |
|---------------+-------+---------------------------------------------------------------+----------+----------+----------+----------+----------+----------+----------+----------+----------+-----------+-----+-------------+-------------+-------------+-------------+-------------+--------------+--------------+--------------+--------------+--------------|
| axis          | sim   | processing                                                    |          |          |          |          |          |          |          |          |          |           |     |             |             |             |             |             |              |              |              |              |              |
| correlation   | 0.069 | Ex: No artifacts,\nno correction                              | 0.067368 | 0.066953 | 0.066546 | 0.066142 | 0.065743 | 0.065349 | 0.064963 | 0.064587 | 0.064220 | 0.063843  | ... | 0.014743    | 0.017815    | 0.015664    | 0.001972    | -0.005822   | -0.005455    | -0.022004    | -0.030182    | -0.018259    | -0.010153    |
|               |       | Ex: Peak artifacts,\nsim.-based pred.\n+ averaging corr.      | 0.048870 | 0.047601 | 0.046439 | 0.045346 | 0.044218 | 0.043114 | 0.042083 | 0.041051 | 0.040049 | 0.039083  | ... | NaN         | NaN         | NaN         | NaN         | NaN         | NaN          | NaN          | NaN          | NaN          | NaN          |
|               |       | Ex: Peak artifacts,\nsim.-based pred.\n+ cut and shift corr.  | 0.062425 | 0.061877 | 0.061353 | 0.060847 | 0.060359 | 0.059861 | 0.059392 | 0.058948 | 0.058517 | 0.058111  | ... | -0.005308   | -0.012228   | -0.019728   | -0.004490   | NaN         | NaN          | NaN          | NaN          | NaN          | NaN          |
|               |       | Avg1: Peak artifacts,\nno correction                          | 0.227222 | 0.226815 | 0.226407 | 0.226002 | 0.225600 | 0.225202 | 0.224806 | 0.224413 | 0.224021 | 0.223632  | ... | -0.028989   | -0.034142   | -0.036995   | -0.037272   | -0.028963   | -0.021312    | -0.030986    | -0.034259    | -0.031788    | -0.031967    |
|               |       | Ex2: Peak artifacts,\nno correction                           | 0.690009 | 0.681992 | 0.674783 | 0.667555 | 0.661034 | 0.654863 | 0.648856 | 0.642526 | 0.635818 | 0.629217  | ... | -0.039290   | -0.071583   | -0.019689   | -0.047476   | -0.050787   | -0.072526    | 0.140779     | 0.086342     | -0.062967    | -0.025313    |
| ...           | ...   | ...                                                           | ...      | ...      | ...      | ...      | ...      | ...      | ...      | ...      | ...      | ...       | ... | ...         | ...         | ...         | ...         | ...         | ...          | ...          | ...          | ...          | ...          |
| lag time [ms] | 0.200 | Avg: Peak artifacts,\nsim.-based pred.\n+ averaging corr.     | 1.000000 | 2.000000 | 3.000000 | 4.000000 | 5.000000 | 6.000000 | 7.000000 | 8.000000 | 9.000000 | 10.000000 | ... | NaN         | NaN         | NaN         | NaN         | NaN         | NaN          | NaN          | NaN          | NaN          | NaN          |
|               |       | Avg3: Peak artifacts,\nno correction                          | 1.000000 | 2.000000 | 3.000000 | 4.000000 | 5.000000 | 6.000000 | 7.000000 | 8.000000 | 9.000000 | 10.000000 | ... | 6656.000000 | 7168.000000 | 7680.000000 | 8192.000000 | 9216.000000 | 10240.000000 | 11264.000000 | 12288.000000 | 13312.000000 | 14336.000000 |
|               |       | Ex1: Peak artifacts,\nno correction                           | 1.000000 | 2.000000 | 3.000000 | 4.000000 | 5.000000 | 6.000000 | 7.000000 | 8.000000 | 9.000000 | 10.000000 | ... | 6656.000000 | 7168.000000 | 7680.000000 | 8192.000000 | 9216.000000 | 10240.000000 | 11264.000000 | 12288.000000 | 13312.000000 | 14336.000000 |
|               |       | Avg: No artifacts,\nno correction                             | 1.000000 | 2.000000 | 3.000000 | 4.000000 | 5.000000 | 6.000000 | 7.000000 | 8.000000 | 9.000000 | 10.000000 | ... | 6656.000000 | 7168.000000 | 7680.000000 | 8192.000000 | 9216.000000 | 10240.000000 | 11264.000000 | 12288.000000 | 13312.000000 | 14336.000000 |
|               |       | Avg: Peak artifacts,\nsim.-based pred.\n+ cut and shift corr. | 1.000000 | 2.000000 | 3.000000 | 4.000000 | 5.000000 | 6.000000 | 7.000000 | 8.000000 | 9.000000 | 10.000000 | ... | 6656.000000 | 7168.000000 | 7680.000000 | 8192.000000 | NaN         | NaN          | NaN          | NaN          | NaN          | NaN          |

126 rows × 94 columns
     :END:
   - I am scaling the correlation and fit of the dirty data with the
     =sklearn.preprocessing.MaxAbsScaler= to make the final plot better
     readable. The relevant information is the difference in lag time. The
     particle number in the simulations was randomized, thus the amplitude of
     the correlation function does not yield reliable information anyway.
   - also, let's compute the residuals of correlations and fits - I use the
     original data for all, not the scaled data
   - here an examplary plot for 1 trace of the effect of scaling - notice the
     difference when scaling the correlation and scaling the fit. That's why I
     compute residuals before and also only scale the dirty data with 3 lines in
     one plot
     #+BEGIN_SRC jupyter-python
       fig = plt.figure(figsize=(16,5))
       count = 0
       for ind in all_fit.loc['fit'].index:
           if count < 2:
               count += 1
               continue
           # lags = all_fit.loc[:, (('lag time [ms]',) + ind)]
           lag_idx = (('lag time [ms]',) + ind)
           corr_idx = (('correlation',) + ind)
           fit_idx = (('fit',) + ind)
           res_idx = (('residual',) + ind)
           plt.subplot(141)
           plt.semilogx(all_fit.loc[lag_idx], all_fit.loc[corr_idx],
                         all_fit.loc[lag_idx], all_fit.loc[fit_idx])
           fit = np.array(all_fit.loc[fit_idx])
           corr = np.array(all_fit.loc[corr_idx])
           residual = corr - fit
           fit_scaled = MaxAbsScaler().fit_transform(fit.reshape(-1, 1)).flatten()
           corr_scaled = MaxAbsScaler().fit_transform(corr.reshape(-1, 1)).flatten()
           plt.subplot(142)
           plt.semilogx(all_fit.loc[lag_idx], corr_scaled,
                        all_fit.loc[lag_idx], fit_scaled)
           all_fit.loc[fit_idx] = pd.Series(fit_scaled)
           all_fit.loc[corr_idx] = pd.Series(corr_scaled)
           plt.subplot(143)
           plt.semilogx(all_fit.loc[lag_idx],
                         all_fit.loc[corr_idx],
                         all_fit.loc[lag_idx],
                         all_fit.loc[fit_idx])

           all_fit.loc[res_idx] = pd.Series(residual)
           plt.subplot(144)
           plt.semilogx(all_fit.loc[lag_idx], all_fit.loc[res_idx])
           plt.show()
           break
     #+END_SRC

     #+RESULTS:
     [[file:./data/exp-220316-publication1/jupyter/4900a04aa27f09df7056038f5748703524df6c23.png]]

   - now the computation of residuals and scaling
     #+BEGIN_SRC jupyter-python
       for ind in all_fit.loc['fit'].index:
           lag_idx = (('lag time [ms]',) + ind)
           corr_idx = (('correlation',) + ind)
           fit_idx = (('fit',) + ind)
           res_idx = (('residual',) + ind)
           fit = np.array(all_fit.loc[fit_idx])
           corr = np.array(all_fit.loc[corr_idx])
           residual = corr - fit
           if f'{ind[1]}' in list(third_avg) + ['Ex1: Peak artifacts,\nno correction',
                              'Ex2: Peak artifacts,\nno correction',
                              'Ex3: Peak artifacts,\nno correction']:
               fit_scaled = MaxAbsScaler().fit_transform(fit.reshape(-1, 1)).flatten()
               corr_scaled = MaxAbsScaler().fit_transform(corr.reshape(-1, 1)).flatten()
               all_fit.loc[fit_idx] = pd.Series(fit_scaled)
               all_fit.loc[corr_idx] = pd.Series(corr_scaled)
           all_fit.loc[res_idx] = pd.Series(residual)

     #+END_SRC

     #+RESULTS:

   - let's first plot the residuals
     #+begin_src jupyter-python
       fig, ax = plt.subplots(len(third-third_avg), len(second),
                              figsize=(16, 15), sharex=True, sharey=False,
                              tight_layout=True)


       for i, sim in enumerate(second):
           for j, proc in enumerate(third-third_avg):
               if proc in ['Ex1: Peak artifacts,\nno correction',
                          'Ex2: Peak artifacts,\nno correction',
                          'Ex3: Peak artifacts,\nno correction']:
                   for k in range(3):
                       sns.lineplot(x=all_fit.loc['lag time [ms]'].loc[sim, f'Ex{k+1}: Peak artifacts,\nno correction'],
                                    y=all_fit.loc['residual'].loc[sim, f'Ex{k+1}: Peak artifacts,\nno correction'],
                                    color=sns.color_palette()[k+3],
                                    # marker=['o', 'v', 's'][k], markersize=5,
                                    ax=ax[j, i], lw=5).set(title=f'{sim}-{proc}')
               else:
                   lag_idx = (('lag time [ms]',) + (sim,) + (proc,))
                   res_idx = (('residual',) + (sim,) + (proc,))
                   sns.lineplot(x=all_fit.loc[lag_idx], y=all_fit.loc[res_idx],
                                color=sns.color_palette()[3],
                                # marker='o', markersize=10,
                                ax=ax[j, i], lw=5).set(
                                    title=f'{sim}-{proc}')


       plt.setp(ax, xscale='log', xlabel=r'lag time $\tau$ $[ms]$',
                ylabel=r'')  # Correlation $G(\tau)$ $[ms]$

       savefig = f'./data/exp-220316-publication1/jupyter/plotsupp1_sim-correlations-fits_residuals_withylabels'
       plt.savefig(f'{savefig}.pdf', dpi=300)
       # os.system(f'pdf2svg {savefig}.pdf {savefig}.svg')
       plt.close('all')
     #+end_src

     #+RESULTS:

   - now plot the single correlations and plots
     #+BEGIN_SRC jupyter-python
       fig, ax = plt.subplots(len(third-third_avg), len(second),
                              figsize=(16, 30), sharex=True, sharey=False,
                              tight_layout=True)


       for i, sim in enumerate(second):
           for j, proc in enumerate(third-third_avg):
               if proc in ['Ex1: Peak artifacts,\nno correction',
                          'Ex2: Peak artifacts,\nno correction',
                          'Ex3: Peak artifacts,\nno correction']:
                   for k in range(3):
                       sns.lineplot(x=all_fit.loc['lag time [ms]'].loc[sim, f'Ex{k+1}: Peak artifacts,\nno correction'],
                                    y=all_fit.loc['correlation'].loc[sim, f'Ex{k+1}: Peak artifacts,\nno correction'],
                                    color=sns.color_palette()[k],
                                    lw=4, marker=['o', 'v', 's'][k], markersize=10,
                                    ax=ax[j, i]).set(title=f'{sim}-{proc}')
                       sns.lineplot(x=all_fit.loc['lag time [ms]'].loc[(sim, f'Ex{k+1}: Peak artifacts,\nno correction')],
                                    y=all_fit.loc['fit'].loc[(sim, f'Ex{k+1}: Peak artifacts,\nno correction')],
                                    color=sns.color_palette()[k+3],
                                    lw=5,
                                    ax=ax[j, i])
               else:
                   lag_idx = (('lag time [ms]',) + (sim,) + (proc,))
                   corr_idx = (('correlation',) + (sim,) + (proc,))
                   fit_idx = (('fit',) + (sim,) + (proc,))
                   sns.lineplot(x=all_fit.loc[lag_idx], y=all_fit.loc[corr_idx],
                                color=sns.color_palette()[0],
                                lw=4, marker='o', markersize=10,
                                ax=ax[j, i]).set(
                                    title=f'{sim}-{proc}')
                   sns.lineplot(x=all_fit.loc[lag_idx], y=all_fit.loc[fit_idx],
                                color=sns.color_palette()[3],
                                lw=5,
                                ax=ax[j, i])


       plt.setp(ax, xscale='log', xlabel=r'lag time $\tau$ $[ms]$',
                ylabel=r'', yticklabels=[])  # Correlation $G(\tau)$ $[ms]$

       savefig = f'./data/exp-220316-publication1/jupyter/plotsupp1_sim-correlations-fits_plots'
       plt.savefig(f'{savefig}.pdf', dpi=300)
       os.system(f'pdf2svg {savefig}.pdf {savefig}.svg')
       plt.close('all')
     #+END_SRC

     #+RESULTS:

   - as an inset, I want the average correlations over 100 traces (for dirty) or 300
     traces (for clean, and corrections).

     #+BEGIN_SRC jupyter-python
       sns.set_theme(style="darkgrid", font_scale=2, palette='colorblind',
                     context='paper')

       fig, ax = plt.subplots(len(third_avg), len(second),
                              figsize=(8, 20), sharex=True, sharey=False,
                              tight_layout=True)


       for i, sim in enumerate(second):
           for j, proc in enumerate(third_avg):
               if proc in ['Avg1: Peak artifacts,\nno correction',
                          'Avg2: Peak artifacts,\nno correction',
                          'Avg3: Peak artifacts,\nno correction']:
                   for k in range(3):
                       sns.lineplot(x=all_fit.loc['lag time [ms]'].loc[sim, f'Avg{k+1}: Peak artifacts,\nno correction'],
                                    y=all_fit.loc['correlation'].loc[sim, f'Avg{k+1}: Peak artifacts,\nno correction'],
                                    color=sns.color_palette()[k], lw=4,
                                    ax=ax[j, i]).set(title=f'{sim}-{proc}')

               else:
                   lag_idx = (('lag time [ms]',) + (sim,) + (proc,))
                   corr_idx = (('correlation',) + (sim,) + (proc,))
                   sns.lineplot(x=all_fit.loc[lag_idx], y=all_fit.loc[corr_idx],
                                color=sns.color_palette()[0], lw=4,
                                ax=ax[j, i]).set(
                                    title=f'{sim}-{proc}')

       plt.setp(ax, xscale='log', xlabel=r'',
                ylabel=r'') #, yticklabels=[])  # Correlation $G(\tau)$ $[ms]$

       savefig = f'./data/exp-220316-publication1/jupyter/plotsupp1_sim-correlations-fits_inset'
       plt.savefig(f'{savefig}.pdf', dpi=300)
       os.system(f'pdf2svg {savefig}.pdf {savefig}.svg')
       plt.close('all')
     #+END_SRC

     #+RESULTS:

     #+BEGIN_SRC jupyter-python
       for i in [0.01, 0.1, 1]:
          print(ans.convert_diffcoeff_to_transittimes(i, 250))
     #+END_SRC

     #+RESULTS:
     : (1127.1055006945026, [558.1720842044252, 2275.941140099269])
     : (112.71055006945028, [70.2697020959065, 180.78443081514223])
     : (11.271055006945026, [8.846431364774256, 14.360217779502584])


**** comparison prediction methods
   :PROPERTIES:
   :header-args:jupyter-python: :session /jpy:localhost#8888:a37e524a-8134-4d8f-b24a-367acaf1bdd3 :pandoc t
   :END:

   - call =jupyter-set-output-directory= and =simulations-prepare-modules=
     #+CALL: jupyter-set-output-directory()

     #+RESULTS:
     : ./data/exp-220316-publication1/jupyter

     #+CALL: simulations-prepare-modules()

     #+RESULTS:
     : /home/lex/Programme/drmed-git
     : 2023-01-25 10:58:57.948217: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
     : 2023-01-25 10:58:57.948249: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.

   - let's *compare correction methods* on 3 different simulated base molecule
     speeds (0.069, 0.2, and 3.0 um^2/s; varying molecule numbers) and af488 vs
     af488+LUV data and hs-pex5 vs tb-pex5 data
     #+BEGIN_SRC jupyter-python
       # dirty correlations - check out from  branch exp-220227-unet
       path1 = Path('/home/lex/Programme/drmed-git/data/exp-220227-unet/2022-04-25_simulations/dirty')
       odot069_dirty_1comp_path = path1 / '0.069_results/dirty_0dot069-all_1comp_outputParam.csv'
       odot069_dirty_2comp_path = path1 / '0.069_results/dirty_0dot069-all_2comp_outputParam.csv'
       odot2_dirty_1comp_path = path1 / '0.2_results/dirty_0dot2-all_1comp_outputParam.csv'
       odot2_dirty_2comp_path = path1 / '0.2_results/dirty_0dot2-all_2comp_outputParam.csv'
       three_dirty_1comp_path = path1 / '3.0_results/dirty_3dot0-all_1comp_outputParam.csv'
       three_dirty_2comp_path = path1 / '3.0_results/dirty_3dot0-all_2comp_outputParam.csv'

       # clean correlations
       path2 = Path('/home/lex/Programme/drmed-git/data/exp-220316-publication1/220719_threshold-prediction/threshold-all-results')
       odot069_clean_1comp_path = path2 / '0dot069_clean_1comp_outputParam.csv'
       odot2_clean_1comp_path = path2 / '0dot2_clean_1comp_outputParam.csv'
       three_clean_1comp_path = path2 / '3dot0_clean_1comp_outputParam.csv'

       # control with prediction threshold
       odot069_2cas_1comp_path = path2 / '0dot069_robust_thresh2_1comp_outputParam.csv'
       odot069_2cas_2comp_path = path2 / '0dot069_robust_thresh2_2comp_outputParam.csv'
       odot2_2cas_1comp_path = path2 / '0dot2_robust_thresh2_1comp_outputParam.csv'
       odot2_2cas_2comp_path = path2 / '0dot2_robust_thresh2_2comp_outputParam.csv'
       three_2cas_1comp_path = path2 / '3dot0_robust_thresh2_1comp_outputParam.csv'
       three_2cas_2comp_path = path2 / '3dot0_robust_thresh2_2comp_outputParam.csv'

       # prediction by best unet 0cd20 - check out from branch exp-220227-unet
       path3 = Path('/home/lex/Programme/drmed-git/data/exp-220227-unet/2022-04-25_simulations/0cd20')
       odot069_0cd20cas_1comp_path = path3 / '0.069_results/0cd20_0dot069-all_1comp_outputParam.csv'
       odot069_0cd20cas_2comp_path = path3 / '0.069_results/0cd20_0dot069-all_2comp_outputParam.csv'
       odot2_0cd20cas_1comp_path = path3 / '0.2_results/0cd20_0dot2-all_1comp_outputParam.csv'
       odot2_0cd20cas_2comp_path = path3 / '0.2_results/0cd20_0dot2-all_2comp_outputParam.csv'
       three_0cd20cas_1comp_path = path3 / '3.0_results/0cd20_3dot0-all_1comp_outputParam.csv'
       three_0cd20cas_2comp_path = path3 / '3.0_results/0cd20_3dot0-all_2comp_outputParam.csv'

       # biological data - af488
       path6 = Path('/home/lex/Programme/drmed-git/data/exp-220227-unet/2022-05-22_experimental-af488')
       path7 = Path('/home/lex/Programme/drmed-git/data/exp-220316-publication1/220726_bioexps/af488')
       path8 = Path('/home/lex/Programme/drmed-git/data/exp-220316-publication1/220726_bioexps/af488+luvs/')
       af488_noc_1comp_path = path6 / 'clean-all-results/clean_no-correction_1comp_outputParam.csv'
       af488_0cd20cas_1comp_path = path6 / 'clean-all-results/clean_0cd20_1comp_outputParam.csv'
       af488_ff67bcas_1comp_path = path6 / 'clean-all-results/clean_ff67b_1comp_outputParam.csv'

       af488_1dot5cas_1comp_path = path7 / 'af488-all-results/af488_thresh-1.5_1comp_outputParam.csv'
       af488_2cas_1comp_path = path7 / 'af488-all-results/af488_thresh-2_1comp_outputParam.csv'
       af488_2dot5cas_1comp_path = path7 / 'af488-all-results/af488_thresh-2.5_1comp_outputParam.csv'

       af488luv_noc_1comp_path = path6 / 'dirty-all-results/dirty_no-correction_1comp_outputParam.csv'
       af488luv_noc_2comp_path = path6 / 'dirty-all-results/dirty_no-correction_2comp_outputParam.csv'
       af488luv_0cd20cas_1comp_path = path6 / 'dirty-all-results/dirty_0cd20_1comp_outputParam.csv'
       af488luv_0cd20cas_2comp_path = path6 / 'dirty-all-results/dirty_0cd20_2comp_outputParam.csv'
       af488luv_ff67bcas_1comp_path = path6 / 'dirty-all-results/dirty_ff67b_1comp_outputParam.csv'
       af488luv_ff67bcas_2comp_path = path6 / 'dirty-all-results/dirty_ff67b_2comp_outputParam.csv'

       af488luv_1dot5cas_1comp_path = path8 / 'af488+luvs-all-results/af488+luvs_thresh-1.5_1comp_outputParam.csv'
       af488luv_1dot5cas_2comp_path = path8 / 'af488+luvs-all-results/af488+luvs_thresh-1.5_2comp_outputParam.csv'
       af488luv_2cas_1comp_path = path8 / 'af488+luvs-all-results/af488+luvs_thresh-2_1comp_outputParam.csv'
       af488luv_2cas_2comp_path = path8 / 'af488+luvs-all-results/af488+luvs_thresh-2_2comp_outputParam.csv'
       af488luv_2dot5cas_1comp_path = path8 / 'af488+luvs-all-results/af488+luvs_thresh-2.5_1comp_outputParam.csv'
       af488luv_2dot5cas_2comp_path = path8 / 'af488+luvs-all-results/af488+luvs_thresh-2.5_2comp_outputParam.csv'

       # Hs-PEX5-eGFP (clean), only 1comp fits, with triplets fixed to 0.04ms (triplet fraction floating)
       # see https://www.sciencedirect.com/science/article/pii/S266707472200012X#bib35 for 0.04 reference
       path9 = Path('/home/lex/Programme/drmed-git/data/exp-220227-unet/2022-06-02_experimental-pex5')
       path10 = Path('/home/lex/Programme/drmed-git/data/exp-220316-publication1/220726_bioexps/Hs-PEX5-eGFP')
       path11 = Path('/home/lex/Programme/drmed-git/data/exp-220316-publication1/220726_bioexps/Tb-PEX5-eGFP/')
       hspex5_noc_1comp_path = path9 / 'clean-all-results/Hs-PEX5-eGFP_no-correction_triplet-0.04_1comp_outputParam.csv'
       hspex5_0cd20cas_1comp_path = path9 / 'clean-all-results/Hs-PEX5-eGFP_0cd20_triplet-0.04_1comp_outputParam.csv'
       hspex5_ff67bcas_1comp_path = path9 / 'clean-all-results/Hs-PEX5-eGFP_ff67b_triplet-0.04_1comp_outputParam.csv'

       hspex5_5cas_1comp_path = path10 / 'Hs-PEX5-eGFP-all-results/Hs-PEX5-eGFP_thresh-5_triplet-0.04_1comp_outputParam.csv'
       hspex5_7cas_1comp_path = path10 / 'Hs-PEX5-eGFP-all-results/Hs-PEX5-eGFP_thresh-7_triplet-0.04_1comp_outputParam.csv'
       hspex5_10cas_1comp_path = path10 / 'Hs-PEX5-eGFP-all-results/Hs-PEX5-eGFP_thresh-10_triplet-0.04_1comp_outputParam.csv'

       tbpex5_noc_1comp_path = path9 / 'dirty-all-results/Tb-PEX5-eGFP_no-correction_triplet-0.04_1comp_outputParam.csv'
       tbpex5_noc_2comp_path = path9 / 'dirty-all-results/Tb-PEX5-eGFP_no-correction_triplet-0.04_2comp_outputParam.csv'
       tbpex5_0cd20cas_1comp_path = path9 / 'dirty-all-results/Tb-PEX5-eGFP_0cd20_triplet-0.04_1comp_outputParam.csv'
       tbpex5_0cd20cas_2comp_path = path9 / 'dirty-all-results/Tb-PEX5-eGFP_0cd20_triplet-0.04_2comp_outputParam.csv'
       tbpex5_ff67bcas_1comp_path = path9 / 'dirty-all-results/Tb-PEX5-eGFP_ff67b_triplet-0.04_1comp_outputParam.csv'
       tbpex5_ff67bcas_2comp_path = path9 / 'dirty-all-results/Tb-PEX5-eGFP_ff67b_triplet-0.04_2comp_outputParam.csv'

       tbpex5_5cas_1comp_path = path11 / 'Tb-PEX5-eGFP-all-results/Tb-PEX5-eGFP_thresh-5_triplet-0.04_1comp_outputParam.csv'
       tbpex5_5cas_2comp_path = path11 / 'Tb-PEX5-eGFP-all-results/Tb-PEX5-eGFP_thresh-5_triplet-0.04_2comp_outputParam.csv'
       tbpex5_7cas_1comp_path = path11 / 'Tb-PEX5-eGFP-all-results/Tb-PEX5-eGFP_thresh-7_triplet-0.04_1comp_outputParam.csv'
       tbpex5_7cas_2comp_path = path11 / 'Tb-PEX5-eGFP-all-results/Tb-PEX5-eGFP_thresh-7_triplet-0.04_2comp_outputParam.csv'
       tbpex5_10cas_1comp_path = path11 / 'Tb-PEX5-eGFP-all-results/Tb-PEX5-eGFP_thresh-10_triplet-0.04_1comp_outputParam.csv'
       tbpex5_10cas_2comp_path = path11 / 'Tb-PEX5-eGFP-all-results/Tb-PEX5-eGFP_thresh-10_triplet-0.04_2comp_outputParam.csv'

       # hspex5_0cd20avg_1comp_path = path / 'hspex5_0cd20_averaging_1comp_outputParam.csv'
       # hspex5_0cd20del_1comp_path = path / 'hspex5_0cd20_delete_1comp_outputParam.csv'

       # tbpex5_0cd20avg_1comp_path = path / 'tbpex5_0cd20_averaging_1comp_outputParam.csv'
       # tbpex5_0cd20avg_2comp_path = path / 'tbpex5_0cd20_averaging_2comp_outputParam.csv'
       # tbpex5_0cd20del_1comp_path = path / 'tbpex5_0cd20_delete_1comp_outputParam.csv'
       # tbpex5_0cd20del_2comp_path = path / 'tbpex5_0cd20_delete_2comp_outputParam.csv'

       # load data
       odot069_clean_1comp = pd.read_csv(odot069_clean_1comp_path, sep=',').assign(sim=300*[0.069,], processing=300*['clean noc',])
       odot2_clean_1comp = pd.read_csv(odot2_clean_1comp_path, sep=',').assign(sim=300*[0.2,], processing=300*['clean noc',])
       three_clean_1comp = pd.read_csv(three_clean_1comp_path, sep=',').assign(sim=300*[3,], processing=300*['clean noc',])

       af488_1comp = pd.read_csv(af488_noc_1comp_path, sep=',').assign(sim=424*[280,], processing=424*['clean noc',])
       af488_0cd20cas_1comp = pd.read_csv(af488_0cd20cas_1comp_path, sep=',').assign(sim=424*[280,], processing=424*['clean 0cd20',])
       af488_ff67bcas_1comp = pd.read_csv(af488_ff67bcas_1comp_path, sep=',').assign(sim=424*[280,], processing=424*['clean ff67b',])
       af488_1dot5cas_1comp = pd.read_csv(af488_1dot5cas_1comp_path, sep=',').assign(sim=422*[280,], processing=422*['clean thr1',])
       af488_2cas_1comp = pd.read_csv(af488_2cas_1comp_path, sep=',').assign(sim=424*[280,], processing=424*['clean thr2',])
       af488_2dot5cas_1comp = pd.read_csv(af488_2dot5cas_1comp_path, sep=',').assign(sim=424*[280,], processing=424*['clean thr3',])

       hspex5_1comp = pd.read_csv(hspex5_noc_1comp_path, sep=',').assign(sim=250*[31,], processing=250*['clean noc',])
       hspex5_0cd20cas_1comp = pd.read_csv(hspex5_0cd20cas_1comp_path, sep=',').assign(sim=250*[31,], processing=250*['clean 0cd20',])
       hspex5_ff67bcas_1comp = pd.read_csv(hspex5_ff67bcas_1comp_path, sep=',').assign(sim=250*[31,], processing=250*['clean ff67b',])
       hspex5_5cas_1comp = pd.read_csv(hspex5_5cas_1comp_path, sep=',').assign(sim=250*[31,], processing=250*['clean thr1',])
       hspex5_7cas_1comp = pd.read_csv(hspex5_7cas_1comp_path, sep=',').assign(sim=250*[31,], processing=250*['clean thr2',])
       hspex5_10cas_1comp = pd.read_csv(hspex5_10cas_1comp_path, sep=',').assign(sim=250*[31,], processing=250*['clean thr3',])

       odot069_dirty_1comp = pd.read_csv(odot069_dirty_1comp_path, sep=',').assign(sim=300*[0.069,], processing=300*['dirty noc',])
       odot069_dirty_2comp = pd.read_csv(odot069_dirty_2comp_path, sep=',').assign(sim=300*[0.069,], processing=300*['dirty noc',])
       odot2_dirty_1comp = pd.read_csv(odot2_dirty_1comp_path, sep=',').assign(sim=300*[0.2,], processing=300*['dirty noc',])
       odot2_dirty_2comp = pd.read_csv(odot2_dirty_2comp_path, sep=',').assign(sim=300*[0.2,], processing=300*['dirty noc',])
       three_dirty_1comp = pd.read_csv(three_dirty_1comp_path, sep=',').assign(sim=300*[3,], processing=300*['dirty noc',])
       three_dirty_2comp = pd.read_csv(three_dirty_2comp_path, sep=',').assign(sim=300*[3,], processing=300*['dirty noc',])
       af488luv_1comp = pd.read_csv(af488luv_noc_1comp_path, sep=',').assign(sim=440*[280,], processing=440*['dirty noc',])
       af488luv_2comp = pd.read_csv(af488luv_noc_2comp_path, sep=',').assign(sim=440*[280,], processing=440*['dirty noc',])
       tbpex5_1comp = pd.read_csv(tbpex5_noc_1comp_path, sep=',').assign(sim=250*[31,], processing=250*['dirty noc',])
       tbpex5_2comp = pd.read_csv(tbpex5_noc_2comp_path, sep=',').assign(sim=250*[31,], processing=250*['dirty noc',])

       odot069_dirty_0cd20cas_1comp = pd.read_csv(odot069_0cd20cas_1comp_path, sep=',').assign(sim=300*[0.069,], processing=300*['dirty 0cd20',])
       odot069_dirty_0cd20cas_2comp = pd.read_csv(odot069_0cd20cas_2comp_path, sep=',').assign(sim=300*[0.069,], processing=300*['dirty 0cd20',])
       odot2_dirty_0cd20cas_1comp = pd.read_csv(odot2_0cd20cas_1comp_path, sep=',').assign(sim=300*[0.2,], processing=300*['dirty 0cd20',])
       odot2_dirty_0cd20cas_2comp = pd.read_csv(odot2_0cd20cas_2comp_path, sep=',').assign(sim=300*[0.2,], processing=300*['dirty 0cd20',])
       three_dirty_0cd20cas_1comp = pd.read_csv(three_0cd20cas_1comp_path, sep=',').assign(sim=300*[3,], processing=300*['dirty 0cd20',])
       three_dirty_0cd20cas_2comp = pd.read_csv(three_0cd20cas_2comp_path, sep=',').assign(sim=300*[3,], processing=300*['dirty 0cd20',])
       odot069_dirty_2cas_1comp = pd.read_csv(odot069_2cas_1comp_path, sep=',').assign(sim=300*[0.069,], processing=300*['dirty thr1',])
       odot069_dirty_2cas_2comp = pd.read_csv(odot069_2cas_2comp_path, sep=',').assign(sim=300*[0.069,], processing=300*['dirty thr1',])
       odot2_dirty_2cas_1comp = pd.read_csv(odot2_2cas_1comp_path, sep=',').assign(sim=300*[0.2,], processing=300*['dirty thr1',])
       odot2_dirty_2cas_2comp = pd.read_csv(odot2_2cas_2comp_path, sep=',').assign(sim=300*[0.2,], processing=300*['dirty thr1',])
       three_dirty_2cas_1comp = pd.read_csv(three_2cas_1comp_path, sep=',').assign(sim=300*[3,], processing=300*['dirty thr1',])
       three_dirty_2cas_2comp = pd.read_csv(three_2cas_2comp_path, sep=',').assign(sim=300*[3,], processing=300*['dirty thr1',])

       # load correction by label information as baseline
       af488luv_0cd20cas_1comp = pd.read_csv(af488luv_0cd20cas_1comp_path, sep=',').assign(sim=440*[280,], processing=440*['dirty 0cd20'])
       af488luv_0cd20cas_2comp = pd.read_csv(af488luv_0cd20cas_2comp_path, sep=',').assign(sim=440*[280,], processing=440*['dirty 0cd20'])
       af488luv_ff67bcas_1comp = pd.read_csv(af488luv_ff67bcas_1comp_path, sep=',').assign(sim=440*[280,], processing=440*['dirty ff67b'])
       af488luv_ff67bcas_2comp = pd.read_csv(af488luv_ff67bcas_2comp_path, sep=',').assign(sim=440*[280,], processing=440*['dirty ff67b'])
       af488luv_1dot5cas_1comp = pd.read_csv(af488luv_1dot5cas_1comp_path, sep=',').assign(sim=440*[280,], processing=440*['dirty thr1'])
       af488luv_1dot5cas_2comp = pd.read_csv(af488luv_1dot5cas_2comp_path, sep=',').assign(sim=440*[280,], processing=440*['dirty thr1'])
       af488luv_2cas_1comp = pd.read_csv(af488luv_2cas_1comp_path, sep=',').assign(sim=440*[280,], processing=440*['dirty thr2'])
       af488luv_2cas_2comp = pd.read_csv(af488luv_2cas_2comp_path, sep=',').assign(sim=440*[280,], processing=440*['dirty thr2'])
       af488luv_2dot5cas_1comp = pd.read_csv(af488luv_2dot5cas_1comp_path, sep=',').assign(sim=431*[280,], processing=431*['dirty thr3'])
       af488luv_2dot5cas_2comp = pd.read_csv(af488luv_2dot5cas_2comp_path, sep=',').assign(sim=431*[280,], processing=431*['dirty thr3'])

       tbpex5_0cd20cas_1comp = pd.read_csv(tbpex5_0cd20cas_1comp_path, sep=',').assign(sim=250*[31,], processing=250*['dirty 0cd20'])
       tbpex5_0cd20cas_2comp = pd.read_csv(tbpex5_0cd20cas_2comp_path, sep=',').assign(sim=250*[31,], processing=250*['dirty 0cd20'])
       tbpex5_ff67bcas_1comp = pd.read_csv(tbpex5_ff67bcas_1comp_path, sep=',').assign(sim=250*[31,], processing=250*['dirty ff67b'])
       tbpex5_ff67bcas_2comp = pd.read_csv(tbpex5_ff67bcas_2comp_path, sep=',').assign(sim=250*[31,], processing=250*['dirty ff67b'])
       tbpex5_5cas_1comp = pd.read_csv(tbpex5_5cas_1comp_path, sep=',').assign(sim=250*[31,], processing=250*['dirty thr1'])
       tbpex5_5cas_2comp = pd.read_csv(tbpex5_5cas_2comp_path, sep=',').assign(sim=250*[31,], processing=250*['dirty thr1'])
       tbpex5_7cas_1comp = pd.read_csv(tbpex5_7cas_1comp_path, sep=',').assign(sim=250*[31,], processing=250*['dirty thr2'])
       tbpex5_7cas_2comp = pd.read_csv(tbpex5_7cas_2comp_path, sep=',').assign(sim=250*[31,], processing=250*['dirty thr2'])
       tbpex5_10cas_1comp = pd.read_csv(tbpex5_10cas_1comp_path, sep=',').assign(sim=250*[31,], processing=250*['dirty thr3'])
       tbpex5_10cas_2comp = pd.read_csv(tbpex5_10cas_2comp_path, sep=',').assign(sim=250*[31,], processing=250*['dirty thr3'])


       # af488luv_0cd20del_1comp = pd.read_csv(af488luv_0cd20del_1comp_path, sep=',').assign(sim=440*[280,], processing=440*['dirty del'])
       # af488luv_0cd20del_2comp = pd.read_csv(af488luv_0cd20del_2comp_path, sep=',').assign(sim=440*[280,], processing=440*['dirty del'])
       # tbpex5_0cd20del_1comp = pd.read_csv(tbpex5_0cd20del_1comp_path, sep=',').assign(sim=250*[31,], processing=250*['dirty del'])
       # tbpex5_0cd20del_2comp = pd.read_csv(tbpex5_0cd20del_2comp_path, sep=',').assign(sim=250*[31,], processing=250*['dirty del'])
       #
       # af488luv_0cd20avg_1comp = pd.read_csv(af488luv_0cd20avg_1comp_path, sep=',').assign(sim=440*[280,], processing=440*['dirty avg'])
       # af488luv_0cd20avg_2comp = pd.read_csv(af488luv_0cd20avg_2comp_path, sep=',').assign(sim=440*[280,], processing=440*['dirty avg'])
       # tbpex5_0cd20avg_1comp = pd.read_csv(tbpex5_0cd20avg_1comp_path, sep=',').assign(sim=250*[31,], processing=250*['dirty avg'])
       # tbpex5_0cd20avg_2comp = pd.read_csv(tbpex5_0cd20avg_2comp_path, sep=',').assign(sim=250*[31,], processing=250*['dirty avg'])


       # # control with prediction threshold
       # odot069_thresh2_1comp = pd.read_csv(odot069_thresh2_1comp_path, sep=',').assign(sim=300*[0.069,], processing=300*['Peak artifacts,\nmanual thresh. pred.\n+ cut and shift corr.:\nscaler=robust\nthreshold=2',])
       # odot069_thresh2_2comp = pd.read_csv(odot069_thresh2_2comp_path, sep=',').assign(sim=300*[0.069,], processing=300*['Peak artifacts,\nmanual thresh. pred.\n+ cut and shift corr.:\nscaler=robust\nthreshold=2',])
       # odot2_thresh2_1comp = pd.read_csv(odot2_thresh2_1comp_path, sep=',').assign(sim=300*[0.2,], processing=300*['Peak artifacts,\nmanual thresh. pred.\n+ cut and shift corr.:\nscaler=robust\nthreshold=2',])
       # odot2_thresh2_2comp = pd.read_csv(odot2_thresh2_2comp_path, sep=',').assign(sim=300*[0.2,], processing=300*['Peak artifacts,\nmanual thresh. pred.\n+ cut and shift corr.:\nscaler=robust\nthreshold=2',])
       # # one_thresh2_1comp = pd.read_csv(one_thresh2_1comp_path, sep=',').assign(sim=300*[1,], processing=300*['Peak artifacts,\nmanual thresh. pred.\n+ cut and shift corr.:\nscaler=robust\nthreshold=2',])
       # # one_thresh2_2comp = pd.read_csv(one_thresh2_2comp_path, sep=',').assign(sim=300*[1,], processing=300*['Peak artifacts,\nmanual thresh. pred.\n+ cut and shift corr.:\nscaler=robust\nthreshold=2',])
       # three_thresh2_1comp = pd.read_csv(three_thresh2_1comp_path, sep=',').assign(sim=300*[3,], processing=300*['Peak artifacts,\nmanual thresh. pred.\n+ cut and shift corr.:\nscaler=robust\nthreshold=2',])
       # three_thresh2_2comp = pd.read_csv(three_thresh2_2comp_path, sep=',').assign(sim=300*[3,], processing=300*['Peak artifacts,\nmanual thresh. pred.\n+ cut and shift corr.:\nscaler=robust\nthreshold=2',])
       #
       # # pred. by best unet 0cd20 - check out from branch exp-220227-unet
       # odot069_0cd20_1comp = pd.read_csv(odot069_0cd20_1comp_path, sep=',').assign(sim=300*[0.069,], processing=300*['Peak artifacts,\nautomatic U-Net pred.\n+ cut and shift corr.:\nscaler=quantile transf.,\nlarge model (200MB)',])
       # odot069_0cd20_2comp = pd.read_csv(odot069_0cd20_2comp_path, sep=',').assign(sim=300*[0.069,], processing=300*['Peak artifacts,\nautomatic U-Net pred.\n+ cut and shift corr.:\nscaler=quantile transf.,\nlarge model (200MB)',])
       # odot2_0cd20_1comp = pd.read_csv(odot2_0cd20_1comp_path, sep=',').assign(sim=300*[0.2,], processing=300*['Peak artifacts,\nautomatic U-Net pred.\n+ cut and shift corr.:\nscaler=quantile transf.,\nlarge model (200MB)',])
       # odot2_0cd20_2comp = pd.read_csv(odot2_0cd20_2comp_path, sep=',').assign(sim=300*[0.2,], processing=300*['Peak artifacts,\nautomatic U-Net pred.\n+ cut and shift corr.:\nscaler=quantile transf.,\nlarge model (200MB)',])
       # # one_0cd20_1comp = pd.read_csv(one_0cd20_1comp_path, sep=',').assign(sim=300*[1,], processing=300*['Peak artifacts,\nautomatic U-Net pred.\n+ cut and shift corr.:\nscaler=quantile transf.,\nlarge model (200MB)',])
       # # one_0cd20_2comp = pd.read_csv(one_0cd20_2comp_path, sep=',').assign(sim=300*[1,], processing=300*['Peak artifacts,\nautomatic U-Net pred.\n+ cut and shift corr.:\nscaler=quantile transf.,\nlarge model (200MB)',])
       # three_0cd20_1comp = pd.read_csv(three_0cd20_1comp_path, sep=',').assign(sim=300*[3,], processing=300*['Peak artifacts,\nautomatic U-Net pred.\n+ cut and shift corr.:\nscaler=quantile transf.,\nlarge model (200MB)',])
       # three_0cd20_2comp = pd.read_csv(three_0cd20_2comp_path, sep=',').assign(sim=300*[3,], processing=300*['Peak artifacts,\nautomatic U-Net pred.\n+ cut and shift corr.:\nscaler=quantile transf.,\nlarge model (200MB)',])



       all_param = pd.concat([odot069_clean_1comp, odot2_clean_1comp, three_clean_1comp,
                              af488_1comp, af488_0cd20cas_1comp, af488_ff67bcas_1comp,
                              af488_1dot5cas_1comp, af488_2cas_1comp, af488_2dot5cas_1comp,
                              hspex5_1comp, hspex5_0cd20cas_1comp, hspex5_ff67bcas_1comp,
                              hspex5_5cas_1comp, hspex5_7cas_1comp, hspex5_10cas_1comp,
                              odot069_dirty_1comp, odot2_dirty_1comp, three_dirty_1comp,
                              odot069_dirty_2comp, odot2_dirty_2comp, three_dirty_2comp,
                              odot069_dirty_0cd20cas_1comp, odot2_dirty_0cd20cas_1comp, three_dirty_0cd20cas_1comp,
                              odot069_dirty_0cd20cas_2comp, odot2_dirty_0cd20cas_2comp, three_dirty_0cd20cas_2comp,
                              odot069_dirty_2cas_1comp, odot2_dirty_2cas_1comp, three_dirty_2cas_1comp,
                              odot069_dirty_2cas_2comp, odot2_dirty_2cas_2comp, three_dirty_2cas_2comp,
                              af488luv_1comp, tbpex5_1comp,
                              af488luv_2comp, tbpex5_2comp,
                              af488luv_0cd20cas_1comp, tbpex5_0cd20cas_1comp,
                              af488luv_0cd20cas_2comp, tbpex5_0cd20cas_2comp,
                              af488luv_ff67bcas_1comp, tbpex5_ff67bcas_1comp,
                              af488luv_ff67bcas_2comp, tbpex5_ff67bcas_2comp,
                              af488luv_1dot5cas_1comp, tbpex5_5cas_1comp,
                              af488luv_1dot5cas_2comp, tbpex5_5cas_2comp,
                              af488luv_2cas_1comp, tbpex5_7cas_1comp,
                              af488luv_2cas_2comp, tbpex5_7cas_2comp,
                              af488luv_2dot5cas_1comp, tbpex5_10cas_1comp,
                              af488luv_2dot5cas_2comp, tbpex5_10cas_2comp])


       # assert the following fit parameters
       assert set(all_param['Dimen']) == {'2D', '3D'}
       assert set(all_param[all_param['Dimen'] == '2D']['sim']) == {0.069, 0.2, 3.0}
       assert set(all_param[all_param['Dimen'] == '3D']['sim']) == {31.0, 280.0}
       assert set(all_param[(all_param['Dimen'] == '3D') & (all_param['sim'] == 31.0)]['AR1']) == {6.0}
       assert set(all_param[(all_param['Dimen'] == '3D') & (all_param['sim'] == 280.0)]['AR1']) == {5.0}
       assert set(all_param['Diff_eq']) == {'Equation 1A', 'Equation 1B'}
       assert set(all_param[all_param['Diff_eq'] == 'Equation 1A']['sim']) == {0.069, 0.2, 3.0}
       assert set(all_param[all_param['Diff_eq'] == 'Equation 1B']['sim']) == {31.0, 280.0}
       assert set(all_param['Triplet_eq']) == {'Triplet Eq 2B', 'no triplet'}
       assert set(all_param[all_param['Triplet_eq'] == 'no triplet']['sim']) == {0.069, 0.2, 3.0, 280.0}
       assert set(all_param[all_param['Triplet_eq'] == 'Triplet Eq 2B']['sim']) == {31.0}
       assert set(all_param['alpha1']) == {1.0}
       assert set(all_param['xmin']) == {0.001018, 1.0}
       assert set(all_param[all_param['xmin'] == 1.0]['sim']) == {0.069, 0.2, 3.0}
       assert set(all_param[all_param['xmin'] == 0.001018]['sim']) == {31.0, 280.0}
       assert set(all_param['xmax']) == {100.66329, 469.762042, 939.52409, 3584.0, 4096.0, 7168.0, 8192.0}
       # biological pex5 correlations were fitted with xmax=1000
       assert set(all_param[all_param['xmax'] == 939.52409]['sim']) == {31.0}
       # biological af488 correlations were fitted with xmax=500 for peak artifacts,
       # and xmax=100 for correlations without peak artifacts,
       assert set(all_param[all_param['xmax'].isin([100.66329, 469.762042])]['sim']) == {280.0}
       assert set(all_param[all_param['xmax'] == 100.66329]['processing']) == {'clean 0cd20', 'clean ff67b', 'clean noc', 'clean thr1', 'clean thr2', 'clean thr3'}
       assert set(all_param[all_param['xmax'] == 469.762042]['processing']) == {'dirty 0cd20', 'dirty ff67b', 'dirty noc', 'dirty thr1', 'dirty thr2', 'dirty thr3'}
       # simulated correlations with and without peak artifacts were fitted with xmax=8192 (except see below)
       assert set(all_param[all_param['xmax'].isin([3584.0, 4096.0, 7168.0, 8192.0])]['sim']) == {0.069, 0.2, 3.0}
       assert set(all_param[all_param['xmax'] == 8192.0]['processing']) == {'clean noc', 'dirty 0cd20', 'dirty noc', 'dirty thr1'}
       # 283, 293, or 282 of 300 correlations with peak artifacts and 0cd20 prediction and cut and stitch correction were fitted with xmax=8192
       # (3 groups depending on simulated molecule speed)
       # this failed for 41 correlations which were too short and thus automatically got xmax={3584, 4096, 7168}
       assert len(set(all_param[(all_param['xmax'] == 8192.0) & (all_param['processing'] == 'dirty 0cd20') & (all_param['sim'] == 0.069)].index)) == 283
       assert len(set(all_param[(all_param['xmax'] == 8192.0) & (all_param['processing'] == 'dirty 0cd20') & (all_param['sim'] == 0.2)].index)) == 293
       assert len(set(all_param[(all_param['xmax'] == 8192.0) & (all_param['processing'] == 'dirty 0cd20') & (all_param['sim'] == 3.0)].index)) == 282
       assert set(all_param[all_param['xmax'].isin([3584.0, 4096.0, 7168.0])]['processing']) == {'dirty 0cd20'}
       assert len(set(all_param[all_param['xmax'].isin([3584.0, 4096.0, 7168.0])]['processing'].index)) == 41

       pprint(all_param.keys())
       all_param = all_param[['name_of_plot', 'Diff_species', 'N (FCS)', 'A1', 'txy1', 'sim', 'processing', 'A2', 'txy2']]
       with pd.option_context("max_colwidth", 1000):
           display(all_param)
     #+END_SRC

     #+RESULTS:
     :RESULTS:
     : Index(['name_of_plot', 'master_file', 'parent_name', 'parent_uqid',
     :        'time of fit', 'Diff_eq', 'Diff_species', 'Triplet_eq',
     :        'Triplet_species', 'Dimen', 'xmin', 'xmax', 'offset', 'stdev(offset)',
     :        'GN0', 'stdev(GN0)', 'N (FCS)', 'cpm (kHz)', 'A1', 'stdev(A1)', 'txy1',
     :        'stdev(txy1)', 'alpha1', 'stdev(alpha1)', 'N (mom)', 'bri (kHz)',
     :        'above zero', 'sim', 'processing', 'AR1', 'stdev(AR1)', 'T1',
     :        'stdev(T1)', 'tauT1', 'stdev(tauT1)', 'A2', 'stdev(A2)', 'txy2',
     :        'stdev(txy2)', 'alpha2', 'stdev(alpha2)', 'AR2', 'stdev(AR2)'],
     :       dtype='object')
     |     | name_of_plot                                                                               | Diff_species | N (FCS)   | A1       | txy1       | sim    | processing | A2       | txy2     |
|-----+--------------------------------------------------------------------------------------------+--------------+-----------+----------+------------+--------+------------+----------+----------|
| 0   | 2022-07-21_multipletau_clean_0dot069_0000_correlation-CH1_1                                | 1            | 13.940974 | 1.000000 | 150.165642 | 0.069  | clean noc  | NaN      | NaN      |
| 1   | 2022-07-21_multipletau_clean_0dot069_0001_correlation-CH1_1                                | 1            | 16.272292 | 1.000000 | 133.758850 | 0.069  | clean noc  | NaN      | NaN      |
| 2   | 2022-07-21_multipletau_clean_0dot069_0002_correlation-CH1_1                                | 1            | 17.590156 | 1.000000 | 109.396491 | 0.069  | clean noc  | NaN      | NaN      |
| 3   | 2022-07-21_multipletau_clean_0dot069_0003_correlation-CH1_1                                | 1            | 13.739355 | 1.000000 | 140.176075 | 0.069  | clean noc  | NaN      | NaN      |
| 4   | 2022-07-21_multipletau_clean_0dot069_0004_correlation-CH1_1                                | 1            | 10.985121 | 1.000000 | 520.784146 | 0.069  | clean noc  | NaN      | NaN      |
| ... | ...                                                                                        | ...          | ...       | ...      | ...        | ...    | ...        | ...      | ...      |
| 245 | 2022-07-27_tttr2xfcs_CH2_BIN1dot0_TbPEX5EGFP 1-1000246_T5364s_1_DELSHIFT_correlation-CH2_2 | 2            | 0.984319  | 0.052588 | 12.537737  | 31.000 | dirty thr3 | 0.947412 | 0.317513 |
| 246 | 2022-07-27_tttr2xfcs_CH2_BIN1dot0_TbPEX5EGFP 1-1000247_T5386s_1_DELSHIFT_correlation-CH2_2 | 2            | 0.962060  | 0.054851 | 18.316749  | 31.000 | dirty thr3 | 0.945149 | 0.344213 |
| 247 | 2022-07-27_tttr2xfcs_CH2_BIN1dot0_TbPEX5EGFP 1-1000248_T5408s_1_DELSHIFT_correlation-CH2_2 | 2            | 0.924462  | 0.014227 | 127.214999 | 31.000 | dirty thr3 | 0.985773 | 0.359032 |
| 248 | 2022-07-27_tttr2xfcs_CH2_BIN1dot0_TbPEX5EGFP 1-1000249_T5430s_1_DELSHIFT_correlation-CH2_2 | 2            | 0.936036  | 0.046250 | 24.217742  | 31.000 | dirty thr3 | 0.953750 | 0.300107 |
| 249 | 2022-07-27_tttr2xfcs_CH2_BIN1dot0_TbPEX5EGFP 1-1000250_T5451s_1_DELSHIFT_correlation-CH2_2 | 2            | 0.898454  | 0.027107 | 27.336500  | 31.000 | dirty thr3 | 0.972893 | 0.344808 |

18604 rows × 9 columns
     :END:

     #+BEGIN_SRC jupyter-python
       def sort_fit(param_ls):
           sim = param_ls[-1]
           nfcs = param_ls[-2]

           tt, tt_low_high = ans.convert_diffcoeff_to_transittimes(sim, fwhm=250)

           array = np.array(list(param_ls)[:-2]).reshape((2, 2))
           # sort by transit times
           array = array[:, array[0, :].argsort()]
           A_fast = array[1, 0]
           A_slow = array[1, 1]
           N_fast = A_fast * nfcs
           N_slow = A_slow * nfcs
           t_fast = array[0, 0]
           t_slow = array[0, 1]
           if np.isnan(t_slow):
               # if tt_low_high[0] <= t_fast <= tt_low_high[1]:
               #     out =

               out = t_fast, nfcs, pd.NA, pd.NA, tt

           elif f'{A_fast:.0%}' == '100%':
               # if tt_low_high[0] <= t_fast <= tt_low_high[1]:
               #     out =

               out = t_fast, N_fast, pd.NA, pd.NA, tt
           elif f'{A_slow:.0%}' == '100%':
               # if tt_low_high[0] <= t_slow <= tt_low_high[1]:
               #     out =
               out = pd.NA, pd.NA, t_slow, N_slow, tt
           else:
               # if (tt_low_high[0] <= t_fast <= tt_low_high[1]) or (
               #     tt_low_high[0] <= t_slow <= tt_low_high[1]):
               #     out =
               out = t_fast, N_fast, t_slow, N_slow, tt

           return out

       def sort_fit_legend(param_ls):
           species = param_ls[0]
           component = param_ls[1]

           if species == 1:
               legend = '$\\tau_D$ from\n1 species fit'
           elif (species == 2) and (component == 'fast'):
               legend = '$\\tau_D$ from\nfast sp. of 2 sp. fit'
           elif (species == 2) and (component == 'slow'):
               legend = '$\\tau_D$ from\nslow sp. of 2 sp. fit'
           return legend

       all_param[['t_fast', 'N_fast', 't_slow', 'N_slow', 'expected transit time']] = all_param[['txy1', 'txy2', 'A1', 'A2', 'N (FCS)', 'sim']].apply(
           lambda x: sort_fit(x), axis=1, result_type='expand')
       all_param = pd.wide_to_long(all_param, stubnames=['t', 'N'],
                                   i=['name_of_plot', 'Diff_species', 'processing'],
                                   j='fit component',
                                   sep='_', suffix=r'\w+')

       all_param = all_param.reset_index()
       # if Diff_species is 1, there is only 1 component
       all_param = all_param[~((all_param['fit component'] == 'slow') & (all_param['Diff_species'] == 1))]
       all_param = all_param.reset_index()

       all_param['legend'] = all_param[['Diff_species', 'fit component']].apply(
           lambda x: sort_fit_legend(x), axis=1)
       print('before dropping NaNs')
       print('1 species fit: {}'.format(len(all_param[all_param["legend"] == "$\\tau_D$ from\n1 species fit"])))
       print('slow sp of 2 sp fit: {}'.format(len(all_param[all_param["legend"] == "$\\tau_D$ from\nslow sp. of 2 sp. fit"])))
       print('fast sp of 2 sp fit: {}'.format(len(all_param[all_param["legend"] == "$\\tau_D$ from\nfast sp. of 2 sp. fit"])))

       all_param = all_param[~pd.isna(all_param['t'])]
       print('after dropping NaNs')
       print('1 species fit: {}'.format(len(all_param[all_param["legend"] == "$\\tau_D$ from\n1 species fit"])))
       print('slow sp of 2 sp fit: {}'.format(len(all_param[all_param["legend"] == "$\\tau_D$ from\nslow sp. of 2 sp. fit"])))
       print('fast sp of 2 sp fit: {}'.format(len(all_param[all_param["legend"] == "$\\tau_D$ from\nfast sp. of 2 sp. fit"])))

       all_param = all_param[['legend', 't', 'N', 'expected transit time', 'sim', 'processing']]
       # with pd.option_context("max_colwidth", 1000):
       #     display(all_param[['legend', 't', 'A', 'expected transit time', 'sim', 'processing']])
       pub_cond1 = ((all_param['legend'] == '$\\tau_D$ from\n1 species fit') &
                    (all_param['processing'].isin(['clean noc', 'clean 0cd20', 'clean thr2', 'dirty noc', 'dirty 0cd20', 'dirty thr2'])) &
                    ~((all_param['processing'].isin(['dirty noc', 'dirty 0cd20', 'dirty thr2'])) &
                      (all_param['expected transit time'] == 0.040253767881946526)))
       pub_cond2 = ((all_param['legend'] == '$\\tau_D$ from\nfast sp. of 2 sp. fit') &
                    ((all_param['processing'].isin(['dirty noc', 'dirty 0cd20', 'dirty thr2'])) &
                     (all_param['expected transit time'] == 0.040253767881946526)))

       pub_param = all_param[pub_cond1 | pub_cond2]
       pub_param
     #+END_SRC

     #+RESULTS:
     :RESULTS:
     : before dropping NaNs
     : 1 species fit: 11773
     : slow sp of 2 sp fit: 6831
     : fast sp of 2 sp fit: 6831
     : after dropping NaNs
     : 1 species fit: 11773
     : slow sp of 2 sp fit: 6228
     : fast sp of 2 sp fit: 6577
     |       | legend                               | t          | N         | expected transit time | sim     | processing |
|-------+--------------------------------------+------------+-----------+-----------------------+---------+------------|
| 0     | $\tau_D$ from\n1 species fit         | 150.165642 | 13.940974 | 163.348623            | 0.069   | clean noc  |
| 1     | $\tau_D$ from\n1 species fit         | 133.75885  | 16.272292 | 163.348623            | 0.069   | clean noc  |
| 2     | $\tau_D$ from\n1 species fit         | 109.396491 | 17.590156 | 163.348623            | 0.069   | clean noc  |
| 3     | $\tau_D$ from\n1 species fit         | 140.176075 | 13.739355 | 163.348623            | 0.069   | clean noc  |
| 4     | $\tau_D$ from\n1 species fit         | 520.784146 | 10.985121 | 163.348623            | 0.069   | clean noc  |
| ...   | ...                                  | ...        | ...       | ...                   | ...     | ...        |
| 22882 | $\tau_D$ from\nfast sp. of 2 sp. fit | 0.045014   | 10.206993 | 0.040254              | 280.000 | dirty thr2 |
| 22884 | $\tau_D$ from\nfast sp. of 2 sp. fit | 0.053724   | 11.11189  | 0.040254              | 280.000 | dirty thr2 |
| 22886 | $\tau_D$ from\nfast sp. of 2 sp. fit | 0.046132   | 10.239869 | 0.040254              | 280.000 | dirty thr2 |
| 22888 | $\tau_D$ from\nfast sp. of 2 sp. fit | 0.039516   | 15.640813 | 0.040254              | 280.000 | dirty thr2 |
| 22890 | $\tau_D$ from\nfast sp. of 2 sp. fit | 0.0381     | 14.548366 | 0.040254              | 280.000 | dirty thr2 |

6792 rows × 6 columns
     :END:

     #+BEGIN_SRC jupyter-python
       list(set(all_param['expected transit time']))[::-1]
     #+END_SRC

     #+RESULTS:
     : 56.35527503472514

     #+BEGIN_SRC jupyter-python
       def simplot(data, x, order, height, aspect, hue, xlim, add_text='other'):
           g = sns.catplot(data=data,
                           x=x,
                           y='processing',
                           order=order,
                           col='expected transit time',
                           col_wrap=3,
                           hue=hue,
                           height=height,
                           aspect=aspect,
                           legend_out=True,
                           kind='boxen',
                           sharex=True,
                           showfliers=False)
           if hue is not None:
               g._legend.remove()
           styles = ['--', ':', '-', '-.', (0, (3, 1, 1, 1, 1, 1)), (0, (3, 1, 1, 1, 1, 1, 1, 1))]
           for i, ax in enumerate(g.axes):
               tt = str(ax.title).split('= ')
               tt = tt[1].strip("')")
               tt = float(tt)
               clean = data[(data['processing'] == 'clean noc') & (data['expected transit time'] == tt)]
               median = clean[x].median()
               if x == 'N':
                   line = ax.axvline(median, lw=4, label='', ls=styles[::-1][i])
                   line_legend = {f'\n$N{{exp}}={median:.2f}$' : line}
               else:
                   line = ax.axvline(median, lw=4, label='', ls=styles[i])
                   line_legend = {f'\n$\\tau_{{exp}}={median:.2f}ms$' : line}
               g._legend_data.update(line_legend)
           g.add_legend(g._legend_data)
           if hue is None:
               dodge = False
           else:
               dodge = True
           g.map_dataframe(sns.stripplot,
                 x=x,
                 y='processing',
                 order=order,
                 hue=hue,
                 dodge=dodge,
                 palette=sns.color_palette(['0.3']),
                 size=2,
                 jitter=0.05,
                 hue_order=['$\\tau_D$ from\n1 species fit', '$\\tau_D$ from\nfast sp. of 2 sp. fit', '$\\tau_D$ from\nslow sp. of 2 sp. fit'])

           g.fig.suptitle('', size=25)
           for ax in g.axes:
               # ax[0].set_title('')
               tt = str(ax.title).split('= ')
               tt = tt[1].strip("')")
               tt = float(tt)
               clean = data[(data['processing'] == 'clean noc') & (data['expected transit time'] == tt)]
               median = clean[x].median()
               if x == 'N':
                   ax.set_title(f'$N_{{exp}}={median:.2f}$')
               else:
                   ax.set_title(f'$\\tau_{{exp}}={median:.2f}ms$')

           if x == 't':
               xscale = 'log'
           else:
               xscale = 'linear'

           plt.setp(g.axes, xscale=xscale, xlabel='log transit time $\tau_{D}$ $[ms]$',
                    ylabel='', xlim=xlim)

           g.tight_layout()
           savefig = f'./data/exp-220316-publication1/jupyter/{add_text}'
           plt.savefig(f'{savefig}.pdf', dpi=300)
           os.system(f'pdf2svg {savefig}.pdf {savefig}.svg')
           plt.close('all')
     #+END_SRC

     #+RESULTS:

   - the following constraints are given by the nanoletters template:
     - one column: up to 240 points wide (3.33 in.)
     - double-column: between 300 and 504 points (4.167 in. and 7 in.).
     - maximum depth: 660 points (9.167 in.) including the caption (allow 12 pts.
       For each line of caption text)
     - Lettering should be no smaller than 4.5 points in the final published
       format. The text should be legible when the graphic is viewed full-size.
       Helvetica or Arial fonts work well for lettering. Lines should be no
       thinner than 0.5 point.

   - plot biological af488 data - transit times
     #+BEGIN_SRC jupyter-python
       sns.set_theme(style="whitegrid", font_scale=1, palette='colorblind',
                     context='paper')

       simplot(pub_param[(pub_param['expected transit time'] == 0.040253767881946526)],
               order=['clean noc', 'clean 0cd20', 'clean thr2', 'dirty noc', 'dirty 0cd20', 'dirty thr2'],
               x='t', height=3, aspect=3/3, hue=None, xlim=None, add_text='plot4_compare-prediction-af488_transit-times')
     #+END_SRC

     #+RESULTS:

   - plot biological af488 data - particle numbers
     #+BEGIN_SRC jupyter-python
       sns.set_theme(style="whitegrid", font_scale=1, palette='colorblind',
                     context='paper')

       simplot(pub_param[(pub_param['expected transit time'] == 0.040253767881946526)],
               order=['clean noc', 'clean 0cd20', 'clean thr2', 'dirty noc', 'dirty 0cd20', 'dirty thr2'],
               x='N', height=3, aspect=3/3, hue=None, xlim=None, add_text='plot4_compare-prediction-af488_particle-numbers')
     #+END_SRC

     #+RESULTS:

   - plot biological pex5 data - transit times
     #+BEGIN_SRC jupyter-python
       sns.set_theme(style="whitegrid", font_scale=1, palette='colorblind',
                     context='paper')
       # sns.set(rc={'figure.figsize':(3.33, 7)})
       simplot(pub_param[(pub_param['expected transit time'] == 0.36358241957887183)],
               order=['clean noc', 'clean 0cd20', 'clean thr2', 'dirty noc', 'dirty 0cd20', 'dirty thr2'],
               x='t', height=3, aspect=3/3, hue=None, xlim=[0.1, 10], add_text='plot4_compare-prediction-pex5_transit-times')
     #+END_SRC

     #+RESULTS:

   - for supplementary, plot 1 and 2 species fits, as well as additional
     prediction methods, for biological af488 data - transit times
     #+BEGIN_SRC jupyter-python
       sns.set_theme(style="whitegrid", font_scale=1, palette='colorblind',
                     context='paper')

       simplot(all_param[(all_param['expected transit time'] == 0.040253767881946526)],
               order=['clean noc', 'clean 0cd20', 'clean ff67b', 'clean thr1', 'clean thr2', 'clean thr3',
                      'dirty noc', 'dirty 0cd20', 'dirty ff67b', 'dirty thr1', 'dirty thr2', 'dirty thr3'],
               x='t', height=5, aspect=3/5, hue='legend', xlim=None, add_text='plot4_compare-prediction-af488_transit-times_allfits')
     #+END_SRC

     #+RESULTS:

   - for supplementary, plot 1 and 2 species fits, as well as additional
     prediction methods, for biological af488 data - particle numbers
     #+BEGIN_SRC jupyter-python
       sns.set_theme(style="whitegrid", font_scale=1, palette='colorblind',
                     context='paper')

       simplot(all_param[(all_param['expected transit time'] == 0.040253767881946526)],
               order=['clean noc', 'clean 0cd20', 'clean ff67b', 'clean thr1', 'clean thr2', 'clean thr3',
                      'dirty noc', 'dirty 0cd20', 'dirty ff67b', 'dirty thr1', 'dirty thr2', 'dirty thr3'],
               x='N', height=5, aspect=3/5, hue='legend', xlim=None, add_text='plot4_compare-prediction-af488_particle-numbers_allfits')
     #+END_SRC

     #+RESULTS:

   - for supplementary, plot 1 and 2 species fits, as well as additional
     prediction methods, for biological pex5 data - transit times
     #+BEGIN_SRC jupyter-python
       sns.set_theme(style="whitegrid", font_scale=1, palette='colorblind',
                     context='paper')

       simplot(all_param[(all_param['expected transit time'] == 0.36358241957887183)],
               order=['clean noc', 'clean 0cd20', 'clean ff67b', 'clean thr1', 'clean thr2', 'clean thr3',
                      'dirty noc', 'dirty 0cd20', 'dirty ff67b', 'dirty thr1', 'dirty thr2', 'dirty thr3'],
               x='t', height=5, aspect=3/5, hue='legend', xlim=[0.1, 10], add_text='plot4_compare-prediction-pex5_transit-times_allfits')
     #+END_SRC

     #+RESULTS:

   - for supplementary, plot 1 and 2 species fits, as well as additional
     prediction methods, for biological pex5 data - particle numbers
     #+BEGIN_SRC jupyter-python
       sns.set_theme(style="whitegrid", font_scale=1, palette='colorblind',
                     context='paper')

       simplot(all_param[(all_param['expected transit time'] == 0.36358241957887183)],
               order=['clean noc', 'clean 0cd20', 'clean ff67b', 'clean thr1', 'clean thr2', 'clean thr3',
                      'dirty noc', 'dirty 0cd20', 'dirty ff67b', 'dirty thr1', 'dirty thr2', 'dirty thr3'],
               x='N', height=5, aspect=3/5, hue='legend', xlim=None, add_text='plot4_compare-prediction-pex5_particle-numbers_allfits')
     #+END_SRC

     #+RESULTS:

   - for supplementary, plot 1 and 2 species fits, as well as additional
     prediction methods, for simulated data - transit times
     #+BEGIN_SRC jupyter-python
       sns.set_theme(style="whitegrid", font_scale=1, palette='colorblind',
                     context='paper')
       # sns.set(rc={'figure.figsize':(3.33, 7)})
       simplot(all_param[~(all_param['expected transit time'].isin([0.040253767881946526, 0.36358241957887183]))],
               x='t', height=5, aspect=3/5, hue='legend', xlim=[1, 10000], add_text='plot4_compare-prediction-sim_transit-times')
     #+END_SRC

     #+RESULTS:
     : /home/lex/Programme/miniconda3/envs/tf/lib/python3.9/site-packages/seaborn/categorical.py:1143: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison
     :   hue_mask = self.plot_hues[i] == hue_level
     : /home/lex/Programme/miniconda3/envs/tf/lib/python3.9/site-packages/seaborn/categorical.py:1143: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison
     :   hue_mask = self.plot_hues[i] == hue_level
     : /home/lex/Programme/miniconda3/envs/tf/lib/python3.9/site-packages/seaborn/categorical.py:1143: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison
     :   hue_mask = self.plot_hues[i] == hue_level

**** Unused: further traces
     #+BEGIN_SRC jupyter-python :pandoc t
       plot1_index = ['3.0-0.01', '3.0-0.1', '3.0-1.0',
                      '0.2-0.01', '0.2-0.1', '0.2-1.0',
                      '0.069-0.01', '0.069-0.1', '0.069-1.0']

       plot1_traceno = [1, 1, 0,
                        5, 0, 0,
                        1, 1, 0]

       plot1_titles = ['fast molecules and slow clusters:\nsimulations', 'fast molecules and medium clusters:\nsimulations', 'fast molecules and fast clusters:\nsimulations',
                       'medium molecules and slow clusters:\nsimulations', 'medium molecules and medium clusters:\nsimulations', 'medium molecules and fast clusters:\nsimulations',
                       'slow molecules and slow clusters:\nsimulations', 'slow molecules and medium clusters:\nsimulations', 'slow molecules and fast clusters:\nsimulations']

       for txt, idx, t in zip(plot1_titles, plot1_index, plot1_traceno):
           fig = plt.figure()
           ax = plt.subplot(111)
           ax.set_prop_cycle(color=[sns.color_palette()[0]])
           sns.lineplot(data=sim_dirty.loc[:, idx].iloc[:, t], label='trace')
           plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)
           plt.setp(ax, xlabel=r'Time [$ms$]', ylabel=r'Intensity [$a.u.$]',
             title=txt)
           plot1_file = f'data/exp-220316-publication1/jupyter/plot1_{txt}'.replace(' ', '_').replace('\n', '_')
           plt.savefig(f'{plot1_file}.pdf', bbox_inches='tight', dpi=300)
           os.system(f'pdf2svg {plot1_file}.pdf {plot1_file}.svg')
           os.system(f'rm {plot1_file}.pdf')

     #+END_SRC

     #+RESULTS:

     file:./data/exp-220316-publication1/jupyter/plot1_slow_molecules_and_slow_clusters:_simulations.svg
     file:./data/exp-220316-publication1/jupyter/plot1_slow_molecules_and_medium_clusters:_simulations.svg
     file:./data/exp-220316-publication1/jupyter/plot1_slow_molecules_and_fast_clusters:_simulations.svg
     file:./data/exp-220316-publication1/jupyter/plot1_medium_molecules_and_slow_clusters:_simulations.svg
     file:./data/exp-220316-publication1/jupyter/plot1_medium_molecules_and_medium_clusters:_simulations.svg
     file:./data/exp-220316-publication1/jupyter/plot1_medium_molecules_and_fast_clusters:_simulations.svg
     file:./data/exp-220316-publication1/jupyter/plot1_fast_molecules_and_slow_clusters:_simulations.svg
     file:./data/exp-220316-publication1/jupyter/plot1_fast_molecules_and_medium_clusters:_simulations.svg
     file:./data/exp-220316-publication1/jupyter/plot1_fast_molecules_and_fast_clusters:_simulations.svg


     #+BEGIN_SRC jupyter-python :pandoc t
       plot1_index = ['3.0-0.01', '3.0-0.1', '3.0-1.0',
                      '0.2-0.01', '0.2-0.1', '0.2-1.0',
                      '0.069-0.01', '0.069-0.1', '0.069-1.0']

       plot1_traceno = [1, 1, 0,
                        5, 0, 0,
                        1, 1, 0]

       plot1_titles1 = ['fast molecules:\nslow cluster labels', 'fast molecules:\nmedium cluster labels', 'fast molecules:\nfast cluster labels',
                        'medium molecules:\nslow cluster labels', 'medium molecules:\nmedium cluster labels', 'medium molecules:\nfast cluster labels',
                        'slow molecules:\nslow cluster labels', 'slow molecules:\nmedium cluster labels', 'slow molecules:\nfast cluster labels']

       for txt, idx, t in zip(plot1_titles1, plot1_index, plot1_traceno):
           fig = plt.figure()
           ax = plt.subplot(111)
           ax.set_prop_cycle(color=[sns.color_palette()[4]])
           sns.lineplot(data=sim_labels.loc[:, idx].iloc[:, t], label='cluster\ntrace')
           plt.axhline(y=0.04, xmin=0, xmax=1, label='label\nthreshold',
                       color=sns.color_palette()[7], linestyle='--')
           plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)
           plt.setp(ax, xlabel=r'Time [$ms$]', ylabel=r'Intensity [$a.u.$]',
             title=txt)
           plot1_file = f'data/exp-220316-publication1/jupyter/plot1_{txt}'.replace(' ', '_').replace('\n', '_')

           plt.savefig(f'{plot1_file}.pdf', bbox_inches='tight', dpi=300)
           os.system(f'pdf2svg {plot1_file}.pdf {plot1_file}.svg')
           os.system(f'rm {plot1_file}.pdf')

     #+END_SRC

     #+RESULTS:


     file:./data/exp-220316-publication1/jupyter/plot1_slow_molecules:_slow_cluster_labels.svg
     file:./data/exp-220316-publication1/jupyter/plot1_slow_molecules:_medium_cluster_labels.svg
     file:./data/exp-220316-publication1/jupyter/plot1_slow_molecules:_fast_cluster_labels.svg
     file:./data/exp-220316-publication1/jupyter/plot1_medium_molecules:_slow_cluster_labels.svg
     file:./data/exp-220316-publication1/jupyter/plot1_medium_molecules:_medium_cluster_labels.svg
     file:./data/exp-220316-publication1/jupyter/plot1_medium_molecules:_fast_cluster_labels.svg
     file:./data/exp-220316-publication1/jupyter/plot1_fast_molecules:_slow_cluster_labels.svg
     file:./data/exp-220316-publication1/jupyter/plot1_fast_molecules:_medium_cluster_labels.svg
     file:./data/exp-220316-publication1/jupyter/plot1_fast_molecules:_fast_cluster_labels.svg


     #+BEGIN_SRC jupyter-python

     #+END_SRC

     #+RESULTS:


     file:./data/exp-220316-publication1/jupyter/plot1_slow_molecules_and_slow_clusters:_label-based_segmentation.svg
     file:./data/exp-220316-publication1/jupyter/plot1_slow_molecules_and_medium_clusters:_label-based_segmentation.svg
     file:./data/exp-220316-publication1/jupyter/plot1_slow_molecules_and_fast_clusters:_label-based_segmentation.svg
     file:./data/exp-220316-publication1/jupyter/plot1_medium_molecules_and_slow_clusters:_label-based_segmentation.svg
     file:./data/exp-220316-publication1/jupyter/plot1_medium_molecules_and_medium_clusters:_label-based_segmentation.svg
     file:./data/exp-220316-publication1/jupyter/plot1_medium_molecules_and_fast_clusters:_label-based_segmentation.svg
     file:./data/exp-220316-publication1/jupyter/plot1_fast_molecules_and_slow_clusters:_label-based_segmentation.svg
     file:./data/exp-220316-publication1/jupyter/plot1_fast_molecules_and_medium_clusters:_label-based_segmentation.svg
     file:./data/exp-220316-publication1/jupyter/plot1_fast_molecules_and_fast_clusters:_label-based_segmentation.svg


     #+RESULTS:


     #+BEGIN_SRC jupyter-python :pandoc t
       with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.max_seq_items', None):
           display(sim_pred.columns)
     #+END_SRC

     #+RESULTS:
     :RESULTS:
     # [goto error]
     : ---------------------------------------------------------------------------
     : NameError                                 Traceback (most recent call last)
     : /tmp/ipykernel_305416/2534154020.py in <module>
     :       1 with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.max_seq_items', None):
     : ----> 2     display(sim_pred.columns)
     :
     : NameError: name 'sim_pred' is not defined
     :END:



#+begin_example
      %cd /beegfs/ye53nis/drmed-git
      import logging
      import os
      import sys

      import matplotlib.pyplot as plt
      import numpy as np
      import pandas as pd
      import seaborn as sns

      from pathlib import Path
      from pprint import pprint
      from tensorflow.keras.optimizers import Adam
      from mlflow.keras import load_model

      FLUOTRACIFY_PATH = '/beegfs/ye53nis/drmed-git/src/'
      sys.path.append(FLUOTRACIFY_PATH)
      from fluotracify.applications import corr_fit_object as cfo
      from fluotracify.training import build_model as bm

      data_path = Path(data_path)
      output_path = Path(output_path)
      log_path = output_path.parent / f'{output_path.name}.log'

      logging.basicConfig(filename=log_path,
                          filemode='w', format='%(asctime)s - %(message)s',
                          force=True)

      log = logging.getLogger(__name__)
      log.setLevel(logging.DEBUG)

      sns.set_theme(style="whitegrid", font_scale=2, palette='colorblind',
                    context='paper')
      class ParameterClass():
          """Stores parameters for correlation """
          def __init__(self):
              # Where the data is stored.
              self.data = []
              self.objectRef = []
              self.subObjectRef = []
              self.colors = ['blue', 'green', 'red', 'cyan', 'magenta',
                             'yellow', 'black']
              self.numOfLoaded = 0
              # very fast from Ncasc ~ 14 onwards
              self.NcascStart = 0
              self.NcascEnd = 30  # 25
              self.Nsub = 6  # 6
              self.photonLifetimeBin = 10  # used for photon decay
              self.photonCountBin = 1  # used for time series

      par_obj = ParameterClass()

      model_ls = ['ff67be0b68e540a9a29a36a2d0c7a5be', '347669d050f344ad9fb9e480c814f727',
                  '714af8cd12c1441eac4ca980e8c20070', '34a6d207ac594035b1009c330fb67a65',
                  '484af471c61943fa90e5f78e78a229f0', '0cd2023eeaf745aca0d3e8ad5e1fc653',
                  'fe81d71c52404ed790b3a32051258da9', '19e3e786e1bc4e2b93856f5dc9de8216',
                  'c1204e3a8a1e4c40a35b5b7b1922d1ce']

      model_name_ls = [f'{s:.5}' for s in model_ls]

      # scaler_ls = ['minmax', 'robust', 'maxabs', 'l2', 'standard', 'quant_g', 'standard',
      #              'standard', 'robust']

      pred_thresh = 0.5

      if data_path.name == "1911DD_atto+LUVs":
          path_clean1 = data_path / 'clean_ptu_part1/'
          path_clean2 = data_path / 'clean_ptu_part2/'
          path_dirty1 = data_path / 'dirty_ptu_part1/'
          path_dirty2 = data_path / 'dirty_ptu_part2/'
          files_clean1 = [path_clean1 / f for f in os.listdir(path_clean1) if f.endswith('.ptu')]
          files_clean2 = [path_clean2 / f for f in os.listdir(path_clean2) if f.endswith('.ptu')]
          files_dirty1 = [path_dirty1 / f for f in os.listdir(path_dirty1) if f.endswith('.ptu')]
          files_dirty2 = [path_dirty2 / f for f in os.listdir(path_dirty2) if f.endswith('.ptu')]

      if data_path.name == "191113_Pex5_2_structured":
          path_clean = data_path / 'HsPEX5EGFP 1-100001'
          path_dirty = data_path / 'TbPEX5EGFP 1-10002'
          files_clean = [path_clean / f for f in os.listdir(path_clean) if f.endswith('.ptu')]
          files_dirty = [path_dirty / f for f in os.listdir(path_dirty) if f.endswith('.ptu')]

      def predict_correct_correlate_ptu(files, model_id, method, out_path):

          logged_scaler = Path(f'/beegfs/ye53nis/drmed-git/data/mlruns/10/{model_ls[model_id]}/params/scaler')
          logged_scaler = !cat $logged_scaler
          logged_scaler = logged_scaler[0]

          logged_model = Path(f'/beegfs/ye53nis/drmed-git/data/mlruns/10/{model_ls[model_id]}/artifacts/model')
          logged_model = load_model(logged_model, compile=False)
          logged_model.compile(loss=bm.binary_ce_dice_loss(),
                               optimizer=Adam(),
                               metrics = bm.unet_metrics([0.1, 0.3, 0.5, 0.7, 0.9]))



          replace('"', '')


       plot1_titles2 = ['fast molecules and slow clusters:\nlabel-based segmentation',
                        'fast molecules and medium clusters:\nlabel-based segmentation',
                        'fast molecules and fast clusters:\nlabel-based segmentation',
                        'medium molecules and slow clusters:\nlabel-based segmentation',
                        'medium molecules and medium clusters:\nlabel-based segmentation',
                        'medium molecules and fast clusters:\nlabel-based segmentation',
                        'slow molecules and slow clusters:\nlabel-based segmentation',
                        'slow molecules and medium clusters:\nlabel-based segmentation',
                        'slow molecules and fast clusters:\nlabel-based segmentation']

       plot1_titles3 = ['fast molecules:\nslow cluster prediction',
                        'fast molecules:\nmedium cluster prediction',
                        'fast molecules:\nfast cluster prediction',
                        'medium molecules:\nslow cluster prediction',
                        'medium molecules:\nmedium cluster prediction',
                        'medium molecules:\nfast cluster prediction',
                        'slow molecules:\nslow cluster prediction',
                        'slow molecules:\nmedium cluster prediction',
                        'slow molecules:\nfast cluster prediction']

       plot1_titles4 = ['fast molecules and slow clusters:\nprediction-based segmentation',
                        'fast molecules and medium clusters:\nprediction-based segmentation',
                        'fast molecules and fast clusters:\nprediction-based segmentation',
                        'medium molecules and slow clusters:\nprediction-based segmentation',
                        'medium molecules and medium clusters:\nprediction-based segmentation',
                        'medium molecules and fast clusters:\nprediction-based segmentation',
                        'slow molecules and slow clusters:\nprediction-based segmentation',
                        'slow molecules and medium clusters:\nprediction-based segmentation',
                        'slow molecules and fast clusters:\nprediction-based segmentation']

       plot1_titles5 = ['fast molecules and slow clusters:\nprediction-based "cut and shift" correction',
                        'fast molecules and medium clusters:\nprediction-based "cut and shift" correction',
                        'fast molecules and fast clusters:\nprediction-based "cut and shift" correction',
                        'medium molecules and slow clusters:\nprediction-based "cut and shift" correction',
                        'medium molecules and medium clusters:\nprediction-based "cut and shift" correction',
                        'medium molecules and fast clusters:\nprediction-based "cut and shift" correction',
                        'slow molecules and slow clusters:\nprediction-based "cut and shift" correction',
                        'slow molecules and medium clusters:\nprediction-based "cut and shift" correction',
                        'slow molecules and fast clusters:\nprediction-based "cut and shift" correction']


#+end_example


     #+RESULTS:

   - plot correlations, then fit with =focus-fit-js=
     #+BEGIN_SRC jupyter-python :pandoc t
       # sns.lineplot(data=sim_pred.loc[:, idx].iloc[:, 0], label='trace')
       # sim_dirty.drop(sim_predbool, axis='index')





       corr_fn = multipletau.autocorrelate(
                a=self.timeSeries[f'{name[0]}'][f'{name[1]}'],
                m=16,
                deltat=self.photonCountBin[f'{name[0]}'],
                normalize=True)


     #+END_SRC

     #+RESULTS:

     [[file:./data/exp-220316-publication1/jupyter/edf056e7749bf85f1d2b663fea4e3e33b4ece5f7.png]]


**** Unused: other

     -----copied from exp-220227-unet
     #+BEGIN_SRC jupyter-python

      all_param = pd.wide_to_long(all_param, stubnames='txy',
                                  i=['name_of_plot'],
                                  j='fitted species (txy)')
      all_param = all_param.reset_index()
      all_param = pd.wide_to_long(all_param, stubnames='A',
                                  i=['name_of_plot', 'fitted species (txy)'],
                                  j='fitted species (A)')
      all_param = all_param.reset_index()
      all_param
    #+END_SRC

    #+RESULTS:
    :RESULTS:
    |       | name_of_plot                                      | fitted species (txy) | fitted species (A) | N (FCS)   | parent_name          | master_file | stdev(A1) | stdev(A2) | parent_uqid | Dimen | ... | cpm (kHz) | alpha1 | xmax   | artifact_speed | stdev(offset) | stdev(txy1) | offset    | bri (kHz) | Diff_species | A        |
|-------+---------------------------------------------------+----------------------+--------------------+-----------+----------------------+-------------+-----------+-----------+-------------+-------+-----+-----------+--------+--------+----------------+---------------+-------------+-----------+-----------+--------------+----------|
| 0     | 2022-04-25_multipletau_0cd20_0dot069-0dot01_26... | 1                    | 2                  | 37.523452 | 0cd20_0dot069-0dot01 | Not known   | None      | None      | 0           | 2D    | ... | 0.026650  | 1.0    | 8192.0 | 1127.0         | None          | None        | -0.001696 | 1.0       | 2            | 0.001292 |
| 1     | 2022-04-25_multipletau_0cd20_0dot069-0dot01_26... | 1                    | 1                  | 37.523452 | 0cd20_0dot069-0dot01 | Not known   | None      | None      | 0           | 2D    | ... | 0.026650  | 1.0    | 8192.0 | 1127.0         | None          | None        | -0.001696 | 1.0       | 2            | 0.998708 |
| 2     | 2022-04-25_multipletau_0cd20_0dot069-0dot01_26... | 1                    | 2                  | 35.394544 | 0cd20_0dot069-0dot01 | Not known   | None      | None      | 0           | 2D    | ... | 0.028253  | 1.0    | 8192.0 | 1127.0         | None          | None        | -0.001883 | 1.0       | 2            | 0.002290 |
| 3     | 2022-04-25_multipletau_0cd20_0dot069-0dot01_26... | 1                    | 1                  | 35.394544 | 0cd20_0dot069-0dot01 | Not known   | None      | None      | 0           | 2D    | ... | 0.028253  | 1.0    | 8192.0 | 1127.0         | None          | None        | -0.001883 | 1.0       | 2            | 0.997710 |
| 4     | 2022-04-25_multipletau_0cd20_0dot069-0dot01_26... | 1                    | 2                  | 27.754107 | 0cd20_0dot069-0dot01 | Not known   | None      | None      | 0           | 2D    | ... | 0.036031  | 1.0    | 8192.0 | 1127.0         | None          | None        | -0.008520 | 1.0       | 2            | 0.526621 |
| ...   | ...                                               | ...                  | ...                | ...       | ...                  | ...         | ...       | ...       | ...         | ...   | ... | ...       | ...    | ...    | ...            | ...           | ...         | ...       | ...       | ...          | ...      |
| 11995 | 2022-04-25_multipletau_0cd20_50dot0-1dot0_1797... | 2                    | 1                  | 1.000001  | 0cd20_50dot0-1dot0   | Not known   | None      | None      | 0           | 2D    | ... | 0.999999  | 1.0    | 1024.0 | 11.3           | None          | None        | -0.000090 | 1.0       | 2            | 1.000000 |
| 11996 | 2022-04-25_multipletau_0cd20_50dot0-1dot0_1798... | 2                    | 2                  | 1.000001  | 0cd20_50dot0-1dot0   | Not known   | None      | None      | 0           | 2D    | ... | 0.999999  | 1.0    | 1024.0 | 11.3           | None          | None        | -0.000090 | 1.0       | 2            | 0.000000 |
| 11997 | 2022-04-25_multipletau_0cd20_50dot0-1dot0_1798... | 2                    | 1                  | 1.000001  | 0cd20_50dot0-1dot0   | Not known   | None      | None      | 0           | 2D    | ... | 0.999999  | 1.0    | 1024.0 | 11.3           | None          | None        | -0.000090 | 1.0       | 2            | 1.000000 |
| 11998 | 2022-04-25_multipletau_0cd20_50dot0-1dot0_1799... | 2                    | 2                  | 1.000001  | 0cd20_50dot0-1dot0   | Not known   | None      | None      | 0           | 2D    | ... | 0.999999  | 1.0    | 1024.0 | 11.3           | None          | None        | -0.000090 | 1.0       | 2            | 0.000000 |
| 11999 | 2022-04-25_multipletau_0cd20_50dot0-1dot0_1799... | 2                    | 1                  | 1.000001  | 0cd20_50dot0-1dot0   | Not known   | None      | None      | 0           | 2D    | ... | 0.999999  | 1.0    | 1024.0 | 11.3           | None          | None        | -0.000090 | 1.0       | 2            | 1.000000 |

12000 rows × 37 columns
    :END:

    #+BEGIN_SRC jupyter-python
      # data = all_param.loc[all_param['correction'].isin(['no correction', 'delete and shift'])]

      g = sns.catplot(data=all_param,
                      y='txy',
                      x='model',
                      hue='fitted species (txy)',
                      col='artifact_speed',
                      row='stationary_speed',
                      sharey=True,
                      height=5,
                      aspect=1,
                      legend_out=True,
                      kind='boxen',
                      showfliers=False,
                      margin_titles=True)
      g.map_dataframe(sns.stripplot,
            y='txy',
            x='model',
            hue="fitted species (txy)",
            dodge=True,
            palette=sns.color_palette(['0.3']),
            size=4,
            jitter=0.2)
      # add hlines
      artifact_speed = np.tile(sorted(set(all_param['artifact_speed'])), 10)
      stationary_speed = np.repeat(sorted(set(all_param['stationary_speed'])), 3)
      g.tight_layout()
      g.fig.suptitle('Prediction and "cut and shift" correction applied on simulated test data', y=1.03, size=20)
      for i, axes in enumerate(g.axes.flat):
      #      _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=45)
          axes.axhline(artifact_speed[i], ls='--', lw=4, c=sns.color_palette()[1])
          axes.axhline(stationary_speed[i], ls='--', lw=4, c=sns.color_palette()[0])
      plt.setp(g.axes, yscale='log', # xlabel='model',
               ylabel=r'transit time $\tau_{D}$ (log)')
      # plt.setp(g.legend, title='Correlator and\nfit parameter')
      plot1_file = 'analysis1_0cd20_simulation_transit-times'
      plt.show()
    #+END_SRC

    #+RESULTS:
    :RESULTS:
    : 0
    [[file:./data/exp-220227-unet/jupyter/ca7e75f176820ccd6a0135473cc2d59ad6c30316.png]]
    :END:
    [[file:./data/exp-220227-unet/jupyter/analysis1_0cd20_simulation_transit-times.svg]]

    #+BEGIN_SRC jupyter-python
            # data = all_param.loc[all_param['correction'].isin(['no correction', 'delete and shift'])]

      g = sns.catplot(data=all_param,
                      y='A',
                      x='model',
                      hue='fitted species (A)',
                      col='artifact_speed',
                      row='stationary_speed',
                      sharey=True,
                      height=5,
                      aspect=1,
                      legend_out=True,
                      kind='boxen',
                      showfliers=False,
                      margin_titles=True)
      g.map_dataframe(sns.stripplot,
            y='A',
            x='model',
            hue="fitted species (A)",
            dodge=True,
            palette=sns.color_palette(['0.3']),
            size=4,
            jitter=0.2)
      # add hlines
      g.tight_layout()
      g.fig.suptitle('Prediction and "cut and shift" correction applied on simulated test data', y=1.03, size=20)
      #for i, axes in enumerate(g.axes.flat):
      #      _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=45)
      plt.setp(g.axes, # xlabel='model',
               ylabel=r'relative fitted fraction size')
      # plt.setp(g.legend, title='Correlator and\nfit parameter')

      plot1_file = 'analysis1_0cd20_simulation_fraction-size'
      plt.savefig(f'{plot1_file}.pdf', bbox_inches='tight', dpi=300)
      os.system(f'pdf2svg {plot1_file}.pdf {plot1_file}.svg')
      os.system(f'rm {plot1_file}.pdf')
    #+END_SRC

**** further code
    - now load all
      #+begin_details
      #+BEGIN_SRC jupyter-python
        path = Path('data/exp-220227-unet/2022-04-25_simulations/0cd20')

        odot069_0dot01_path = path / '0.069_results/0cd20_0dot069-0dot01_outputParam.csv'
        odot069_0dot1_path = path / '0.069_results/0cd20_0dot069-0dot1_outputParam.csv'
        odot069_1dot0_path = path / '0.069_results/0cd20_0dot069-1dot0_outputParam.csv'

        odot08_0dot01_path = path / '0.08_results/0cd20_0dot08-0dot01_outputParam.csv'
        odot08_0dot1_path = path / '0.08_results/0cd20_0dot08-0dot1_outputParam.csv'
        odot08_1dot0_path = path / '0.08_results/0cd20_0dot08-1dot0_outputParam.csv'

        odot1_0dot01_path = path / '0.1_results/0cd20_0dot1-0dot01_outputParam.csv'
        odot1_0dot1_path = path / '0.1_results/0cd20_0dot1-0dot1_outputParam.csv'
        odot1_1dot0_path = path / '0.1_results/0cd20_0dot1-1dot0_outputParam.csv'

        odot2_0dot01_path = path / '0.2_results/0cd20_0dot2-0dot01_outputParam.csv'
        odot2_0dot1_path = path / '0.2_results/0cd20_0dot2-0dot1_outputParam.csv'
        odot2_1dot0_path = path / '0.2_results/0cd20_0dot2-1dot0_outputParam.csv'

        odot4_0dot01_path = path / '0.4_results/0cd20_0dot4-0dot01_outputParam.csv'
        odot4_0dot1_path = path / '0.4_results/0cd20_0dot4-0dot1_outputParam.csv'
        odot4_1dot0_path = path / '0.4_results/0cd20_0dot4-1dot0_outputParam.csv'

        odot6_0dot01_path = path / '0.6_results/0cd20_0dot6-0dot01_outputParam.csv'
        odot6_0dot1_path = path / '0.6_results/0cd20_0dot6-0dot1_outputParam.csv'
        odot6_1dot0_path = path / '0.6_results/0cd20_0dot6-1dot0_outputParam.csv'

        one_0dot01_path = path / '1.0_results/0cd20_1dot0-0dot01_outputParam.csv'
        one_0dot1_path = path / '1.0_results/0cd20_1dot0-0dot1_outputParam.csv'
        one_1dot0_path = path / '1.0_results/0cd20_1dot0-1dot0_outputParam.csv'

        three_0dot01_path = path / '3.0_results/0cd20_3dot0-0dot01_outputParam.csv'
        three_0dot1_path = path / '3.0_results/0cd20_3dot0-0dot1_outputParam.csv'
        three_1dot0_path = path / '3.0_results/0cd20_3dot0-1dot0_outputParam.csv'

        ten_0dot01_path = path / '10.0_results/0cd20_10dot0-0dot01_outputParam.csv'
        ten_0dot1_path = path / '10.0_results/0cd20_10dot0-0dot1_outputParam.csv'
        ten_1dot0_path = path / '10.0_results/0cd20_10dot0-1dot0_outputParam.csv'

        fifty_0dot01_path = path / '50.0_results/0cd20_50dot0-0dot01_outputParam.csv'
        fifty_0dot1_path = path / '50.0_results/0cd20_50dot0-0dot1_outputParam.csv'
        fifty_1dot0_path = path / '50.0_results/0cd20_50dot0-1dot0_outputParam.csv'

      #+END_SRC

      #+RESULTS:
      #+end_details

    - we load the data and combine it. We convert the simulated D [um2/s] in
      t[ms]. Based on an exploratory plot of A1 and A2 we swap A1-A2 and
      txy1-txy2, so that A1 and txy1 always represent the bigger fitted fraction
      size.
      #+begin_details
      #+BEGIN_SRC jupyter-python
        odot069_0dot01 = pd.read_csv(odot069_0dot01_path, sep=',').assign(
            artifact_speed=100*[1127,], stationary_speed=100*[163,],
            model=100*['0cd20',], correction=100*['cut and shift',])
        odot069_0dot1 = pd.read_csv(odot069_0dot1_path, sep=',').assign(
            artifact_speed=100*[113,], stationary_speed=100*[163,],
            model=100*['0cd20',], correction=100*['cut and shift',])
        odot069_1dot0 = pd.read_csv(odot069_1dot0_path, sep=',').assign(
            artifact_speed=100*[11.3,], stationary_speed=100*[163,],
            model=100*['0cd20',], correction=100*['cut and shift',])

        odot08_0dot01 = pd.read_csv(odot08_0dot01_path, sep=',').assign(
            artifact_speed=100*[1127,], stationary_speed=100*[141,],
            model=100*['0cd20',], correction=100*['cut and shift',])
        odot08_0dot01[['txy1', 'txy2']] = odot08_0dot01[['txy2', 'txy1']]
        odot08_0dot01[['A1', 'A2']] = odot08_0dot01[['A2', 'A1']]
        odot08_0dot1 = pd.read_csv(odot08_0dot1_path, sep=',').assign(
            artifact_speed=100*[113,], stationary_speed=100*[141,],
            model=100*['0cd20',], correction=100*['cut and shift',])
        odot08_1dot0 = pd.read_csv(odot08_1dot0_path, sep=',').assign(
            artifact_speed=100*[11.3,], stationary_speed=100*[141,],
            model=100*['0cd20',], correction=100*['cut and shift',])

        odot1_0dot01 = pd.read_csv(odot1_0dot01_path, sep=',').assign(
            artifact_speed=100*[1127,], stationary_speed=100*[113,],
            model=100*['0cd20',], correction=100*['cut and shift',])
        odot1_0dot1 = pd.read_csv(odot1_0dot1_path, sep=',').assign(
            artifact_speed=100*[113,], stationary_speed=100*[113,],
            model=100*['0cd20',], correction=100*['cut and shift',])
        odot1_1dot0 = pd.read_csv(odot1_1dot0_path, sep=',').assign(
            artifact_speed=100*[11.3,], stationary_speed=100*[113,],
            model=100*['0cd20',], correction=100*['cut and shift',])

        odot2_0dot01 = pd.read_csv(odot2_0dot01_path, sep=',').assign(
            artifact_speed=100*[1127,], stationary_speed=100*[56.4,],
            model=100*['0cd20',], correction=100*['cut and shift',])
        odot2_0dot1 = pd.read_csv(odot2_0dot1_path, sep=',').assign(
            artifact_speed=100*[113,], stationary_speed=100*[56.4,],
            model=100*['0cd20',], correction=100*['cut and shift',])
        odot2_0dot1[['txy1', 'txy2']] = odot2_0dot1[['txy2', 'txy1']]
        odot2_0dot1[['A1', 'A2']] = odot2_0dot1[['A2', 'A1']]
        odot2_1dot0 = pd.read_csv(odot2_1dot0_path, sep=',').assign(
            artifact_speed=100*[11.3,], stationary_speed=100*[56.4,],
            model=100*['0cd20',], correction=100*['cut and shift',])

        odot4_0dot01 = pd.read_csv(odot4_0dot01_path, sep=',').assign(
            artifact_speed=100*[1127,], stationary_speed=100*[28.2,],
            model=100*['0cd20',], correction=100*['cut and shift',])
        odot4_0dot1 = pd.read_csv(odot4_0dot1_path, sep=',').assign(
            artifact_speed=100*[113,], stationary_speed=100*[28.2,],
            model=100*['0cd20',], correction=100*['cut and shift',])
        odot4_0dot1[['txy1', 'txy2']] = odot4_0dot1[['txy2', 'txy1']]
        odot4_0dot1[['A1', 'A2']] = odot4_0dot1[['A2', 'A1']]
        odot4_1dot0 = pd.read_csv(odot4_1dot0_path, sep=',').assign(
            artifact_speed=100*[11.3,], stationary_speed=100*[28.2,],
            model=100*['0cd20',], correction=100*['cut and shift',])

        odot6_0dot01 = pd.read_csv(odot6_0dot01_path, sep=',').assign(
            artifact_speed=100*[1127,], stationary_speed=100*[18.8,],
            model=100*['0cd20',], correction=100*['cut and shift',])
        odot6_0dot1 = pd.read_csv(odot6_0dot1_path, sep=',').assign(
            artifact_speed=100*[113,], stationary_speed=100*[18.8,],
            model=100*['0cd20',], correction=100*['cut and shift',])
        odot6_1dot0 = pd.read_csv(odot6_1dot0_path, sep=',').assign(
            artifact_speed=100*[11.3,], stationary_speed=100*[18.8,],
            model=100*['0cd20',], correction=100*['cut and shift',])

        one_0dot01 = pd.read_csv(one_0dot01_path, sep=',').assign(
            artifact_speed=100*[1127,], stationary_speed=100*[11.3,],
            model=100*['0cd20',], correction=100*['cut and shift',])
        one_0dot1 = pd.read_csv(one_0dot1_path, sep=',').assign(
            artifact_speed=100*[113,], stationary_speed=100*[11.3,],
            model=100*['0cd20',], correction=100*['cut and shift',])
        one_1dot0 = pd.read_csv(one_1dot0_path, sep=',').assign(
            artifact_speed=100*[11.3,], stationary_speed=100*[11.3,],
            model=100*['0cd20',], correction=100*['cut and shift',])

        three_0dot01 = pd.read_csv(three_0dot01_path, sep=',').assign(
            artifact_speed=100*[1127,], stationary_speed=100*[3.76,],
            model=100*['0cd20',], correction=100*['cut and shift',])
        three_0dot1 = pd.read_csv(three_0dot1_path, sep=',').assign(
            artifact_speed=100*[113,], stationary_speed=100*[3.76,],
            model=100*['0cd20',], correction=100*['cut and shift',])
        three_0dot1[['txy1', 'txy2']] = three_0dot1[['txy2', 'txy1']]
        three_0dot1[['A1', 'A2']] = three_0dot1[['A2', 'A1']]
        three_1dot0 = pd.read_csv(three_1dot0_path, sep=',').assign(
            artifact_speed=100*[11.3,], stationary_speed=100*[3.76,],
            model=100*['0cd20',], correction=100*['cut and shift',])

        ten_0dot01 = pd.read_csv(ten_0dot01_path, sep=',').assign(
            artifact_speed=100*[1127,], stationary_speed=100*[1.13,],
            model=100*['0cd20',], correction=100*['cut and shift',])
        ten_0dot1 = pd.read_csv(ten_0dot1_path, sep=',').assign(
            artifact_speed=100*[113,], stationary_speed=100*[1.13,],
            model=100*['0cd20',], correction=100*['cut and shift',])
        ten_1dot0 = pd.read_csv(ten_1dot0_path, sep=',').assign(
            artifact_speed=100*[11.3,], stationary_speed=100*[1.13,],
            model=100*['0cd20',], correction=100*['cut and shift',])

        fifty_0dot01 = pd.read_csv(fifty_0dot01_path, sep=',').assign(
            artifact_speed=100*[1127,], stationary_speed=100*[0.23,],
            model=100*['0cd20',], correction=100*['cut and shift',])
        fifty_0dot1 = pd.read_csv(fifty_0dot1_path, sep=',').assign(
            artifact_speed=100*[113,], stationary_speed=100*[0.23,],
            model=100*['0cd20',], correction=100*['cut and shift',])
        fifty_0dot1[['txy1', 'txy2']] = fifty_0dot1[['txy2', 'txy1']]
        fifty_0dot1[['A1', 'A2']] = fifty_0dot1[['A2', 'A1']]
        fifty_1dot0 = pd.read_csv(fifty_1dot0_path, sep=',').assign(
            artifact_speed=100*[11.3,], stationary_speed=100*[0.23,],
            model=100*['0cd20',], correction=100*['cut and shift',])

        all_param = pd.concat([odot069_0dot01, odot069_0dot1, odot069_1dot0,
                               odot08_0dot01, odot08_0dot1, odot08_1dot0,
                               odot1_0dot01, odot1_0dot1, odot1_1dot0,
                               odot2_0dot01, odot2_0dot1, odot2_1dot0,
                               odot4_0dot01, odot4_0dot1, odot4_1dot0,
                               odot6_0dot01, odot6_0dot1, odot6_1dot0,
                               one_0dot01, one_0dot1, one_1dot0,
                               three_0dot01, three_0dot1, three_1dot0,
                               ten_0dot01, ten_0dot1, ten_1dot0,
                               fifty_0dot01, fifty_0dot1, fifty_1dot0],
                              ignore_index=True)

        all_param = pd.wide_to_long(all_param, stubnames='txy',
                                    i=['name_of_plot'],
                                    j='fitted species (txy)')
        all_param = all_param.reset_index()
        all_param = pd.wide_to_long(all_param, stubnames='A',
                                    i=['name_of_plot', 'fitted species (txy)'],
                                    j='fitted species (A)')
        all_param = all_param.reset_index()
        all_param
      #+END_SRC
      #+end_details

      #+RESULTS:
      :RESULTS:
      |       | name_of_plot                                      | fitted species (txy) | fitted species (A) |   N (FCS) | parent_name          | master_file | stdev(A1) | stdev(A2) | parent_uqid | Dimen | ... | cpm (kHz) | alpha1 |   xmax | artifact_speed | stdev(offset) | stdev(txy1) |    offset | bri (kHz) | Diff_species |        A |
      |-------+---------------------------------------------------+----------------------+--------------------+-----------+----------------------+-------------+-----------+-----------+-------------+-------+-----+-----------+--------+--------+----------------+---------------+-------------+-----------+-----------+--------------+----------|
      |     0 | 2022-04-25_multipletau_0cd20_0dot069-0dot01_26... |                    1 |                  2 | 37.523452 | 0cd20_0dot069-0dot01 | Not known   | None      | None      |           0 |    2D | ... |  0.026650 |    1.0 | 8192.0 |         1127.0 | None          | None        | -0.001696 |       1.0 |            2 | 0.001292 |
      |     1 | 2022-04-25_multipletau_0cd20_0dot069-0dot01_26... |                    1 |                  1 | 37.523452 | 0cd20_0dot069-0dot01 | Not known   | None      | None      |           0 |    2D | ... |  0.026650 |    1.0 | 8192.0 |         1127.0 | None          | None        | -0.001696 |       1.0 |            2 | 0.998708 |
      |     2 | 2022-04-25_multipletau_0cd20_0dot069-0dot01_26... |                    1 |                  2 | 35.394544 | 0cd20_0dot069-0dot01 | Not known   | None      | None      |           0 |    2D | ... |  0.028253 |    1.0 | 8192.0 |         1127.0 | None          | None        | -0.001883 |       1.0 |            2 | 0.002290 |
      |     3 | 2022-04-25_multipletau_0cd20_0dot069-0dot01_26... |                    1 |                  1 | 35.394544 | 0cd20_0dot069-0dot01 | Not known   | None      | None      |           0 |    2D | ... |  0.028253 |    1.0 | 8192.0 |         1127.0 | None          | None        | -0.001883 |       1.0 |            2 | 0.997710 |
      |     4 | 2022-04-25_multipletau_0cd20_0dot069-0dot01_26... |                    1 |                  2 | 27.754107 | 0cd20_0dot069-0dot01 | Not known   | None      | None      |           0 |    2D | ... |  0.036031 |    1.0 | 8192.0 |         1127.0 | None          | None        | -0.008520 |       1.0 |            2 | 0.526621 |
      |   ... | ...                                               |                  ... |                ... |       ... | ...                  | ...         | ...       | ...       |         ... |   ... | ... |       ... |    ... |    ... |            ... | ...           | ...         |       ... |       ... |          ... |      ... |
      | 11995 | 2022-04-25_multipletau_0cd20_50dot0-1dot0_1797... |                    2 |                  1 |  1.000001 | 0cd20_50dot0-1dot0   | Not known   | None      | None      |           0 |    2D | ... |  0.999999 |    1.0 | 1024.0 |           11.3 | None          | None        | -0.000090 |       1.0 |            2 | 1.000000 |
      | 11996 | 2022-04-25_multipletau_0cd20_50dot0-1dot0_1798... |                    2 |                  2 |  1.000001 | 0cd20_50dot0-1dot0   | Not known   | None      | None      |           0 |    2D | ... |  0.999999 |    1.0 | 1024.0 |           11.3 | None          | None        | -0.000090 |       1.0 |            2 | 0.000000 |
      | 11997 | 2022-04-25_multipletau_0cd20_50dot0-1dot0_1798... |                    2 |                  1 |  1.000001 | 0cd20_50dot0-1dot0   | Not known   | None      | None      |           0 |    2D | ... |  0.999999 |    1.0 | 1024.0 |           11.3 | None          | None        | -0.000090 |       1.0 |            2 | 1.000000 |
      | 11998 | 2022-04-25_multipletau_0cd20_50dot0-1dot0_1799... |                    2 |                  2 |  1.000001 | 0cd20_50dot0-1dot0   | Not known   | None      | None      |           0 |    2D | ... |  0.999999 |    1.0 | 1024.0 |           11.3 | None          | None        | -0.000090 |       1.0 |            2 | 0.000000 |
      | 11999 | 2022-04-25_multipletau_0cd20_50dot0-1dot0_1799... |                    2 |                  1 |  1.000001 | 0cd20_50dot0-1dot0   | Not known   | None      | None      |           0 |    2D | ... |  0.999999 |    1.0 | 1024.0 |           11.3 | None          | None        | -0.000090 |       1.0 |            2 | 1.000000 |

  12000 rows × 37 columns
      :END:

      #+begin_details
      #+BEGIN_SRC jupyter-python
        # data = all_param.loc[all_param['correction'].isin(['no correction', 'delete and shift'])]

        g = sns.catplot(data=all_param,
                        y='txy',
                        x='model',
                        hue='fitted species (txy)',
                        col='artifact_speed',
                        row='stationary_speed',
                        sharey=True,
                        height=5,
                        aspect=1,
                        legend_out=True,
                        kind='boxen',
                        showfliers=False,
                        margin_titles=True)
        g.map_dataframe(sns.stripplot,
              y='txy',
              x='model',
              hue="fitted species (txy)",
              dodge=True,
              palette=sns.color_palette(['0.3']),
              size=4,
              jitter=0.2)
        # add hlines
        artifact_speed = np.tile(sorted(set(all_param['artifact_speed'])), 10)
        stationary_speed = np.repeat(sorted(set(all_param['stationary_speed'])), 3)
        g.tight_layout()
        g.fig.suptitle('Prediction and "cut and shift" correction applied on simulated test data', y=1.03, size=20)
        for i, axes in enumerate(g.axes.flat):
        #      _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=45)
            axes.axhline(artifact_speed[i], ls='--', lw=4, c=sns.color_palette()[1])
            axes.axhline(stationary_speed[i], ls='--', lw=4, c=sns.color_palette()[0])
        plt.setp(g.axes, yscale='log', # xlabel='model',
                 ylabel=r'transit time $\tau_{D}$ (log)')
        # plt.setp(g.legend, title='Correlator and\nfit parameter')
        plot1_file = 'analysis1_0cd20_simulation_transit-times'
        plt.show()
      #+END_SRC
      #+end_details

      #+RESULTS:
      :RESULTS:
      : 0
      [[file:./data/exp-220227-unet/jupyter/ca7e75f176820ccd6a0135473cc2d59ad6c30316.png]]
      :END:
      [[file:./data/exp-220227-unet/jupyter/analysis1_0cd20_simulation_transit-times.svg]]

      #+begin_details
      #+BEGIN_SRC jupyter-python
              # data = all_param.loc[all_param['correction'].isin(['no correction', 'delete and shift'])]

        g = sns.catplot(data=all_param,
                        y='A',
                        x='model',
                        hue='fitted species (A)',
                        col='artifact_speed',
                        row='stationary_speed',
                        sharey=True,
                        height=5,
                        aspect=1,
                        legend_out=True,
                        kind='boxen',
                        showfliers=False,
                        margin_titles=True)
        g.map_dataframe(sns.stripplot,
              y='A',
              x='model',
              hue="fitted species (A)",
              dodge=True,
              palette=sns.color_palette(['0.3']),
              size=4,
              jitter=0.2)
        # add hlines
        g.tight_layout()
        g.fig.suptitle('Prediction and "cut and shift" correction applied on simulated test data', y=1.03, size=20)
        #for i, axes in enumerate(g.axes.flat):
        #      _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=45)
        plt.setp(g.axes, # xlabel='model',
                 ylabel=r'relative fitted fraction size')
        # plt.setp(g.legend, title='Correlator and\nfit parameter')

        plot1_file = 'analysis1_0cd20_simulation_fraction-size'
        plt.savefig(f'{plot1_file}.pdf', bbox_inches='tight', dpi=300)
        os.system(f'pdf2svg {plot1_file}.pdf {plot1_file}.svg')
        os.system(f'rm {plot1_file}.pdf')
      #+END_SRC
      #+end_details
      #+RESULTS:

      [[file:./data/exp-220227-unet/jupyter/analysis1_0cd20_simulation_fraction-size.png]]

      #+BEGIN_SRC jupyter-python
        all_param.keys()
      #+END_SRC

      #+RESULTS:
      : Index(['name_of_plot', 'fitted species (txy)', 'fitted species (A)',
      :        'stdev(alpha2)', 'GN0', 'alpha2', 'xmax', 'stdev(A2)',
      :        'Triplet_species', 'Dimen', 'time of fit', 'above zero', 'cpm (kHz)',
      :        'N (mom)', 'stdev(txy2)', 'parent_uqid', 'stdev(offset)', 'N (FCS)',
      :        'stdev(A1)', 'correction', 'txy', 'Triplet_eq', 'stationary_speed',
      :        'master_file', 'stdev(txy1)', 'parent_name', 'artifact_speed',
      :        'stdev(GN0)', 'Diff_species', 'xmin', 'model', 'alpha1',
      :        'stdev(alpha1)', 'bri (kHz)', 'offset', 'Diff_eq', 'A'],
      :       dtype='object')

      #+BEGIN_SRC jupyter-python
        %cd ~/Programme/drmed-git
      #+END_SRC

*** Plot 3: all bioexps
    :PROPERTIES:
    :header-args:jupyter-python: :session /jpy:localhost#8888:137e524a-8134-4d8f-b24a-367acaf1bdd3 :pandoc t
    :END:

     #+BEGIN_SRC jupyter-python
       %cd ~/Programme/drmed-git
     #+END_SRC

     #+RESULTS:
     : /home/lex/Programme/drmed-git

     #+BEGIN_SRC jupyter-python
       import os
       import numpy as np
       import matplotlib.pyplot as plt
       import matplotlib.ticker as ticker
       import pandas as pd
       import seaborn as sns
       from pathlib import Path
       from pprint import pprint



       sns.set_theme(style="whitegrid", font_scale=2, palette='colorblind',
                     context='paper')

       model_ls = ['ff67be0b68e540a9a29a36a2d0c7a5be', '347669d050f344ad9fb9e480c814f727',
                   '714af8cd12c1441eac4ca980e8c20070', '34a6d207ac594035b1009c330fb67a65',
                   '484af471c61943fa90e5f78e78a229f0', '0cd2023eeaf745aca0d3e8ad5e1fc653',
                   'fe81d71c52404ed790b3a32051258da9', '19e3e786e1bc4e2b93856f5dc9de8216',
                   'c1204e3a8a1e4c40a35b5b7b1922d1ce']

       model_name_ls = [f'{s:.5}' for s in model_ls]

       pred_thresh = 0.5
     #+END_SRC

     #+RESULTS:

     #+BEGIN_SRC jupyter-python
       pprint(sns.plotting_context("paper"))
     #+END_SRC
**** plot bio example traces

     #+BEGIN_SRC jupyter-python
       corr_af488 = pd.read_csv(Path(output_path) / 'clean_nocorrection_3D-AR5_1spec_rawFitData.csv',
                                index_col=0, usecols=[0, 1, 3, 5], na_values=' ')
       fit_af488 = pd.read_csv(Path(output_path) / 'clean_nocorrection_3D-AR5_1spec_rawFitData.csv',
                               index_col=0, usecols=[0, 2, 4, 6], na_values=' ')
       param_af488 = pd.read_csv(Path(output_path) / 'clean_nocorrection_3D-AR5_1spec_outputParam.csv',
                                 index_col=0)

       preds_af488 = pd.read_csv(Path(output_path) / 'clean_subsample_preds.csv', index_col=0)
       predtraces_af488 = pd.read_csv(Path(output_path) / 'clean_subsample_predtraces.csv', index_col=0)
       traces_af488 = pd.read_csv(Path(output_path) / 'clean_subsample_traces.csv', index_col=0)
       corrtraces_af488 = pd.read_csv(Path(output_path) / 'clean_subsample_corrtraces.csv', index_col=0)

       corr_noc_af488luv = pd.read_csv(Path(output_path) / 'dirty_nocorrection_3D-AR5_2spec_rawFitData.csv',
                                index_col=0, usecols=[0, 1, 3, 5], na_values=' ')
       fit_noc_af488luv = pd.read_csv(Path(output_path) / 'dirty_nocorrection_3D-AR5_2spec_rawFitData.csv',
                               index_col=0, usecols=[0, 2, 4, 6], na_values=' ')
       param_noc_af488luv = pd.read_csv(Path(output_path) / 'dirty_nocorrection_3D-AR5_2spec_outputParam.csv',
                                     index_col=0)
       corr_cas_af488luv = pd.read_csv(Path(output_path) / 'dirty_cutandshift_3D-AR5_2spec_rawFitData.csv',
                                index_col=0, usecols=[0, 1, 3, 5], na_values=' ')
       fit_cas_af488luv = pd.read_csv(Path(output_path) / 'dirty_cutandshift_3D-AR5_2spec_rawFitData.csv',
                               index_col=0, usecols=[0, 2, 4, 6], na_values=' ')
       param_cas_af488luv = pd.read_csv(Path(output_path) / 'dirty_cutandshift_3D-AR5_2spec_outputParam.csv',
                                index_col=0)

       preds_af488luv = pd.read_csv(Path(output_path) / 'dirty_subsample_preds.csv', index_col=0)
       predtraces_af488luv = pd.read_csv(Path(output_path) / 'dirty_subsample_predtraces.csv', index_col=0)
       traces_af488luv = pd.read_csv(Path(output_path) / 'dirty_subsample_traces.csv', index_col=0)
       corrtraces_af488luv = pd.read_csv(Path(output_path) / 'dirty_subsample_corrtraces.csv', index_col=0)

       preds_hspex5 = pd.read_csv(Path(output_path) / 'HsPEX5EGFP_1-100001_3of250_preds.csv', index_col=0)
       predtraces_hspex5 = pd.read_csv(Path(output_path) / 'HsPEX5EGFP_1-100001_3of250_predtraces.csv', index_col=0)
       traces_hspex5 = pd.read_csv(Path(output_path) / 'HsPEX5EGFP_1-100001_3of250_traces.csv', index_col=0)
       corrtraces_hspex5 = pd.read_csv(Path(output_path) / 'HsPEX5EGFP_1-100001_3of250_corrtraces.csv', index_col=0)

       preds_tbpex5 = pd.read_csv(Path(output_path) / 'TbPEX5EGFP_1-10002_3of250_preds.csv', index_col=0)
       predtraces_tbpex5 = pd.read_csv(Path(output_path) / 'TbPEX5EGFP_1-10002_3of250_predtraces.csv', index_col=0)
       traces_tbpex5 = pd.read_csv(Path(output_path) / 'TbPEX5EGFP_1-10002_3of250_traces.csv', index_col=0)
       corrtraces_tbpex5 = pd.read_csv(Path(output_path) / 'TbPEX5EGFP_1-10002_3of250_corrtraces.csv', index_col=0)

       plot3_af488_corr = pd.read_csv(Path(output_path) / 'af488noc_af488luv-noc_af488luv-0cd20_af488luv-thr-2_2comp_rawFitData.csv',
                                       index_col=0, usecols=[0, 1, 3, 5, 7], na_values=' ')
       plot3_af488_param = pd.read_csv(Path(output_path) / 'af488noc_af488luv-noc_af488luv-0cd20_af488luv-thr-2_2comp_outputParam.csv',
                                       index_col=0)
       plot3_pex5_corr = pd.read_csv(Path(output_path) / 'hs-pex5-noc_tb-pex5-noc_tb-pex5-0cd20_tb-pex5-thr-7_triplet-0.04_1comp_rawFitData.csv',
                                      index_col=0, usecols=[0, 1, 3, 5, 7], na_values=' ')
       plot3_pex5_param = pd.read_csv(Path(output_path) / 'hs-pex5-noc_tb-pex5-noc_tb-pex5-0cd20_tb-pex5-thr-7_triplet-0.04_1comp_outputParam.csv',
                                       index_col=0)
     #+END_SRC

     #+RESULTS:

     #+BEGIN_SRC jupyter-python
       def plot_bio_traces(df, txt):
           for i, t in enumerate(df.items()):
               t = t[1]
               fig = plt.figure()
               ax = plt.subplot(111)
               ax.set_prop_cycle(color=[sns.color_palette()[0]])
               sns.lineplot(data=t, label=txt)
               plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)
               plt.setp(ax, xlabel=r'Time [$ms$]', ylabel=r'Intensity [a.u.]',
                 title='')
               plot1_file = f'data/exp-220316-publication1/jupyter/plot1_{txt}_{i}'.replace(
                   ' ', '_').replace('\n', '_').replace('(', '').replace(')', '').replace('"', '')
               plt.savefig(f'{plot1_file}.pdf', bbox_inches='tight', dpi=300)
               os.system(f'pdf2svg {plot1_file}.pdf {plot1_file}.svg')
               os.system(f'rm {plot1_file}.pdf')
           plt.close('all')

       def plot_bio_prediction_based_cut_and_shift_correction(df, txt):
           for i, t in enumerate(df.items()):
               t = t[1]
               fig = plt.figure()
               ax = plt.subplot(111)
               ax.set_prop_cycle(color=[sns.color_palette()[0]])
               sns.lineplot(data=t, label='trace')
               plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)
               plt.setp(ax, xlabel=r'Time [$ms$]', ylabel=r'Intensity [a.u.]',
                 title=txt)
               break
               plot1_file = f'{filename}_{txt}'.replace(' ', '_').replace(
                   '\n', '_').replace('"', '')
               plt.savefig(f'{plot1_file}.pdf', bbox_inches='tight', dpi=300)
               os.system(f'pdf2svg {plot1_file}.pdf {plot1_file}.svg')
               os.system(f'rm {plot1_file}.pdf')

       def plot_bio_cluster_prediction(df, txt):
           for i, col in enumerate(df.columns):
               fig = plt.figure()
               ax = plt.subplot(111)
               ax.set_prop_cycle(color=[sns.color_palette()[3]])
               sns.lineplot(data=df.loc[:, col],
                            label='prediction')
               plt.axhline(y=pred_thresh, xmin=0, xmax=1,
                           label='prediction\nthreshold',
                           color=sns.color_palette()[7], linestyle='--')
               plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)
               plt.setp(ax, xlabel=r'Time [$ms$]', ylabel=r'artifact probability',
                 title=txt, ylim=[0, 1])
               plot1_file = f'data/exp-220316-publication1/jupyter/plot1_{txt}_preds_{i}'.replace(
                   ' ', '_').replace('\n', '_').replace('(', '').replace(')', '')
               plt.savefig(f'{plot1_file}.pdf', bbox_inches='tight', dpi=300)
               os.system(f'pdf2svg {plot1_file}.pdf {plot1_file}.svg')
               os.system(f'rm {plot1_file}.pdf')
           plt.close('all')

       def plot_bio_prediction_based_segmentation(traces_df, pred_df, txt):
           for i, (t, p) in enumerate(zip(traces_df.items(), pred_df.items())):
               t = t[1]
               p = p[1] > pred_thresh
               p = p[:t.size]
               p_bool = t.max() * p
               p_invbool = t.max() * ~p

               fig = plt.figure()
               ax = plt.subplot(111)
               ax.set_prop_cycle(color=[sns.color_palette()[3]])
               sns.lineplot(data=p_bool, alpha=0.5)
               plt.fill_between(x=p_bool.index,
                                y1=p_bool,
                                y2=0, alpha=0.5, label='prediction:\npeak artifacts')

               ax.set_prop_cycle(color=[sns.color_palette()[2]])
               plt.fill_between(x=p_invbool.index,
                                y1=p_invbool,
                                y2=0, alpha=0.5, label='\nprediction\nno artifacts')
               ax.set_prop_cycle(color=[sns.color_palette()[0]])
               sns.lineplot(data=t, label=txt)
               plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)
               plt.setp(ax, xlabel=r'Time [$ms$]', ylabel=r'Intensity [a.u.]',
                 title='')
               plot1_file = f'data/exp-220316-publication1/jupyter/plot1_{txt}_seg_{i}'.replace(
                   ' ', '_').replace('\n', '_').replace('(', '').replace(')', '')
               plt.savefig(f'{plot1_file}.pdf', bbox_inches='tight', dpi=300)
               os.system(f'pdf2svg {plot1_file}.pdf {plot1_file}.svg')
               os.system(f'rm {plot1_file}.pdf')
           plt.close('all')

       def plot_bio_clean_corr_and_fit(corr_df, fit_df, param_df, txt):
           for i, (c, f, param) in enumerate(zip(corr_df.items(), fit_df.items(), param_df.T.items())):
               c = c[1]
               f = f[1]
               txy = param[1].loc['txy1']
               f_xmin = f.dropna().index[0]
               f_xmax = f.dropna().index[-1]
               xlims = [f_xmin - 0.5*f_xmin, f_xmax + f_xmax]
               ylims = [f.dropna().iloc[-1] - 0.01, f.dropna().iloc[0] + 0.01]

               fig = plt.figure()
               ax = plt.subplot(111)
               ax.set_prop_cycle(color=[sns.color_palette()[0]])
               plt.semilogx(c.index, c, '.', label='Correlation')
               ax.set_prop_cycle(color=[sns.color_palette()[2]])
               plt.semilogx(f.index, f, '-', lw=3.0, label='Fit\n'+rf'$\tau_D=${txy:.3f}')
               plt.axvline(x=txy, color=sns.color_palette()[2])
               plt.setp(ax, xlim=xlims, ylim=ylims, title=txt,
                        xlabel=r'$\tau$ (ms)', ylabel=r'Correlation G($\tau$)')
               ax.grid(False)
               plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)
               plot1_file = f'data/exp-220316-publication1/jupyter/plot1_{txt}_{i}'.replace(
                   ' ', '_').replace('\n', '_').replace('(', '').replace(')', '')
               plt.savefig(f'{plot1_file}.pdf', bbox_inches='tight', dpi=300)
               os.system(f'pdf2svg {plot1_file}.pdf {plot1_file}.svg')
               os.system(f'rm {plot1_file}.pdf')
           plt.close('all')

       def plot_bio_corrs_and_fits(corr_df1, corr_df2, fit_df1, fit_df2, param_df1, param_df2, txt):
           for i, (c1, c2, f1, f2, param1, param2) in enumerate(zip(corr_df1.items(), corr_df2.items(),
                      fit_df1.items(), fit_df2.items(), param_df1.T.items(), param_df2.T.items())):
               c1, c2 = c1[1], c2[1]
               f1, f2 = f1[1], f2[1]
               txy11, txy12 = param1[1].loc['txy1'], param2[1].loc['txy1']
               txy21, txy22 = param1[1].loc['txy2'], param2[1].loc['txy2']
               f_xmin = np.min([f1.dropna().index[0], f2.dropna().index[0]])
               f_xmax = np.max([f1.dropna().index[-1], f2.dropna().index[-1]])
               f_ymin1, f_ymin2 = f1.dropna().iloc[-1], f2.dropna().iloc[-1]
               f_ymax1, f_ymax2 = f1.dropna().iloc[0], f2.dropna().iloc[0]
               xlims = [f_xmin - 0.5*f_xmin, f_xmax + f_xmax]
               ylims1 = [f_ymin1 - np.abs(2*f_ymin1), f_ymax1 + 0.1*f_ymax1]
               ylims2= [f_ymin2 - np.abs(2*f_ymin2), f_ymax2 + 0.1*f_ymax2]
               fig = plt.figure()
               ax1 = plt.subplot(111)
               ax1.set_prop_cycle(color=[sns.color_palette()[0]])
               p1, = ax1.semilogx(c1.index, c1, '.', label='Correlation\n(no correction)')
               ax1.set_prop_cycle(color=[sns.color_palette()[2]])
               f1_label = 'Fit (no correction)\n'+r'$\tau_{D,1}=$'+f'{txy11:.3f}\n'+r'$\tau_{D,2}=$'+f'{txy21:.3f}'
               p2, = ax1.semilogx(f1.index, f1, '-', lw=3.0, label=f1_label)

               ax2 = ax1.twinx()
               ax2.set_prop_cycle(color=[sns.color_palette()[1]])
               p3, = ax2.semilogx(c2.index, c2, '.', label='Correlation\n(cut and shift)')
               ax2.set_prop_cycle(color=[sns.color_palette()[3]])
               f2_label = 'Fit (cut and shift)\n'+r'$\tau_{D,1}=$'+f'{txy12:.3f}\n'+r'$\tau_{D,2}=$'+f'{txy22:.3f}'
               p4, = ax2.semilogx(f2.index, f2, '-', lw=3.0, label=f2_label)
               plt.axvline(x=txy11, color=sns.color_palette()[2])
               plt.axvline(x=txy12, color=sns.color_palette()[3])
               plt.axvline(x=txy21, color=sns.color_palette()[2])
               plt.axvline(x=txy22, color=sns.color_palette()[3])

               plt.setp(ax1, xlim=xlims, ylim=ylims1, title=txt,
                        xlabel=r'$\tau$ (ms)', ylabel=r'Normalized $G(\tau)$',
                        yticklabels=[], yticks=[])
               ax1.grid(False)
               plt.setp(ax2, xlim=xlims, ylim=ylims2, title=txt,
                        xlabel=r'$\tau$ (ms)', yticklabels=[], yticks=[])
               ax2.grid(False)
               plt.legend(handles=[p1, p3, p2, p4], bbox_to_anchor=(1.02, 1),
                          loc='upper left', borderaxespad=0)
               plot1_file = f'data/exp-220316-publication1/jupyter/plot1_{txt}_{i}'.replace(
                   ' ', '_').replace('\n', '_').replace('(', '').replace(')', '')
               plt.savefig(f'{plot1_file}.pdf', bbox_inches='tight', dpi=300)
               os.system(f'pdf2svg {plot1_file}.pdf {plot1_file}.svg')
               os.system(f'rm {plot1_file}.pdf')
           plt.close('all')
     #+END_SRC

     #+RESULTS:

     #+BEGIN_SRC jupyter-python
       plot_bio_traces(traces_af488, txt='AlexaFluor488\n(n=1,\nno artifacts,\nno correction)')
       plot_bio_traces(traces_af488luv, txt='AF488 + DiO\nLUVs (n=1,\npeak artifacts,\nno correction)')
       plot_bio_traces(traces_hspex5, txt='Hs-PEX5-eGFP\n(n=1,\nno artifacts,\nno correction)')
       plot_bio_traces(traces_tbpex5, txt='Tb-PEX5-eGFP\n(n=1,\npeak artifacts,\nno correction)')
       plot_bio_traces(corrtraces_af488, txt='AlexaFluor488\n(n=1,\nno artifacts,\nautom. corr.)')
       plot_bio_traces(corrtraces_af488luv, txt='AF488 + DiO\nLUVs (n=1,\npeak artifacts,\nautom. corr.)')
       plot_bio_traces(corrtraces_hspex5, txt='Hs-PEX5-eGFP\n(n=1,\nno artifacts,\nautom. corr.)')
       plot_bio_traces(corrtraces_tbpex5, txt='Tb-PEX5-eGFP\n(n=1,\npeak artifacts,\nautom. corr.)')
     #+END_SRC

     #+RESULTS:


     file:./data/exp-220316-publication1/jupyter/plot1_AlexaFluor_488_in_solution_clean_0.svg
     file:./data/exp-220316-publication1/jupyter/plot1_AlexaFluor_488_in_solution_clean_1.svg
     file:./data/exp-220316-publication1/jupyter/plot1_AlexaFluor_488_in_solution_clean_2.svg
     file:./data/exp-220316-publication1/jupyter/plot1_AlexaFluor_488_+_DiO_LUVs_in_solution_dirty_0.svg
     file:./data/exp-220316-publication1/jupyter/plot1_AlexaFluor_488_+_DiO_LUVs_in_solution_dirty_1.svg
     file:./data/exp-220316-publication1/jupyter/plot1_AlexaFluor_488_+_DiO_LUVs_in_solution_dirty_2.svg


     #+BEGIN_SRC jupyter-python
       plot_bio_cluster_prediction(preds_af488, txt='AlexaFluor 488 in solution (clean):\nLUV prediction')
       plot_bio_cluster_prediction(preds_af488luv, txt='AlexaFluor 488 + DiO LUVs in solution (dirty):\nLUV prediction')
     #+END_SRC

     #+RESULTS:

     file:./data/exp-220316-publication1/jupyter/plot1_AlexaFluor_488_+_DiO_LUVs_in_solution_dirty:_LUV_prediction_preds_0.svg
     file:./data/exp-220316-publication1/jupyter/plot1_AlexaFluor_488_+_DiO_LUVs_in_solution_dirty:_LUV_prediction_preds_1.svg
     file:./data/exp-220316-publication1/jupyter/plot1_AlexaFluor_488_+_DiO_LUVs_in_solution_dirty:_LUV_prediction_preds_2.svg
     [[file:./data/exp-220316-publication1/jupyter/plot1_AlexaFluor_488_in_solution_clean:_LUV_prediction_preds_0.svg]]
     [[file:./data/exp-220316-publication1/jupyter/plot1_AlexaFluor_488_in_solution_clean:_LUV_prediction_preds_1.svg]]
     [[file:./data/exp-220316-publication1/jupyter/plot1_AlexaFluor_488_in_solution_clean:_LUV_prediction_preds_2.svg]]


     #+BEGIN_SRC jupyter-python
       traces_hspex5
     #+END_SRC

     #+RESULTS:
     :RESULTS:

     :END:

     #+BEGIN_SRC jupyter-python

       plot_bio_prediction_based_segmentation(
           traces_af488luv, preds_af488luv,
           txt='\nAF488 + DiO\nLUVs (n=1,\npeak artifacts,\nno correction)')
       plot_bio_prediction_based_segmentation(
           traces_af488, preds_af488,
           txt='\nAlexaFluor488\n(n=1,\nno artifacts,\nno correction)')
       plot_bio_prediction_based_segmentation(
           traces_hspex5, preds_hspex5,
           txt='\nHs-PEX5-eGFP\n(n=1,\nno artifacts,\nno correction)')
       plot_bio_prediction_based_segmentation(
           traces_tbpex5, preds_tbpex5,
           txt='\nTb-PEX5-eGFP\n(n=1,\npeak artifacts,\nno correction)')
     #+END_SRC

     #+RESULTS:



     file:./data/exp-220316-publication1/jupyter/plot1_AlexaFluor_488_+_DiO_LUVs_in_solution_dirty:_prediction-based_segmentation_0.svg
     file:./data/exp-220316-publication1/jupyter/plot1_AlexaFluor_488_+_DiO_LUVs_in_solution_dirty:_prediction-based_segmentation_1.svg
     file:./data/exp-220316-publication1/jupyter/plot1_AlexaFluor_488_+_DiO_LUVs_in_solution_dirty:_prediction-based_segmentation_2.svg
     [[file:./data/exp-220316-publication1/jupyter/plot1_AlexaFluor_488_in_solution_clean:_prediction-based_segmentation_0.svg]]
     [[file:./data/exp-220316-publication1/jupyter/plot1_AlexaFluor_488_in_solution_clean:_prediction-based_segmentation_1.svg]]
     [[file:./data/exp-220316-publication1/jupyter/plot1_AlexaFluor_488_in_solution_clean:_prediction-based_segmentation_2.svg]]


     #+BEGIN_SRC jupyter-python
       plot_bio_traces(corrtraces_af488,
                       txt='AlexaFluor 488 (no artifacts):\nprediction-based "cut and shift" correction')
       plot_bio_traces(corrtraces_af488luv,
                       txt='AlexaFluor 488 + DiO LUVs (peak artifacts):\nprediction-based "cut and shift" correction')
       plot_bio_traces(corrtraces_hspex5,
                       txt='Homo Sapiens-PEX5-eGFP (no artifacts):\nprediction-based "cut and shift" correction')
       plot_bio_traces(corrtraces_tbpex5,
                       txt='Trypanosoma brucei-PEX5-eGFP (peak artifacts):\nprediction-based "cut and shift" correction')
     #+END_SRC

     #+RESULTS:

     file:./data/exp-220316-publication1/jupyter/plot1_AlexaFluor_488_+_DiO_LUVs_in_solution_dirty:_prediction-based_cut_and_shift_correction_0.svg
     file:./data/exp-220316-publication1/jupyter/plot1_AlexaFluor_488_+_DiO_LUVs_in_solution_dirty:_prediction-based_cut_and_shift_correction_1.svg
     file:./data/exp-220316-publication1/jupyter/plot1_AlexaFluor_488_+_DiO_LUVs_in_solution_dirty:_prediction-based_cut_and_shift_correction_2.svg
     [[file:./data/exp-220316-publication1/jupyter/plot1_AlexaFluor_488_in_solution_clean:_prediction-based_cut_and_shift_correction_0.svg]]
     [[file:./data/exp-220316-publication1/jupyter/plot1_AlexaFluor_488_in_solution_clean:_prediction-based_cut_and_shift_correction_1.svg]]
     [[file:./data/exp-220316-publication1/jupyter/plot1_AlexaFluor_488_in_solution_clean:_prediction-based_cut_and_shift_correction_2.svg]]


     #+BEGIN_SRC jupyter-python
       plot_bio_clean_corr_and_fit(corr_af488, fit_af488, param_af488,
                                   txt='AlexaFluor 488 in solution (clean):\nCorrelation and 1-component Fit')
       plot_bio_corrs_and_fits(corr_noc_af488luv, corr_cas_af488luv, fit_noc_af488luv, fit_cas_af488luv, param_noc_af488luv, param_cas_af488luv,
                               txt='AlexaFluor 488 + DiO LUVs in solution (dirty):\nCorrelations and 2-component Fits')

     #+END_SRC

     #+RESULTS:

     file:./data/exp-220316-publication1/jupyter/plot1_AlexaFluor_488_+_DiO_LUVs_in_solution_dirty:_Correlations_and_2-component_Fits_0.svg
     file:./data/exp-220316-publication1/jupyter/plot1_AlexaFluor_488_+_DiO_LUVs_in_solution_dirty:_Correlations_and_2-component_Fits_1.svg
     file:./data/exp-220316-publication1/jupyter/plot1_AlexaFluor_488_+_DiO_LUVs_in_solution_dirty:_Correlations_and_2-component_Fits_2.svg
     [[file:./data/exp-220316-publication1/jupyter/plot1_AlexaFluor_488_in_solution_clean:_Correlation_and_1-component_Fit_0.svg]]
     [[file:./data/exp-220316-publication1/jupyter/plot1_AlexaFluor_488_in_solution_clean:_Correlation_and_1-component_Fit_1.svg]]
     [[file:./data/exp-220316-publication1/jupyter/plot1_AlexaFluor_488_in_solution_clean:_Correlation_and_1-component_Fit_2.svg]]

     #+BEGIN_SRC jupyter-python
       def plot3_bio_corrs(corr_df, param_df, bio):
           if bio == 'af488':
               legend = ['AlexaFluor488\n(n=424,\nno correction)',
                         '\nAF488 + DiO\nLUVs(n=440,\nno correction)',
                         '\nAF488 + DiO\nLUVs(n=440,\nautom. corr.)',
                         '\nAF488 + DiO\nLUVs(n=440,\nmanual corr.)']
           elif bio == 'pex5':
               legend = ['Hs-PEX5-eGFP\n(n=250,\nno correction)',
                         '\nTb-PEX5-eGFP\n(n=250,\nno correction)',
                         '\nTb-PEX5-eGFP\n(n=250,\nautom. corr.)',
                         '\nTb-PEX5-eGFP\n(n=250,\nmanual corr.)']
           sns.set_theme(style="whitegrid", font_scale=3.5, palette='colorblind',
                         context='paper')

           xmin = min(param_df['xmin'])
           xmax = max(param_df['xmax'])
           ymin = min(param_df['offset'])
           ymax = max(param_df['GN0'])
           xlims = [xmin - 0.5*xmin, xmax + xmax]
           ylims = [ymin - np.abs(5*ymin), ymax + 0.1*ymax]
           fig = plt.figure(figsize=(16,9))
           ax1 = plt.subplot(111)
           lines = sns.lineplot(data=corr_df, lw=6, ls=':', legend=False)
           plt.legend(legend, loc = 2, bbox_to_anchor = (1,1))
           plt.setp(ax1, xlim=xlims, ylim=ylims, xscale='log',
                    xlabel=r'$\tau [ms]$', ylabel=r'Avg correlation $G(\tau)$ [a.u.]')
           plt.tight_layout()

           plot_file = f'plot3_bioexps_{bio}_avg-correlations'
           savefig = f'./data/exp-220316-publication1/jupyter/{plot_file}'
           plt.savefig(f'{savefig}.pdf', bbox_inches='tight', dpi=300)
           os.system(f'pdf2svg {savefig}.pdf {savefig}.svg')
           # os.system(f'rm {plot1_file}.pdf')
           plt.close('all')
       plot3_bio_corrs(plot3_af488_corr, plot3_af488_param, 'af488')
       plot3_bio_corrs(plot3_pex5_corr, plot3_pex5_param, 'pex5')
     #+END_SRC

     #+RESULTS:

**** plot bio fit results
   - now let's load all the fit data from biological experiments and extract the
     transit times
     #+BEGIN_SRC jupyter-python
       af488_path1 = Path('data/exp-220227-unet/2022-05-22_experimental-af488')
       af488_path2 = Path('data/exp-220316-publication1/220726_bioexps/af488')
       af488_path3 = Path('data/exp-220316-publication1/220726_bioexps/af488+luvs/')
       pex5_path1 = Path('data/exp-220227-unet/2022-06-02_experimental-pex5/')
       pex5_path2 = Path('data/exp-220316-publication1/220726_bioexps/Hs-PEX5-eGFP')
       pex5_path3 = Path('data/exp-220316-publication1/220726_bioexps/Tb-PEX5-eGFP/')

       # af488 only (clean), only 1comp fits
       af488_noc_1comp_path = af488_path1 / 'clean-all-results/clean_no-correction_1comp_outputParam.csv'
       af488_0cd20_1comp_path = af488_path1 / 'clean-all-results/clean_0cd20_1comp_outputParam.csv'
       af488_34a6d_1comp_path = af488_path1 / 'clean-all-results/clean_34a6d_1comp_outputParam.csv'
       af488_fe81d_1comp_path = af488_path1 / 'clean-all-results/clean_fe81d_1comp_outputParam.csv'
       af488_ff67b_1comp_path = af488_path1 / 'clean-all-results/clean_ff67b_1comp_outputParam.csv'
       af488_0dot8_1comp_path = af488_path2 / 'af488-all-results/af488_thresh-0.8_1comp_outputParam.csv'
       af488_1_1comp_path = af488_path2 / 'af488-all-results/af488_thresh-1_1comp_outputParam.csv'
       af488_1dot5_1comp_path = af488_path2 / 'af488-all-results/af488_thresh-1.5_1comp_outputParam.csv'
       af488_2_1comp_path = af488_path2 / 'af488-all-results/af488_thresh-2_1comp_outputParam.csv'
       af488_2dot5_1comp_path = af488_path2 / 'af488-all-results/af488_thresh-2.5_1comp_outputParam.csv'

       # af488 + LUVs (dirty), 1comp and 2comp
       af488luv_noc_1comp_path = af488_path1 / 'dirty-all-results/dirty_no-correction_1comp_outputParam.csv'
       af488luv_noc_2comp_path = af488_path1 / 'dirty-all-results/dirty_no-correction_2comp_outputParam.csv'
       af488luv_0cd20_1comp_path = af488_path1 / 'dirty-all-results/dirty_0cd20_1comp_outputParam.csv'
       af488luv_0cd20_2comp_path = af488_path1 / 'dirty-all-results/dirty_0cd20_2comp_outputParam.csv'
       af488luv_34a6d_1comp_path = af488_path1 / 'dirty-all-results/dirty_34a6d_1comp_outputParam.csv'
       af488luv_34a6d_2comp_path = af488_path1 / 'dirty-all-results/dirty_34a6d_2comp_outputParam.csv'
       af488luv_fe81d_1comp_path = af488_path1 / 'dirty-all-results/dirty_fe81d_1comp_outputParam.csv'
       af488luv_fe81d_2comp_path = af488_path1 / 'dirty-all-results/dirty_fe81d_2comp_outputParam.csv'
       af488luv_ff67b_1comp_path = af488_path1 / 'dirty-all-results/dirty_ff67b_1comp_outputParam.csv'
       af488luv_ff67b_2comp_path = af488_path1 / 'dirty-all-results/dirty_ff67b_2comp_outputParam.csv'
       af488luv_0dot8_1comp_path = af488_path3 / 'af488+luvs-all-results/af488+luvs_thresh-0.8_1comp_outputParam.csv'
       af488luv_0dot8_2comp_path = af488_path3 / 'af488+luvs-all-results/af488+luvs_thresh-0.8_2comp_outputParam.csv'
       af488luv_1_1comp_path = af488_path3 / 'af488+luvs-all-results/af488+luvs_thresh-1_1comp_outputParam.csv'
       af488luv_1_2comp_path = af488_path3 / 'af488+luvs-all-results/af488+luvs_thresh-1_2comp_outputParam.csv'
       af488luv_1dot5_1comp_path = af488_path3 / 'af488+luvs-all-results/af488+luvs_thresh-1.5_1comp_outputParam.csv'
       af488luv_1dot5_2comp_path = af488_path3 / 'af488+luvs-all-results/af488+luvs_thresh-1.5_2comp_outputParam.csv'
       af488luv_2_1comp_path = af488_path3 / 'af488+luvs-all-results/af488+luvs_thresh-2_1comp_outputParam.csv'
       af488luv_2_2comp_path = af488_path3 / 'af488+luvs-all-results/af488+luvs_thresh-2_2comp_outputParam.csv'
       af488luv_2dot5_1comp_path = af488_path3 / 'af488+luvs-all-results/af488+luvs_thresh-2.5_1comp_outputParam.csv'
       af488luv_2dot5_2comp_path = af488_path3 / 'af488+luvs-all-results/af488+luvs_thresh-2.5_2comp_outputParam.csv'

       # Hs-PEX5-eGFP (clean), only 1comp fits, with triplets fixed to 0.04ms (triplet fraction floating)
       # see https://www.sciencedirect.com/science/article/pii/S266707472200012X#bib35 for 0.04 reference
       # Tb-PEX5-eGFP (dirty), 1comp and 2comp fits, with triplets fixed to 0.04
       hs_noc_1comp_path = pex5_path1 / 'clean-all-results/Hs-PEX5-eGFP_no-correction_triplet-0.04_1comp_outputParam.csv'
       hs_0cd20_1comp_path = pex5_path1 / 'clean-all-results/Hs-PEX5-eGFP_0cd20_triplet-0.04_1comp_outputParam.csv'
       hs_fe81d_1comp_path = pex5_path1 / 'clean-all-results/Hs-PEX5-eGFP_fe81d_triplet-0.04_1comp_outputParam.csv'
       hs_ff67b_1comp_path = pex5_path1 / 'clean-all-results/Hs-PEX5-eGFP_ff67b_triplet-0.04_1comp_outputParam.csv'
       hs_5_1comp_path = pex5_path2 / 'Hs-PEX5-eGFP-all-results/Hs-PEX5-eGFP_thresh-5_triplet-0.04_1comp_outputParam.csv'
       hs_7_1comp_path = pex5_path2 / 'Hs-PEX5-eGFP-all-results/Hs-PEX5-eGFP_thresh-7_triplet-0.04_1comp_outputParam.csv'
       hs_10_1comp_path = pex5_path2 / 'Hs-PEX5-eGFP-all-results/Hs-PEX5-eGFP_thresh-10_triplet-0.04_1comp_outputParam.csv'


       tb_noc_1comp_path = pex5_path1 / 'dirty-all-results/Tb-PEX5-eGFP_no-correction_triplet-0.04_1comp_outputParam.csv'
       tb_noc_2comp_path = pex5_path1 / 'dirty-all-results/Tb-PEX5-eGFP_no-correction_triplet-0.04_2comp_outputParam.csv'
       tb_0cd20_1comp_path = pex5_path1 / 'dirty-all-results/Tb-PEX5-eGFP_0cd20_triplet-0.04_1comp_outputParam.csv'
       tb_0cd20_2comp_path = pex5_path1 / 'dirty-all-results/Tb-PEX5-eGFP_0cd20_triplet-0.04_2comp_outputParam.csv'
       tb_fe81d_1comp_path = pex5_path1 / 'dirty-all-results/Tb-PEX5-eGFP_fe81d_triplet-0.04_1comp_outputParam.csv'
       tb_fe81d_2comp_path = pex5_path1 / 'dirty-all-results/Tb-PEX5-eGFP_fe81d_triplet-0.04_2comp_outputParam.csv'
       tb_ff67b_1comp_path = pex5_path1 / 'dirty-all-results/Tb-PEX5-eGFP_ff67b_triplet-0.04_1comp_outputParam.csv'
       tb_ff67b_2comp_path = pex5_path1 / 'dirty-all-results/Tb-PEX5-eGFP_ff67b_triplet-0.04_2comp_outputParam.csv'
       tb_5_1comp_path = pex5_path3 / 'Tb-PEX5-eGFP-all-results/Tb-PEX5-eGFP_thresh-5_triplet-0.04_1comp_outputParam.csv'
       tb_5_2comp_path = pex5_path3 / 'Tb-PEX5-eGFP-all-results/Tb-PEX5-eGFP_thresh-5_triplet-0.04_2comp_outputParam.csv'
       tb_7_1comp_path = pex5_path3 / 'Tb-PEX5-eGFP-all-results/Tb-PEX5-eGFP_thresh-7_triplet-0.04_1comp_outputParam.csv'
       tb_7_2comp_path = pex5_path3 / 'Tb-PEX5-eGFP-all-results/Tb-PEX5-eGFP_thresh-7_triplet-0.04_2comp_outputParam.csv'
       tb_10_1comp_path = pex5_path3 / 'Tb-PEX5-eGFP-all-results/Tb-PEX5-eGFP_thresh-10_triplet-0.04_1comp_outputParam.csv'
       tb_10_2comp_path = pex5_path3 / 'Tb-PEX5-eGFP-all-results/Tb-PEX5-eGFP_thresh-10_triplet-0.04_2comp_outputParam.csv'

       # read af488 only (clean)
       af488_noc_1comp = pd.read_csv(af488_noc_1comp_path, sep=',').assign(bio=424*['af488',], processing=424*['Baseline control:\nno correction',])
       af488_0cd20_1comp = pd.read_csv(af488_0cd20_1comp_path, sep=',').assign(bio=424*['af488',], processing=424*['Automatic U-Net pred.\n+ cut and shift corr.:\nscaler=quantile transf.,\nlarge model (200MB)',])
       af488_34a6d_1comp = pd.read_csv(af488_34a6d_1comp_path, sep=',').assign(bio=424*['af488',], processing=424*['Automatic U-Net pred.\n+ cut and shift corr.:\nscaler=l2,\nsmall model (7MB)',])
       af488_fe81d_1comp = pd.read_csv(af488_fe81d_1comp_path, sep=',').assign(bio=424*['af488',], processing=424*['Automatic U-Net pred.\n+ cut and shift corr.:\nscaler=standard,\nlarge model (186MB)',])
       af488_ff67b_1comp = pd.read_csv(af488_ff67b_1comp_path, sep=',').assign(bio=424*['af488',], processing=424*['Automatic U-Net pred.\n+ cut and shift corr.:\nscaler=minmax,\nsmall model (14MB)',])
       af488_0dot8_1comp = pd.read_csv(af488_0dot8_1comp_path, sep=',').assign(bio=420*['af488',], processing=420*['Manual thresh. pred.\n+ cut and shift corr.:\nscaler=robust\nthreshold=0.8',])
       af488_1_1comp = pd.read_csv(af488_1_1comp_path, sep=',').assign(bio=424*['af488',], processing=424*['Manual thresh. pred.\n+ cut and shift corr.:\nscaler=robust\nthreshold=1',])
       af488_1dot5_1comp = pd.read_csv(af488_1dot5_1comp_path, sep=',').assign(bio=422*['af488',], processing=422*['Manual thresh. pred.\n+ cut and shift corr.:\nscaler=robust\nthreshold=1.5',])
       af488_2_1comp = pd.read_csv(af488_2_1comp_path, sep=',').assign(bio=424*['af488',], processing=424*['Manual thresh. pred.\n+ cut and shift corr.:\nscaler=robust\nthreshold=2',])
       af488_2dot5_1comp = pd.read_csv(af488_2dot5_1comp_path, sep=',').assign(bio=424*['af488',], processing=424*['Manual thresh. pred.\n+ cut and shift corr.:\nscaler=robust\nthreshold=2.5',])

       # read af488 + LUVs (dirty)p
       af488luv_noc_1comp = pd.read_csv(af488luv_noc_1comp_path, sep=',').assign(bio=440*['af488luv',], processing=440*['Baseline control:\nno correction',])
       af488luv_noc_2comp = pd.read_csv(af488luv_noc_2comp_path, sep=',').assign(bio=440*['af488luv',], processing=440*['Baseline control:\nno correction',])
       af488luv_0cd20_1comp = pd.read_csv(af488luv_0cd20_1comp_path, sep=',').assign(bio=440*['af488luv',], processing=440*['Automatic U-Net pred.\n+ cut and shift corr.:\nscaler=quantile transf.,\nlarge model (200MB)',])
       af488luv_0cd20_2comp = pd.read_csv(af488luv_0cd20_2comp_path, sep=',').assign(bio=440*['af488luv',], processing=440*['Automatic U-Net pred.\n+ cut and shift corr.:\nscaler=quantile transf.,\nlarge model (200MB)',])
       af488luv_34a6d_1comp = pd.read_csv(af488luv_34a6d_1comp_path, sep=',').assign(bio=440*['af488luv',], processing=440*['Automatic U-Net pred.\n+ cut and shift corr.:\nscaler=l2,\nsmall model (7MB)'])
       af488luv_34a6d_2comp = pd.read_csv(af488luv_34a6d_2comp_path, sep=',').assign(bio=440*['af488luv',], processing=440*['Automatic U-Net pred.\n+ cut and shift corr.:\nscaler=l2,\nsmall model (7MB)'])
       af488luv_fe81d_1comp = pd.read_csv(af488luv_fe81d_1comp_path, sep=',').assign(bio=440*['af488luv',], processing=440*['Automatic U-Net pred.\n+ cut and shift corr.:\nscaler=standard,\nlarge model (186MB)',])
       af488luv_fe81d_2comp = pd.read_csv(af488luv_fe81d_2comp_path, sep=',').assign(bio=440*['af488luv',], processing=440*['Automatic U-Net pred.\n+ cut and shift corr.:\nscaler=standard,\nlarge model (186MB)',])
       af488luv_ff67b_1comp = pd.read_csv(af488luv_ff67b_1comp_path, sep=',').assign(bio=440*['af488luv',], processing=440*['Automatic U-Net pred.\n+ cut and shift corr.:\nscaler=minmax,\nsmall model (14MB)',])
       af488luv_ff67b_2comp = pd.read_csv(af488luv_ff67b_2comp_path, sep=',').assign(bio=440*['af488luv',], processing=440*['Automatic U-Net pred.\n+ cut and shift corr.:\nscaler=minmax,\nsmall model (14MB)',])
       af488luv_0dot8_1comp = pd.read_csv(af488luv_0dot8_1comp_path, sep=',').assign(bio=440*['af488luv',], processing=440*['Manual thresh. pred.\n+ cut and shift corr.:\nscaler=robust\nthreshold=0.8',])
       af488luv_0dot8_2comp = pd.read_csv(af488luv_0dot8_2comp_path, sep=',').assign(bio=440*['af488luv',], processing=440*['Manual thresh. pred.\n+ cut and shift corr.:\nscaler=robust\nthreshold=0.8',])
       af488luv_1_1comp = pd.read_csv(af488luv_1_1comp_path, sep=',').assign(bio=440*['af488luv',], processing=440*['Manual thresh. pred.\n+ cut and shift corr.:\nscaler=robust\nthreshold=1',])
       af488luv_1_2comp = pd.read_csv(af488luv_1_2comp_path, sep=',').assign(bio=440*['af488luv',], processing=440*['Manual thresh. pred.\n+ cut and shift corr.:\nscaler=robust\nthreshold=1',])
       af488luv_1dot5_1comp = pd.read_csv(af488luv_1dot5_1comp_path, sep=',').assign(bio=440*['af488luv',], processing=440*['Manual thresh. pred.\n+ cut and shift corr.:\nscaler=robust\nthreshold=1.5',])
       af488luv_1dot5_2comp = pd.read_csv(af488luv_1dot5_2comp_path, sep=',').assign(bio=440*['af488luv',], processing=440*['Manual thresh. pred.\n+ cut and shift corr.:\nscaler=robust\nthreshold=1.5',])
       af488luv_2_1comp = pd.read_csv(af488luv_2_1comp_path, sep=',').assign(bio=440*['af488luv',], processing=440*['Manual thresh. pred.\n+ cut and shift corr.:\nscaler=robust\nthreshold=2',])
       af488luv_2_2comp = pd.read_csv(af488luv_2_2comp_path, sep=',').assign(bio=440*['af488luv',], processing=440*['Manual thresh. pred.\n+ cut and shift corr.:\nscaler=robust\nthreshold=2',])
       af488luv_2dot5_1comp = pd.read_csv(af488luv_2dot5_1comp_path, sep=',').assign(bio=431*['af488luv',], processing=431*['Manual thresh. pred.\n+ cut and shift corr.:\nscaler=robust\nthreshold=2.5',])
       af488luv_2dot5_2comp = pd.read_csv(af488luv_2dot5_2comp_path, sep=',').assign(bio=431*['af488luv',], processing=431*['Manual thresh. pred.\n+ cut and shift corr.:\nscaler=robust\nthreshold=2.5',])

       # read Hs-PEX5-eGFP (clean)
       hs_noc_1comp = pd.read_csv(hs_noc_1comp_path, sep=',').assign(bio=250*['hs',], processing=250*['Baseline control:\nno correction',])
       hs_0cd20_1comp = pd.read_csv(hs_0cd20_1comp_path, sep=',').assign(bio=250*['hs',], processing=250*['Automatic U-Net pred.\n+ cut and shift corr.:\nscaler=quantile transf.,\nlarge model (200MB)',])
       hs_fe81d_1comp = pd.read_csv(hs_fe81d_1comp_path, sep=',').assign(bio=250*['hs',], processing=250*['Automatic U-Net pred.\n+ cut and shift corr.:\nscaler=standard,\nlarge model (186MB)',])
       hs_ff67b_1comp = pd.read_csv(hs_ff67b_1comp_path, sep=',').assign(bio=250*['hs',], processing=250*['Automatic U-Net pred.\n+ cut and shift corr.:\nscaler=minmax,\nsmall model (14MB)',])
       hs_5_1comp = pd.read_csv(hs_5_1comp_path, sep=',').assign(bio=250*['hs',], processing=250*['Manual thresh. pred.\n+ cut and shift corr.:\nscaler=robust\nthreshold=5',])
       hs_7_1comp = pd.read_csv(hs_7_1comp_path, sep=',').assign(bio=250*['hs',], processing=250*['Manual thresh. pred.\n+ cut and shift corr.:\nscaler=robust\nthreshold=7',])
       hs_10_1comp = pd.read_csv(hs_10_1comp_path, sep=',').assign(bio=250*['hs',], processing=250*['Manual thresh. pred.\n+ cut and shift corr.:\nscaler=robust\nthreshold=10',])

       tb_noc_1comp = pd.read_csv(tb_noc_1comp_path, sep=',').assign(bio=250*['tb',], processing=250*['Baseline control:\nno correction',])
       tb_noc_2comp = pd.read_csv(tb_noc_2comp_path, sep=',').assign(bio=250*['tb',], processing=250*['Baseline control:\nno correction',])
       tb_0cd20_1comp = pd.read_csv(tb_0cd20_1comp_path, sep=',').assign(bio=250*['tb',], processing=250*['Automatic U-Net pred.\n+ cut and shift corr.:\nscaler=quantile transf.,\nlarge model (200MB)',])
       tb_0cd20_2comp = pd.read_csv(tb_0cd20_2comp_path, sep=',').assign(bio=250*['tb',], processing=250*['Automatic U-Net pred.\n+ cut and shift corr.:\nscaler=quantile transf.,\nlarge model (200MB)',])
       tb_fe81d_1comp = pd.read_csv(tb_fe81d_1comp_path, sep=',').assign(bio=250*['tb',], processing=250*['Automatic U-Net pred.\n+ cut and shift corr.:\nscaler=standard,\nlarge model (186MB)',])
       tb_fe81d_2comp = pd.read_csv(tb_fe81d_2comp_path, sep=',').assign(bio=250*['tb',], processing=250*['Automatic U-Net pred.\n+ cut and shift corr.:\nscaler=standard,\nlarge model (186MB)',])
       tb_ff67b_1comp = pd.read_csv(tb_ff67b_1comp_path, sep=',').assign(bio=250*['tb',], processing=250*['Automatic U-Net pred.\n+ cut and shift corr.:\nscaler=minmax,\nsmall model (14MB)',])
       tb_ff67b_2comp = pd.read_csv(tb_ff67b_2comp_path, sep=',').assign(bio=250*['tb',], processing=250*['Automatic U-Net pred.\n+ cut and shift corr.:\nscaler=minmax,\nsmall model (14MB)',])
       tb_5_1comp = pd.read_csv(tb_5_1comp_path, sep=',').assign(bio=250*['tb',], processing=250*['Manual thresh. pred.\n+ cut and shift corr.:\nscaler=robust\nthreshold=5',])
       tb_5_2comp = pd.read_csv(tb_5_2comp_path, sep=',').assign(bio=250*['tb',], processing=250*['Manual thresh. pred.\n+ cut and shift corr.:\nscaler=robust\nthreshold=5',])
       tb_7_1comp = pd.read_csv(tb_7_1comp_path, sep=',').assign(bio=250*['tb',], processing=250*['Manual thresh. pred.\n+ cut and shift corr.:\nscaler=robust\nthreshold=7',])
       tb_7_2comp = pd.read_csv(tb_7_2comp_path, sep=',').assign(bio=250*['tb',], processing=250*['Manual thresh. pred.\n+ cut and shift corr.:\nscaler=robust\nthreshold=7',])
       tb_10_1comp = pd.read_csv(tb_10_1comp_path, sep=',').assign(bio=250*['tb',], processing=250*['Manual thresh. pred.\n+ cut and shift corr.:\nscaler=robust\nthreshold=10',])
       tb_10_2comp = pd.read_csv(tb_10_2comp_path, sep=',').assign(bio=250*['tb',], processing=250*['Manual thresh. pred.\n+ cut and shift corr.:\nscaler=robust\nthreshold=10',])

       all_param = pd.concat([af488_noc_1comp, af488_0cd20_1comp, af488_ff67b_1comp,
                              # af488_34a6d_1comp, af488_fe81d_1comp, af488_0dot8_1comp, af488_1_1comp,
                              af488_1dot5_1comp, af488_2_1comp,
                              af488_2dot5_1comp, af488luv_noc_1comp, af488luv_noc_2comp,
                              af488luv_0cd20_1comp, af488luv_0cd20_2comp, # af488luv_34a6d_1comp,
                              # af488luv_34a6d_2comp, af488luv_fe81d_1comp, af488luv_fe81d_2comp,
                              af488luv_ff67b_1comp, af488luv_ff67b_2comp, # af488luv_0dot8_1comp,
                              # af488luv_0dot8_2comp, af488luv_1_1comp, af488luv_1_2comp,
                              af488luv_1dot5_1comp, af488luv_1dot5_2comp, af488luv_2_1comp,
                              af488luv_2_2comp, af488luv_2dot5_1comp, af488luv_2dot5_2comp,
                              hs_noc_1comp, hs_0cd20_1comp, # hs_fe81d_1comp,
                              hs_ff67b_1comp, hs_5_1comp, hs_7_1comp,
                              hs_10_1comp, tb_noc_1comp, tb_noc_2comp,
                              tb_0cd20_1comp, tb_0cd20_2comp, # tb_fe81d_1comp, tb_fe81d_2comp,
                              tb_ff67b_1comp, tb_ff67b_2comp,
                              tb_5_1comp, tb_5_2comp, tb_7_1comp,
                              tb_7_2comp, tb_10_1comp, tb_10_2comp])

       assert set(all_param['Dimen']) == {'3D'}
       assert set(all_param['Diff_eq']) == {'Equation 1B'}
       assert set(all_param['Triplet_eq']) == {'Triplet Eq 2B', 'no triplet'}
       assert set(all_param['Triplet_species']) == {1}
       assert set(all_param['AR1']) == {5.0, 6.0}
       assert set(all_param[all_param['AR1'] == 5.0]['bio']) == {'af488', 'af488luv'}
       assert set(all_param[all_param['AR1'] == 6.0]['bio']) == {'hs', 'tb'}
       assert set(all_param['xmin']) == {0.001018}
       assert set(all_param['xmax']) == {939.52409, 100.66329, 469.762042}
       assert set(all_param[all_param['xmax'] == 100.66329]['bio']) == {'af488'}
       assert set(all_param[all_param['xmax'] == 469.762042]['bio']) == {'af488luv'}
       assert set(all_param[all_param['xmax'] == 939.52409]['bio']) == {'hs', 'tb'}
       assert set(all_param['alpha1']) == {1.0}
       assert set(all_param['alpha1']) == {1.0}
       assert set(all_param['tauT1'].dropna()) == {0.04}

       all_param = all_param[['name_of_plot', 'Diff_species', 'N (FCS)', 'N (mom)', 'A1', 'txy1',
                              'bio', 'processing', 'A2', 'txy2']]
       with pd.option_context("max_colwidth", 1000):
           display(all_param)

     #+END_SRC

     #+RESULTS:
     :RESULTS:
     |     | name_of_plot                                                                               | Diff_species | N (FCS)   | N (mom)   | A1       | txy1       | bio   | processing                                                                | A2       | txy2     |
|-----+--------------------------------------------------------------------------------------------+--------------+-----------+-----------+----------+------------+-------+---------------------------------------------------------------------------+----------+----------|
| 0   | 2022-05-26_tttr2xfcs_CH2_BIN1dot0_20 nM AF4881_T0s_1_correlation-CH2_2                     | 1            | 14.315572 | 75.045477 | 1.000000 | 0.038136   | af488 | Baseline control:\nno correction                                          | NaN      | NaN      |
| 1   | 2022-05-26_tttr2xfcs_CH2_BIN1dot0_20 nM AF4882_T14s_1_correlation-CH2_2                    | 1            | 14.368362 | 73.260903 | 1.000000 | 0.039811   | af488 | Baseline control:\nno correction                                          | NaN      | NaN      |
| 2   | 2022-05-26_tttr2xfcs_CH2_BIN1dot0_20 nM AF4883_T26s_1_correlation-CH2_2                    | 1            | 14.122089 | 73.529686 | 1.000000 | 0.040138   | af488 | Baseline control:\nno correction                                          | NaN      | NaN      |
| 3   | 2022-05-26_tttr2xfcs_CH2_BIN1dot0_20 nM AF4884_T38s_1_correlation-CH2_2                    | 1            | 14.236361 | 72.838417 | 1.000000 | 0.039936   | af488 | Baseline control:\nno correction                                          | NaN      | NaN      |
| 4   | 2022-05-26_tttr2xfcs_CH2_BIN1dot0_20 nM AF4885_T50s_1_correlation-CH2_2                    | 1            | 14.192151 | 70.663922 | 1.000000 | 0.039690   | af488 | Baseline control:\nno correction                                          | NaN      | NaN      |
| ... | ...                                                                                        | ...          | ...       | ...       | ...      | ...        | ...   | ...                                                                       | ...      | ...      |
| 245 | 2022-07-27_tttr2xfcs_CH2_BIN1dot0_TbPEX5EGFP 1-1000246_T5364s_1_DELSHIFT_correlation-CH2_2 | 2            | 0.984319  | 2.073106  | 0.052588 | 12.537737  | tb    | Manual thresh. pred.\n+ cut and shift corr.:\nscaler=robust\nthreshold=10 | 0.947412 | 0.317513 |
| 246 | 2022-07-27_tttr2xfcs_CH2_BIN1dot0_TbPEX5EGFP 1-1000247_T5386s_1_DELSHIFT_correlation-CH2_2 | 2            | 0.962060  | 2.001833  | 0.054851 | 18.316749  | tb    | Manual thresh. pred.\n+ cut and shift corr.:\nscaler=robust\nthreshold=10 | 0.945149 | 0.344213 |
| 247 | 2022-07-27_tttr2xfcs_CH2_BIN1dot0_TbPEX5EGFP 1-1000248_T5408s_1_DELSHIFT_correlation-CH2_2 | 2            | 0.924462  | 2.050364  | 0.014227 | 127.214999 | tb    | Manual thresh. pred.\n+ cut and shift corr.:\nscaler=robust\nthreshold=10 | 0.985773 | 0.359032 |
| 248 | 2022-07-27_tttr2xfcs_CH2_BIN1dot0_TbPEX5EGFP 1-1000249_T5430s_1_DELSHIFT_correlation-CH2_2 | 2            | 0.936036  | 1.974608  | 0.046250 | 24.217742  | tb    | Manual thresh. pred.\n+ cut and shift corr.:\nscaler=robust\nthreshold=10 | 0.953750 | 0.300107 |
| 249 | 2022-07-27_tttr2xfcs_CH2_BIN1dot0_TbPEX5EGFP 1-1000250_T5451s_1_DELSHIFT_correlation-CH2_2 | 2            | 0.898454  | 1.935038  | 0.027107 | 27.336500  | tb    | Manual thresh. pred.\n+ cut and shift corr.:\nscaler=robust\nthreshold=10 | 0.972893 | 0.344808 |

12304 rows × 10 columns
     :END:


     #+BEGIN_SRC jupyter-python
       def sort_fit(param_ls):
           N = param_ls[-1]
           array = np.array(list(param_ls)[:-1]).reshape((2, 2))
           # sort by transit times
           array = array[:, array[0, :].argsort()]
           A_fast = array[1, 0]
           N_fast = A_fast * N
           t_fast = array[0, 0]
           A_slow = array[1, 1]
           N_slow = A_slow * N
           t_slow = array[0, 1]
           if np.isnan(t_slow):
               out = t_fast, N_fast, pd.NA, pd.NA
           elif f'{A_fast:.0%}' == '100%':
               out = t_fast, N_fast, pd.NA, pd.NA
           elif f'{A_slow:.0%}' == '100%':
               out = pd.NA, pd.NA, t_slow, N_slow
           else:
               out = t_fast, N_fast, t_slow, N_slow
           return out

       def sort_fit_legend(param_ls):
           species = param_ls[0]
           component = param_ls[1]
           bio = param_ls[2]

           if (species == 1) and (bio == 'af488'):
               legend_t = '\nAlexaFluor488\n(n=424, no\nartifacts, $\\tau_D$ from\n1 species fit)'
               legend_N = '\nAlexaFluor488\n(n=424, no artifacts,\n$N (FCS)$ from\n1 species fit)'
           elif (species == 1) and (bio == 'af488luv'):
               legend_t = '\nAF488 + DiO LUVs\n(n=440, peak\nartifacts, $\\tau_D$ from\n1 species fit)'
               legend_N = '\nAF488 + DiO LUVs\n(n=440, peak artifacts,\n$N (FCS)$ from\n1 species fit)'
           elif (species == 1) and (bio == 'hs'):
               legend_t = '\nHs-PEX5-eGFP\n(n=250, no\nartifacts, $\\tau_D$ from\n1 species fit)'
               legend_N = '\nHs-PEX5-eGFP\n(n=250, no artifacts,\n$N (FCS)$ from\n1 species fit)'
           elif (species == 1) and (bio == 'tb'):
               legend_t = '\nTb-PEX5-eGFP\n(n=250, peak\nartifacts, $\\tau_D$ from\n1 species fit)'
               legend_N = '\nTb-PEX5-eGFP\n(n=250, peak artifacts,\n$N (FCS)$ from\n1 species fit)'
           elif (species == 2) and (bio == 'af488luv'):
               legend_t = f'\nAF488 + DiO LUVs\n(n=440, peak\nartifacts, $\\tau_D$ from\n{component} sp. of 2 sp. fit)'
               legend_N = f'\nAF488 + DiO LUVs\n(n=440, peak artifacts,\n$N (FCS)\\cdot A$ from\n{component} sp. of 2 sp. fit)'
           elif (species == 2) and (bio == 'tb'):
               legend_t = f'\nTb-PEX5-eGFP\n(n=250, peak\nartifacts, $\\tau_D$ from\n{component} sp. of 2 sp. fit)'
               legend_N = f'\nTb-PEX5-eGFP\n(n=250, peak artifacts,\n$N (FCS)\\cdot A$ from\n{component} sp. of 2 sp. fit)'
           else:
               legend_t, legend_N = 'none', 'none'
           return legend_t, legend_N

       all_param[['t_fast', 'N_fast', 't_slow', 'N_slow']] = all_param[['txy1', 'txy2', 'A1', 'A2', 'N (FCS)']].apply(
           lambda x: sort_fit(x), axis=1, result_type='expand')

       all_param = pd.wide_to_long(all_param, stubnames=['t', 'N'],
                                   i=['name_of_plot', 'processing', 'Diff_species'],
                                   j='fit component',
                                   sep='_', suffix=r'\w+')

       all_param = all_param.reset_index()
       # if Diff_species is 1, there is only 1 component
       all_param = all_param[~((all_param['fit component'] == 'slow') & (all_param['Diff_species'] == 1))]
       all_param = all_param.reset_index()

       all_param[['legend_t', 'legend_N']] = all_param[['Diff_species', 'fit component', 'bio']].apply(
           lambda x: sort_fit_legend(x), axis=1, result_type='expand')
       for i in set(all_param['legend_t']):
           print(len(all_param[all_param['legend_t'] == i]))
       print('--- now delete all t=pd.NA, which I inserted if A_fast or A_slow was 0 ---')
       all_param = all_param[~pd.isna(all_param['t'])]
       for i in set(all_param['legend_t']):
           print(len(all_param[all_param['legend_t'] == i]))

       af488_param = all_param[(all_param['bio'] == 'af488') | (all_param['bio'] == 'af488luv')]
       af488_median_t = af488_param[(af488_param['legend_t'] == '\nAlexaFluor488\n(n=424, no\nartifacts, $\\tau_D$ from\n1 species fit)') &\
                                    (af488_param['processing'] == 'Baseline control:\nno correction')]['t'].median()
       af488_median_N = af488_param[(af488_param['legend_N'] == '\nAlexaFluor488\n(n=424, no artifacts,\n$N (FCS)$ from\n1 species fit)') &\
                                    (af488_param['processing'] == 'Baseline control:\nno correction')]['N'].median()
       af488_plot = af488_param[((af488_param['legend_t'] == '\nAlexaFluor488\n(n=424, no\nartifacts, $\\tau_D$ from\n1 species fit)') | \
                                 (af488_param['legend_t'] == '\nAF488 + DiO LUVs\n(n=440, peak\nartifacts, $\\tau_D$ from\nfast sp. of 2 sp. fit)')) & \
                                ((af488_param['processing'] == 'Baseline control:\nno correction') | \
                                 (af488_param['processing'] == 'Automatic U-Net pred.\n+ cut and shift corr.:\nscaler=quantile transf.,\nlarge model (200MB)') | \
                                 (af488_param['processing'] == 'Manual thresh. pred.\n+ cut and shift corr.:\nscaler=robust\nthreshold=2'))]

       pex5_param = all_param[(all_param['bio'] == 'hs') | (all_param['bio'] == 'tb')]
       pex5_median_t = pex5_param[(pex5_param['legend_t'] == '\nHs-PEX5-eGFP\n(n=250, no\nartifacts, $\\tau_D$ from\n1 species fit)') &\
                                  (pex5_param['processing'] == 'Baseline control:\nno correction')]['t'].median()
       pex5_median_N = pex5_param[(pex5_param['legend_N'] == '\nHs-PEX5-eGFP\n(n=250, no artifacts,\n$N (FCS)$ from\n1 species fit)') &\
                                  (pex5_param['processing'] == 'Baseline control:\nno correction')]['N'].median()
       pex5_plot = pex5_param[((pex5_param['legend_t'] == '\nHs-PEX5-eGFP\n(n=250, no\nartifacts, $\\tau_D$ from\n1 species fit)') | \
                               (pex5_param['legend_t'] == '\nTb-PEX5-eGFP\n(n=250, peak\nartifacts, $\\tau_D$ from\n1 species fit)')) & \
                              ((pex5_param['processing'] == 'Baseline control:\nno correction') | \
                               (pex5_param['processing'] == 'Automatic U-Net pred.\n+ cut and shift corr.:\nscaler=quantile transf.,\nlarge model (200MB)') | \
                               (pex5_param['processing'] == 'Manual thresh. pred.\n+ cut and shift corr.:\nscaler=robust\nthreshold=7'))]

       with pd.option_context("max_colwidth", 1000):
           display(all_param)
     #+END_SRC

     #+RESULTS:
     :RESULTS:
     #+begin_example
       2631
       2631
       1500
       1500
       2631
       2542
       1500
       1500
       --- now delete all t=pd.NA, which I inserted if A_fast or A_slow was 0 ---
       2630
       2631
       1500
       1493
       2631
       2542
       1297
       1500
     #+end_example
     |       | index | name_of_plot                                                                               | processing                                                                | Diff_species | fit component | txy1       | txy2     | bio   | N (mom)   | A2       | N (FCS)   | A1       | t          | N         | legend_t                                                                       | legend_N                                                                               |
|-------+-------+--------------------------------------------------------------------------------------------+---------------------------------------------------------------------------+--------------+---------------+------------+----------+-------+-----------+----------+-----------+----------+------------+-----------+--------------------------------------------------------------------------------+----------------------------------------------------------------------------------------|
| 0     | 0     | 2022-05-26_tttr2xfcs_CH2_BIN1dot0_20 nM AF4881_T0s_1_correlation-CH2_2                     | Baseline control:\nno correction                                          | 1            | fast          | 0.038136   | NaN      | af488 | 75.045477 | NaN      | 14.315572 | 1.000000 | 0.038136   | 14.315572 | \nAlexaFluor488\n(n=424, no\nartifacts, $\tau_D$ from\n1 species fit)          | \nAlexaFluor488\n(n=424, no artifacts,\n$N (FCS)$ from\n1 species fit)                 |
| 1     | 2     | 2022-05-26_tttr2xfcs_CH2_BIN1dot0_20 nM AF4882_T14s_1_correlation-CH2_2                    | Baseline control:\nno correction                                          | 1            | fast          | 0.039811   | NaN      | af488 | 73.260903 | NaN      | 14.368362 | 1.000000 | 0.039811   | 14.368362 | \nAlexaFluor488\n(n=424, no\nartifacts, $\tau_D$ from\n1 species fit)          | \nAlexaFluor488\n(n=424, no artifacts,\n$N (FCS)$ from\n1 species fit)                 |
| 2     | 4     | 2022-05-26_tttr2xfcs_CH2_BIN1dot0_20 nM AF4883_T26s_1_correlation-CH2_2                    | Baseline control:\nno correction                                          | 1            | fast          | 0.040138   | NaN      | af488 | 73.529686 | NaN      | 14.122089 | 1.000000 | 0.040138   | 14.122089 | \nAlexaFluor488\n(n=424, no\nartifacts, $\tau_D$ from\n1 species fit)          | \nAlexaFluor488\n(n=424, no artifacts,\n$N (FCS)$ from\n1 species fit)                 |
| 3     | 6     | 2022-05-26_tttr2xfcs_CH2_BIN1dot0_20 nM AF4884_T38s_1_correlation-CH2_2                    | Baseline control:\nno correction                                          | 1            | fast          | 0.039936   | NaN      | af488 | 72.838417 | NaN      | 14.236361 | 1.000000 | 0.039936   | 14.236361 | \nAlexaFluor488\n(n=424, no\nartifacts, $\tau_D$ from\n1 species fit)          | \nAlexaFluor488\n(n=424, no artifacts,\n$N (FCS)$ from\n1 species fit)                 |
| 4     | 8     | 2022-05-26_tttr2xfcs_CH2_BIN1dot0_20 nM AF4885_T50s_1_correlation-CH2_2                    | Baseline control:\nno correction                                          | 1            | fast          | 0.039690   | NaN      | af488 | 70.663922 | NaN      | 14.192151 | 1.000000 | 0.03969    | 14.192151 | \nAlexaFluor488\n(n=424, no\nartifacts, $\tau_D$ from\n1 species fit)          | \nAlexaFluor488\n(n=424, no artifacts,\n$N (FCS)$ from\n1 species fit)                 |
| ...   | ...   | ...                                                                                        | ...                                                                       | ...          | ...           | ...        | ...      | ...   | ...       | ...      | ...       | ...      | ...        | ...       | ...                                                                            | ...                                                                                    |
| 16430 | 24603 | 2022-07-27_tttr2xfcs_CH2_BIN1dot0_TbPEX5EGFP 1-1000248_T5408s_1_DELSHIFT_correlation-CH2_2 | Manual thresh. pred.\n+ cut and shift corr.:\nscaler=robust\nthreshold=10 | 2            | slow          | 127.214999 | 0.359032 | tb    | 2.050364  | 0.985773 | 0.924462  | 0.014227 | 127.214999 | 0.013152  | \nTb-PEX5-eGFP\n(n=250, peak\nartifacts, $\tau_D$ from\nslow sp. of 2 sp. fit) | \nTb-PEX5-eGFP\n(n=250, peak artifacts,\n$N (FCS)\cdot A$ from\nslow sp. of 2 sp. fit) |
| 16431 | 24604 | 2022-07-27_tttr2xfcs_CH2_BIN1dot0_TbPEX5EGFP 1-1000249_T5430s_1_DELSHIFT_correlation-CH2_2 | Manual thresh. pred.\n+ cut and shift corr.:\nscaler=robust\nthreshold=10 | 2            | fast          | 24.217742  | 0.300107 | tb    | 1.974608  | 0.953750 | 0.936036  | 0.046250 | 0.300107   | 0.892745  | \nTb-PEX5-eGFP\n(n=250, peak\nartifacts, $\tau_D$ from\nfast sp. of 2 sp. fit) | \nTb-PEX5-eGFP\n(n=250, peak artifacts,\n$N (FCS)\cdot A$ from\nfast sp. of 2 sp. fit) |
| 16432 | 24605 | 2022-07-27_tttr2xfcs_CH2_BIN1dot0_TbPEX5EGFP 1-1000249_T5430s_1_DELSHIFT_correlation-CH2_2 | Manual thresh. pred.\n+ cut and shift corr.:\nscaler=robust\nthreshold=10 | 2            | slow          | 24.217742  | 0.300107 | tb    | 1.974608  | 0.953750 | 0.936036  | 0.046250 | 24.217742  | 0.043292  | \nTb-PEX5-eGFP\n(n=250, peak\nartifacts, $\tau_D$ from\nslow sp. of 2 sp. fit) | \nTb-PEX5-eGFP\n(n=250, peak artifacts,\n$N (FCS)\cdot A$ from\nslow sp. of 2 sp. fit) |
| 16433 | 24606 | 2022-07-27_tttr2xfcs_CH2_BIN1dot0_TbPEX5EGFP 1-1000250_T5451s_1_DELSHIFT_correlation-CH2_2 | Manual thresh. pred.\n+ cut and shift corr.:\nscaler=robust\nthreshold=10 | 2            | fast          | 27.336500  | 0.344808 | tb    | 1.935038  | 0.972893 | 0.898454  | 0.027107 | 0.344808   | 0.874099  | \nTb-PEX5-eGFP\n(n=250, peak\nartifacts, $\tau_D$ from\nfast sp. of 2 sp. fit) | \nTb-PEX5-eGFP\n(n=250, peak artifacts,\n$N (FCS)\cdot A$ from\nfast sp. of 2 sp. fit) |
| 16434 | 24607 | 2022-07-27_tttr2xfcs_CH2_BIN1dot0_TbPEX5EGFP 1-1000250_T5451s_1_DELSHIFT_correlation-CH2_2 | Manual thresh. pred.\n+ cut and shift corr.:\nscaler=robust\nthreshold=10 | 2            | slow          | 27.336500  | 0.344808 | tb    | 1.935038  | 0.972893 | 0.898454  | 0.027107 | 27.3365    | 0.024355  | \nTb-PEX5-eGFP\n(n=250, peak\nartifacts, $\tau_D$ from\nslow sp. of 2 sp. fit) | \nTb-PEX5-eGFP\n(n=250, peak artifacts,\n$N (FCS)\cdot A$ from\nslow sp. of 2 sp. fit) |

16224 rows × 16 columns
     :END:

     #+begin_src jupyter-python
       # for all fit data of af488/af488+luv: data=af488_param, aspect=4
       # for publication choose the following and use: data=af488_plot, aspect=1
       # for all fit data of hs-pex5/tb-pex5: data=pex5_param, aspect=4
       # for publication choose the following and use: data=pex5_plot, aspect=1
       def bioplot(data, bio, y, aspect):
           if bio == 'af488':
               if y == 't':
                   median = af488_median_t
                   hline_text = f'\nMedian(AF488)\n=${median:.2f}ms$ (n=424,\nno artifacts,\nno correction)'
               elif y == 'N':
                   median = af488_median_N
                   hline_text = f'\nMedian(AF488)\n=${median:.2f}fl^{{-1}}$ (n=424,\nno artifacts,\nno correction)'
           elif bio == 'pex5':
               if y == 't':
                   median = pex5_median_t
                   hline_text = f'\nMedian(Hs-PEX5-eGFP)\n=${median:.2f}ms$ (n=250,\nno artifacts,\nno correction)'
               elif y == 'N':
                   median = pex5_median_N
                   hline_text = f'\nMedian(Hs-PEX5-eGFP)\n=${median:.2f}fl^{{-1}}$ (n=250,\nno artifacts,\nno correction)'
           if y == 't':
               hue = 'legend_t'
               ylabel = r'log transit time $\tau_{D}$ $[ms]$'
               yscale = 'log'
               y_text = 'transit-times'
           elif y == 'N':
               hue = 'legend_N'
               ylabel = r'particle number $N[fl^{-1}]$]'
               yscale = 'linear'
               y_text = 'particle-number'
           if aspect == 3.5:
               add_text = 'all-fits'
           elif aspect == 1.5:
               add_text = 'pub'
           else:
               add_text = 'other'
           g = sns.catplot(data=data,
                           y=y,
                           x='processing',
                           hue=hue,
                           height=10,
                           aspect=aspect,
                           legend_out=True,
                           kind='boxen',
                           showfliers=False,
                           sharey=False)
           hline = g.ax.axhline(median, lw=2)
           g._legend.remove()
           hline_legend = {hline_text : hline}
           g._legend_data.update(hline_legend)
           g.add_legend(g._legend_data)
           g.map_dataframe(sns.stripplot,
                 y=y,
                 x='processing',
                 hue=hue,
                 dodge=True,
                 palette=sns.color_palette(['0.3']),
                 size=4,
                 jitter=0.2)
           g.fig.suptitle('')
           plt.setp(g.axes, yscale=yscale, xlabel='', ylabel=ylabel)
           if (y == 't') & (aspect == 1):
               for ax in g.axes.flatten():
                   ax.grid(visible=True, which='both', axis='y')
           g.tight_layout()

           plot_file = f'plot3_bioexps_{bio}_{add_text}_{y_text}'
           savefig = f'./data/exp-220316-publication1/jupyter/{plot_file}'
           plt.savefig(f'{savefig}.pdf',
                       bbox_inches='tight', dpi=300)
           os.system(f'pdf2svg {savefig}.pdf {savefig}.svg')
           plt.close('all')
     #+end_src

     #+RESULTS:


     #+BEGIN_SRC jupyter-python
       sns.set_theme(style="whitegrid", font_scale=4, palette='colorblind',
                     context='paper')
       bioplot(af488_param, bio='af488', y='t', aspect=3.5)
       bioplot(af488_param, bio='af488', y='N', aspect=3.5)
       bioplot(pex5_param, bio='pex5', y='t', aspect=3.5)
       bioplot(pex5_param, bio='pex5', y='N', aspect=3.5)
     #+END_SRC

#+RESULTS:

     #+BEGIN_SRC jupyter-python
       sns.set_theme(style="whitegrid", font_scale=3.3, palette='colorblind',
                     context='paper')
       bioplot(af488_plot, bio='af488', y='t', aspect=1.5)
       bioplot(af488_plot, bio='af488', y='N', aspect=1.5)
       bioplot(pex5_plot, bio='pex5', y='t', aspect=1.5)
       bioplot(pex5_plot, bio='pex5', y='N', aspect=1.5)
     #+END_SRC

     #+RESULTS:


   - now we load a ptufile just to visualize the correction method on TCSPC data
     (here: TTTR)
     #+BEGIN_SRC jupyter-python
       data_path = Path("/home/lex/Programme/drmed-collections/drmed-bioexps/brightbursts/1911DD_alexafluor488+LUVs")
       path_clean = data_path / 'clean_subsample/'
       path_dirty = data_path / 'dirty_subsample/'
       output_path = Path("data/exp-220316-publication1/220323_bioexps")

       def get_ptu(path, output_path):

           files = [path / f for f in os.listdir(path) if f.endswith('.ptu')]

           for myfile in (files):
               myfile = Path(myfile)
               (out, ptu_tags, ptu_num_records, glob_res) = ptu.import_ptu(myfile)
               (subChanArrFull, trueTimeArrFull, dTimeArrFull,
                resolution) = (out["chanArr"], out["trueTimeArr"],
                               out["dTimeArr"], out["resolution"])
                # Remove Overflow and Markers; they are not handled at the
                # moment.
               subChanArr = np.array([i for i in subChanArrFull
                                      if not isinstance(i, tuple)])
               trueTimeArr = np.array([i for i in trueTimeArrFull
                                       if not isinstance(i, tuple)])
               dTimeArr = np.array([i for i in dTimeArrFull
                                    if not isinstance(i, tuple)])
               return trueTimeArr, dTimeArr

       ptufile = get_ptu(path_dirty, output_path)
     #+END_SRC

     #+RESULTS:

     #+BEGIN_SRC jupyter-python
       test = pd.DataFrame(data=[ptufile[0], ptufile[1]], index=['macroscopic times', 'microscobpic times'], dtype=int)
       test
     #+END_SRC

     #+RESULTS:
     :RESULTS:
     : /home/lex/Programme/miniconda3/envs/tf/lib/python3.9/site-packages/pandas/core/frame.py:702: FutureWarning: In a future version, passing float-dtype values and an integer dtype to DataFrame will retain floating dtype if they cannot be cast losslessly (matching Series behavior). To retain the old behavior, use DataFrame(data).astype(dtype)
     :   mgr = arrays_to_mgr(
     |                    |   0 |    1 |    2 | ... |    5018346 |    5018347 |    5018348 |
     |--------------------+-----+------+------+-----+------------+------------+------------|
     | macroscopic times  | 875 | 1400 | 2525 | ... | 9996191498 | 9996192848 | 9996196673 |
     | microscobpic times |  39 |   46 |  602 | ... |        238 |        111 |         42 |

     2 rows × 5018349 columns
     :END:

     #+BEGIN_SRC jupyter-python

       display(test.iloc[:, 750:753])
       display(test.iloc[:, 1540:1543])
     #+END_SRC

     #+RESULTS:
     :RESULTS:
     |                    |     750 |     751 |     752 |
     |--------------------+---------+---------+---------|
     | macroscopic times  |  999781 | 1001257 | 1001582 |
     | microscobpic times |      84 |     101 |     220 |

     |                   |    1540 |    1541 |    1542 |
     |-------------------+---------+---------+---------|
     | macroscopic times | 1999613 | 2000339 | 2000814 |
     | microscopic times |      72 |     103 |     970 |
     :END:

     #+BEGIN_SRC jupyter-python
       %cd /beegfs/ye53nis/drmed-git
     #+END_SRC

     #+BEGIN_SRC jupyter-python
       %cd /beegfs/ye53nis/drmed-git
     #+END_SRC

     #+BEGIN_SRC jupyter-python
       %cd /beegfs/ye53nis/drmed-git
     #+END_SRC

*** some additional useful computations and notes
**** the trained models

   10. all metrics after 100th epoch with hparams
       | run                                | val_auc | val_f1 0.5 | val_prec 0.5 | val_recall 0.5 | model size | hp_batch_size | hp_first_filters | hp_input_size       | hp_lr_power |        hp_lr_start | hp_n_levels | hp_pool_size | hp_scaler |
       |------------------------------------+---------+------------+--------------+----------------+------------+---------------+------------------+---------------------+-------------+--------------------+-------------+--------------+-----------|
       | 484af471c61943fa90e5f78e78a229f0   |  0.9814 |     0.9187 |       0.9091 |         0.9285 | 275 MB     |            26 |               44 | 16384 (here: 14000) |           1 | 0.0136170138242663 |           7 |            2 | standard  |
       | 0cd2023eeaf745aca0d3e8ad5e1fc653   |  0.9818 |     0.9069 |       0.8955 |         0.9185 | 200 MB     |            15 |               23 | 16384 (here: 14000) |           7 | 0.0305060808685107 |           6 |            4 | quant_g   |
       | fe81d71c52404ed790b3a32051258da9   |  0.9849 |     0.9260 |       0.9184 |         0.9338 | 186 MB     |            20 |               78 | 16384 (here: 14000) |           4 | 0.0584071108418767 |           4 |            4 | standard  |
       | ff67be0b68e540a9a29a36a2d0c7a5be + |  0.9859 |     0.9298 |       0.9230 |         0.9367 | 14 MB      |            28 |                6 | 16384 (here: 14000) |           1 | 0.0553313915596308 |           5 |            4 | minmax    |
       | 19e3e786e1bc4e2b93856f5dc9de8216   |  0.9595 |     0.8911 |       0.8983 |         0.8839 | 172 MB     |            20 |              128 | 16384 (here: 14000) |           1 |  0.043549707353273 |           3 |            4 | standard  |
       | 347669d050f344ad9fb9e480c814f727 + |  0.9848 |     0.9246 |       0.9254 |         0.9238 | 73 MB      |            10 |               16 | 8192 (here: 14000)  |           1 | 0.0627676336651573 |           5 |            4 | robust    |
       | c1204e3a8a1e4c40a35b5b7b1922d1ce   |  0.9858 |     0.9207 |       0.9179 |         0.9234 | 312 MB     |            14 |               16 | 16384 (here: 14000) |           5 | 0.0192390310290551 |           9 |            2 | robust    |
       | 714af8cd12c1441eac4ca980e8c20070 + |  0.9843 |     0.9304 |       0.9257 |         0.9352 | 234 MB     |             9 |               64 | 4096 (here: 14000)  |           1 | 0.0100697459464075 |           5 |            4 | maxabs    |
       | 34a6d207ac594035b1009c330fb67a65 + |  0.9652 |     0.8613 |       0.8598 |         0.8629 | 7 MB       |            17 |               16 | 16384 (here: 14000) |           5 | 0.0101590069352232 |           3 |            4 | l2        |
**** simulated diffrates to transittimes
   :PROPERTIES:
   :header-args:jupyter-python: :session /jpy:localhost#8888:a37e524a-8134-4d8f-b24a-367acaf1bdd3 :pandoc t
   :END:
   - to interprete the correlations correctly, let's plot the underlying
     experimental data.
     #+BEGIN_SRC jupyter-python
       %cd ~/Programme/drmed-git
     #+END_SRC

     #+RESULTS:
     : /home/lex/Programme/drmed-git

     #+BEGIN_SRC jupyter-python
       import sys
       FLUOTRACIFY_PATH = '/home/lex/Programme/drmed-git/src/'
       sys.path.append(FLUOTRACIFY_PATH)
       from fluotracify.simulations.analyze_simulations import convert_diffcoeff_to_transittimes

       diffcoeffs = [0.01, 0.069, 0.08, 0.1, 0.2, 0.4, 0.6, 1.0, 3.0, 10, 50]
       for i in diffcoeffs:
           tt, _ = convert_diffcoeff_to_transittimes(i, 250)
           print(f'{i} um^2 / s -> {tt:.2f} ms')

       print('----------')
       diffcoeff, _ = convert_diffcoeff_to_transittimes(0.04, 250)
       print(f'af488: {0.04} ms -> {diffcoeff:.2f} um^2 / s')
       diffcoeff, _ = convert_diffcoeff_to_transittimes(0.36, 250)
       print(f'hspex5: {0.36} ms -> {diffcoeff:.2f} um^2 / s')

     #+END_SRC

     #+RESULTS:
     #+begin_example
       2023-01-20 18:47:56.187088: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
       2023-01-20 18:47:56.187203: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
       0.01 um^2 / s -> 1127.11 ms
       0.069 um^2 / s -> 163.35 ms
       0.08 um^2 / s -> 140.89 ms
       0.1 um^2 / s -> 112.71 ms
       0.2 um^2 / s -> 56.36 ms
       0.4 um^2 / s -> 28.18 ms
       0.6 um^2 / s -> 18.79 ms
       1.0 um^2 / s -> 11.27 ms
       3.0 um^2 / s -> 3.76 ms
       10 um^2 / s -> 1.13 ms
       50 um^2 / s -> 0.23 ms
       ----------
       af488: 0.04 ms -> 281.78 um^2 / s
       hspex5: 0.36 ms -> 31.31 um^2 / s
     #+end_example
