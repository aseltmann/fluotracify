#+TITLE: Lab book Fluotracify
#+AUTHOR: Alexander Seltmann
#+LANGUAGE: en
#+PROPERTY: header-args :eval never-export :exports both
#+OPTIONS: toc:4
#+OPTIONS: H:4
#+HTML_HEAD_EXTRA: <style type="text/css">.example {background-color: #FBFBBF;}</style>
#+HTML_HEAD_EXTRA: <style type="text/css">pre.src-emacs-lisp {background-color: #F7ECFB;}</style>
#+HTML_HEAD_EXTRA: <style type="text/css">pre.src-sh {background-color: #F0FBE9;}</style>
#+HTML_HEAD_EXTRA: <style type="text/css">pre.src-tmux {background-color: #E1EED8;}</style>
#+HTML_HEAD_EXTRA: <style type="text/css">pre.src-python {background-color: #E6EDF4;}</style>
#+HTML_HEAD_EXTRA: <style type="text/css">pre.src-jupyter-python {background-color: #FAEAE1;}</style>
#+HTML_HEAD_EXTRA: <style type="text/css">details {padding: 1em; background-color: #f0f0f0; border-radius: 15px; color: hsl(157 75% 20%); font-size: 0.9em; box-shadow: 0.05em 0.1em 5px 0.01em  #00000057;}</style>
#+HTML_HEAD_EXTRA: <style type="text/css">details:hover {background:pink;}</style>
#+HTML_HEAD_EXTRA: <style type="text/css">table {display: block; overflow-x: auto; white-space: nowrap;}</style>


* Technical Notes
** README
*** General:
   - This file corresponds to my lab book for my doctoral thesis tackling
     artifact correction in Fluorescence Correlation Spectroscopy (FCS)
     measurements using Deep Neural Networks. It also contains notes taken
     during the process of setting up this workflow for reproducible research.
   - This file contains explanations of how things are organized, of the
     workflow for doing experiments, changes made to the code, and the observed
     behavior in the "* Data" section.
   - The branching model used is described in [[http://starpu-simgrid.gforge.inria.fr/misc/SIGOPS_paper.pdf][this paper]]. Therefore: if you
     are interested in the "* Data" section, you have to =git clone= the /data/
     branch of the repository. The /main/ branch is clean from any results, it
     contains only source code and the analysis.
   - This project is my take on [[https://en.wikipedia.org/wiki/Open-notebook_science][Open-notebook science]]. The idea was postulated in
     a blog post in 2006:
     #+BEGIN_QUOTE
     ... there is a URL to a laboratory notebook that is freely available and
     indexed on common search engines. It does not necessarily have to look like
     a paper notebook but it is essential that all of the information available
     to the researchers to make their conclusions is equally available to the
     rest of the world ---Jean-Claude Bradley
     #+END_QUOTE
   - Proposal on how to deal with truly private data (e.g. notes from a
     confidential meeting with a colleague), which might otherwise be noted in a
     normal Lab notebook: do not include them here. Only notes relevant to the
     current project should be taken
*** Code block languages used in this document

   #+BEGIN_SRC sh
     # This is a sh block for shell / bash scripting. In the context of this file,
     # these blocks are mainly used for operations on my local computer.
     # In the LabBook.html rendering of this document, these blocks will have a
     # light green colour (#F0FBE9)
   #+END_SRC

   #+BEGIN_SRC tmux
     # This block can open and access tmux sessions, used for shell scripting on
     # remote computing clusters.
     # In the LabBook.html rendering of this document, these blocks will have a
     # distinct light green colour (#E1EED8)
   #+END_SRC

   #+BEGIN_SRC python
     # This is a python block. In the context of this file, it is seldomly used
     # (only for examplary scripts.)
     # In the LabBook.html rendering of this document, these blocks will have a
     # light blue colour (#E6EDF4)
   #+END_SRC

   #+BEGIN_SRC jupyter-python :session /jpy:localhost#8889:704d35be-572a-4268-a70b-565164b8620f
     # This is a jupyter-python block. The code is sent to a jupyter kernel running
     # on a remote high performance computing cluster. Most of my jupyter code is
     # executed this way.
     # In the LabBook.html rendering of this document, these blocks will have a
     # light orange colour (#FAEAE1)
   #+END_SRC

   #+BEGIN_SRC emacs-lisp
     ;; This is a emacs-lisp block, the language used to customize Emacs, which is
     ;; sometimes necessary, since the reproducible workflow of this LabBook is
     ;; tightly integrated with Emacs and org-mode.
     ;; In the LabBook.html rendering of this document, these blocks will have a
     ;; light violet colour (#F7ECFB)
   #+END_SRC

   #+begin_example
     This is a literal example block. It can be used very flexibly - in the context
     of this document the output of most code blocks is displayed this way.
     In the LabBook.html rendering of this document, these blocks will have a light
     yellow colour (#FBFBBF)
   #+end_example

   #+begin_details
   #+begin_example
     This is a literal example block enclosed in a details block. This is useful to
     make the page more readable by collapsing large amounts of output.
     In the Labbook.html rendering of this document, the details block will have a
     light grey colour (#f0f0f0) and a pink color when hovering above it.
   #+end_example
   #+end_details

*** Experiments workflow:
   1) Create a new branch from =main=
   2) Print out the git log from the latest commit and the metadata
   3) Call the analysis scripts, follow the principles outlined in
      [[* Organization of code]]
   4) All machine learning runs are saved in =data/mlruns=, all other data in
      =data/#experiment-name=
   5) Add a ~** exp-<date>-<name>~" section to this file under [[* Data]]
   6) Commit/push the results of this separate branch
   7) Merge this new branch with the remote =data= branch
*** Example for experimental setup procedure

**** Setting starting a jupyter kernel from a remote jupyter session using =emacs-jupyter= in =org babel=
    :PROPERTIES:
    :CUSTOM_ID: sec-jupyter-setup
    :END:

*** tools used (notes)
**** Emacs =magit=
   - =gitflow-avh= (=magit-flow=) to follow the flow
   - possibly https://github.com/magit/magit-annex for large files. Follow this:
     https://git-annex.branchable.com/walkthrough/
   - maybe check out git-toolbelt at some point
     https://github.com/nvie/git-toolbelt#readme with
     https://nvie.com/posts/git-power-tools/
**** jupyter
   - emacs jupyter for running and connecting to kernel on server:
     https://github.com/dzop/emacs-jupyter
   - if I actually still would use .ipynb files, these might come handy:
     + jupytext: https://github.com/mwouts/jupytext
     + nbstripout: https://github.com/kynan/nbstripout
**** mlflow
   - https://docs.faculty.ai/user-guide/experiments/index.html and
     https://docs.microsoft.com/en-us/azure/databricks/_static/notebooks/hls-image-processing/02-image-segmentation-dl.html
**** tensorflow
   - https://www.tensorflow.org/tensorboard/image_summaries

** Template for data entry and setup notes:
*** exp-#date-#title
**** git:

    #+begin_src sh
    git log -1
    #+end_src

**** System Metadata:

    #+NAME: jp-metadata
    #+BEGIN_SRC jupyter-python :var _long="true"
      import os
      import pprint

      ramlist = os.popen('free -th').readlines()[-1].split()[1:]

      print('No of CPUs in system:', os.cpu_count())
      print('No of CPUs the current process can use:',
            len(os.sched_getaffinity(0)))
      print('load average:', os.getloadavg())
      print('os.uname(): ', os.uname())
      print('PID of process:', os.getpid())
      print('RAM total: {}, RAM used: {}, RAM free: {}'.format(
          ramlist[0], ramlist[1], ramlist[2]))

      !echo the current directory: $PWD
      !echo My disk usage:
      !df -h
      if _long:
          %conda list
          pprint.pprint(dict(os.environ), sort_dicts=False)

    #+END_SRC

**** Tmux setup and scripts
    :PROPERTIES:
    :CUSTOM_ID: scripts-tmux
    :END:

    #+NAME: setup-tmux
    #+BEGIN_SRC sh :session local
    rm ~/.tmux-local-socket-remote-machine
    REMOTE_SOCKET=$(ssh ara 'tmux ls -F "#{socket_path}"' | head -1)
    echo $REMOTE_SOCKET
    ssh ara -tfN \
        -L ~/.tmux-local-socket-remote-machine:$REMOTE_SOCKET
    #+END_SRC

    #+RESULTS: setup-tmux
    | rm:                                  | cannot                               | remove    | '/home/lex/.tmux-local-socket-remote-machine': | No | such | file | or | directory |
    | ye53nis@ara-login01.rz.uni-jena.de's | password:                            |           |                                                |    |      |      |    |           |
    | /tmp/tmux-67339/default              |                                      |           |                                                |    |      |      |    |           |
    | >                                    | ye53nis@ara-login01.rz.uni-jena.de's | password: |                                                |    |      |      |    |           |

**** SSH tunneling
    :PROPERTIES:
    :CUSTOM_ID: ssh-tunneling
    :END:

    Different applications can be run on the remote compute node. If I want to
    access them at the local machine, and open them with the browser, I use this
    tunneling script.

    #+NAME: ssh-tunnel
    #+BEGIN_SRC sh :session org-tunnel :var port="8889" :var node="node001"
    ssh -t -t ara -L $port:localhost:$port ssh $node -L $port:Localhost:$port
    #+END_SRC

    Apps I use that way:
    - Jupyter lab for running Python 3-Kernels
    - TensorBoard
    - Mlflow ui

**** jupyter scripts
    :PROPERTIES:
    :CUSTOM_ID: scripts-jp
    :END:

    Starting a jupyter instance on a server where the necessary libraries are
    installed is easy using this script:

    #+NAME: jpt-tmux
    #+BEGIN_SRC tmux :socket ~/.tmux-local-socket-remote-machine
    conda activate tf
    export PORT=8889
    export XDG_RUNTIME_DIR=''
    export XDG_RUNTIME_DIR=""
    jupyter lab --no-browser --port=$PORT
    #+END_SRC

    On the compute node of the HPC, the users' environment is managed through
    module files using the system [[https://lmod.readthedocs.io][Lmod]]. The =export XDG_RUNTIME_DIR= statements
    are needed because of a jupyter bug which did not let it start. Right now,
    =ob-tmux= does not support a =:var= header like normal =org-babel= does. So
    the =$port= variable has to be set here in the template.

    Now this port has to be tunnelled on our local computer (See
    [[#ssh-tunneling]]). While the tmux session above keeps running, no matter if
    Emacs is running or not, this following ssh tunnel needs to be active
    locally to connect to the notebook. If you close Emacs, it would need to be
    reestablished

*** Setup notes
**** Setting up a tmux connection from using =ob-tmux= in =org-babel=
    :PROPERTIES:
    :CUSTOM_ID: sec-tmux-setup
    :END:
    - prerequisite: tmux versions need to be the same locally and on the server.
      Let's verify that now.
      - the local tmux version:

        #+BEGIN_SRC sh
        tmux -V
        #+END_SRC

        #+RESULTS:
        : tmux 3.0a

      - the remote tmux version:

       #+BEGIN_SRC sh :session local
        ssh ara tmux -V
      #+END_SRC

        #+RESULTS:
        | ye53nis@ara-login01.rz.uni-jena.de's | password: |
        | tmux                                 | 3.0a      |

    - as is described in [[https://github.com/ahendriksen/ob-tmux][the ob-tmux readme]], the following code snippet creates
      a socket on the remote machine and forwards this socket to the local
      machine (note that =socket_path= was introduced in tmux version 2.2)

      #+BEGIN_SRC sh :session local
      REMOTE_SOCKET=$(ssh ara 'tmux ls -F "#{socket_path}"' | head -1)
      echo $REMOTE_SOCKET
      ssh ara -tfN \
          -L ~/.tmux-local-socket-remote-machine:$REMOTE_SOCKET
      #+END_SRC

      #+RESULTS:
      | ye53nis@ara-login01.rz.uni-jena.de's | password:                            |           |
      | /tmp/tmux-67339/default              |                                      |           |
      | >                                    | ye53nis@ara-login01.rz.uni-jena.de's | password: |

    - now a new tmux session with name =ob-NAME= is created when using a code
      block which looks like this: =#+BEGIN_SRC tmux :socket
      ~/.tmux-local-socket-remote-machine :session NAME=
    - Commands can be sent now to the remote tmux session, BUT note that the
      output is not printed yet
    - there is a workaround for getting output back to our LabBook.org: A [[#scripts-tmux][script]]
      which allows to print the output from the tmux session in an
      =#+begin_example=-Block below the tmux block by pressing =C-c C-o= or =C-c
      C-v C-o= when the pointer is inside the tmux block.

**** =emacs-jupyter= Setup

    =Emacs-jupyter= aims to be an API for a lot of functionalities of the
    =jupyter= project. The documentation can be found on [[https://github.com/dzop/emacs-jupyter][GitHub]].

    1. For the *whole document*: connect to a running jupyter instance
       1. =M-x jupyter-server-list-kernels=
          1. set server URL, e.g. =http://localhost:8889=
          2. set websocket URL, e.g. =http://localhost:8889=
       2. two possibilities
          1. kernel already exists $\to$ list of kernels and =kernel-ID= is displayed
          2. kernel does not exist $\to$ prompt asks if you want to start one $\to$
             *yes* $\to$ type kernel you want to start, e.g. =Python 3=
    2. In the *subtree* where you want to use =jupyter-python= blocks with =org
       babel=
       1. set the =:header-args:jupyter-python :session
          /jpy:localhost#kernel:8889-ID=
       2. customize the output folder using the following org-mode variable:
          #+BEGIN_SRC  emacs-lisp
            (setq org-babel-jupyter-resource-directory "./data/exp-test/plots")
          #+END_SRC

          #+RESULTS:
          : ./data/exp-test/plots
    3. For each *individual block*, the following customizations might be useful
       1. jupyter kernels can return multiple kinds of rich output (images,
          html, ...) or scalar data (plain text, numbers, lists, ...). To force
          a plain output, use =:results scalar=. To show the output in the
          minibuffer only, use =:results silent=
       2. to change the priority of different rich outputs, use =:display=
          header argument, e.g. =:display text/plain text/html= prioritizes
          plain text over html. All supported mimetypes in default order:
          1. text/org
          2. image/svg+xml, image/jpeg, image/png
          3. text/html
          4. text/markdown
          5. text/latex
          6. text/plain
       3. We can set jupyter to output pandas DataFrames as org tables
          automatically using the source block header argument =:pandoc t=
       4. useful keybindings
          - =M-i= to open the documentation for wherever your pointer is (like
            pressing =Shift-TAB= in Jupyter notebooks)
          - =C-c C-i= to interrupt the kernel, =C-c C-r= to restart the kernel

*** Notes on archiving
**** Exporting the LabBook.org to html in a twbs style
     - I am partial to the twitter bootstrap theme of html, since I like it's
       simple design, but clear structure with a nice table of contents at the
       side → the following org mode extension supports a seemless export to
       twitter bootstrap html: https://github.com/marsmining/ox-twbs
     - when installed, the export can be triggered via the command
       =(org-twbs-export-as-html)= or via the keyboard shortcut for export =C-c
       C-e= followed by =w= for Twitter bootstrap and =h= for saving the .html
     - _Things to configure:_
       - in general, there are multiple export options:
         https://orgmode.org/manual/Export-Settings.html
       - E.g. I set 2 =#+OPTIONS= keywords at the begin of the file: =toc:4= and
         =H:4= which make sure that in my export my sidebar table of contents
         will show numbered headings till a depth of 4.
       - I configured my code blocks so that they will not be evaluated when
         exporting (I would recommend this especially if you only export for
         archiving) and that both the code block and the output will be exported
         with the keyword: =#+PROPERTY: header-args :eval never-export :exports
         both=
       - To discriminate between code blocks for different languages I gave each
         of them a distinct colour using =#+HTML_HEAD_EXTRA: <style...= (see
         above)
       - I had to configure a style for =table=, so that the
         - =display: block; overflow-x: auto;= gets the table to be restricted
           to the width of the text and if it is larger, activates scrolling
         - =white-space: nowrap;= makes it that there is no wrap in a column, so
           it might be broader, but better readable if you have scrolling anyway
     - _Things to do before exporting / Troubleshooting while exporting:_
       - when using a dark theme for you emacs, the export of the code blocks
         might show some ugly dark backgrounds from the theme. If this becomes
         an issue, change to a light theme for the export with =M-x
         (load-theme)= and choose =solarized-light=
       - only in the =data= branch you set the git tags after merging. If you
         want to show them here, execute the corresponding function in [[Git TAGs]]
       - make sure your file links work properly! I recommend referencing your
         files relatively (e.g. [ [ f ile:./data/exp-XXXXXX-test/test.png]]
         without spaces). Otherwise there will be errors in your /*Messages*/
         buffer
       - There might be errors with your code blocks
         - e.g. the export function expects you to assign a default variable to
           your functions
         - if you call a function via the =#+CALL= mechanism, it wants you to
           include two parentheses for the function, e.g. =#+CALL: test()=
       - check indentation of code blocks inside lists
       - add a =details= block around large output cells. This makes them
         expandable. I added some =#+HTML_HEAD_EXTRA: <style...= inspired by
         [[https://alhassy.github.io/org-special-block-extras/#Folded-Details][alhassy]]. That's how the =details= block looks like:
         #+begin_example
         #+begin_details

         #+end_details
         #+end_example
       - If you reference a parameter with an underscore in the name, use the
         org markdown tricks to style them like code (~==~ or =~~=), otherwise
         the part after the underscore will be rendered like a subscript:
         =under_score= vs under_score
     - _Things to do after exporting:_
       - In my workflow, the exported =LabBook.html= with the overview of all
         experiments is in the =data= folder. If you move the file, you will
         have to fix the file links for the new location, e.g. via "Find and
         replace" =M-%=:
         - if you move the org file → in the org file find =[[file:./data/= and
           replace with =[[file:./= → then export with =C-c C-e w h=
         - if you export first with =C-c C-e w h= and move the html file to
           =data= → in the html file find =./data= and replace with =.=
** Organization of git

*** remote/origin/main branch:
  - contains all the source code in folder **src/** which is used for experiments.
  - contains the **LabBook.org** template
  - contains setup- and metadata files such as **MLproject** or **conda.yaml**
  - the log contains only lasting alterations on the folders and files mentioned
    above, which are e.g. used for conducting experiments or which introduce new
    features. Day-to-day changes in code
*** remote/origin/exp### branches:
  - if an experiment is done, the code and templates will be branched out from
    *main* in an *#experiment-name* branch, ### meaning some meaningful
    descriptor.
  - all data generated during the experiment (e.g. .csv files, plots, images,
    etc), is stored in a folder with the name **data/#experiment-name**, except
    machine learning-specific data and metadata from `mlflow` runs, which are
    saved under **data/mlruns** (this allows easily comparing machine learning
    runs with different experimental settings)
  - The **LabBook.org** file is essential
    - If possible, all code is executed from inside this file (meaning analysis
      scripts or calling the code from the **scr/** directory).
    - All other steps taken during an experiment are noted down, as well as
      conclusions or my thought process while conducting the experiment
    - Provenance data, such as Metadata about the environment the code was
      executed in, the command line output of the code, and some
*** remote/origin/develop branch:
  - this is the branch I use for day to day work on features and exploration.
    All of my current activity can be followed here.
*** remote/origin/data branch:
  - contains a full cronicle of the whole research process
  - all *#experiment-name* branches are merged here. Afterwards the original
    branch is deleted and on the data branch there is a *Git tag* which shows
    the merge commit to make accessing single experiments easy.
  - the *develop* branch is merged here as well.

*** Git TAGs
**** Stable versions:
**** All tags from git:
   #+begin_src sh :results output
    git push origin --tags
    git tag -n1
   #+end_src

   #+RESULTS:
   : exp-200402-test Merge branch 'exp-200402-test' into data
   : exp-200520-unet Merge branch 'exp-310520-unet' into data
   : exp-200531-unet Merge branch 'heads/exp-310520-unet' into data
   : exp-201231-clustsim exp-201231-clustsim
   : exp-210204-unet Add exp-210204-unet LabBook part 3
   : exp-310520-unet move exp-310520-unet to data branch manually
** Organization of code
*** scripts:
*** src/
**** fluotracify/
***** imports/
***** simulations/
***** training/
***** applications/
***** doc/
    - use Sphinx
      - follow this: https://daler.github.io/sphinxdoc-test/includeme.html
      - evtl export org-mode Readme to rst via https://github.com/msnoigrs/ox-rst
      - possibly heavily use
        http://www.sphinx-doc.org/en/master/usage/extensions/autodoc.html
    - for examples sphinx-galleries could be useful
      https://sphinx-gallery.github.io/stable/getting_started.html

**** nanosimpy/
    - cloned from dwaithe with refactoring for Python 3-compatibility

** Changes in this repository (without "* Data" in this file)
*** Changes in LabBook.org (without "* Data")
**** 2022-02-19
     - Add =#+HTML_HEAD_EXTRA: <style...= for =table= to enable scrolling if the
       table overflows
**** 2021-12-16
     - Add =details= blocks, corresponding =#+HTML_HEAD_EXTRA: <style...= and
       documentation in  [[Notes on archiving]]
**** 2021-08-05
     - Rename =master= branch to =main= branch
**** 2021-04-04
     - Add =#+OPTIONS: H:4= and =#+OPTIONS: toc:4= to show up to 4 levels of
       depth in the html (twbs) export of this LabBook in the table of contents
       at the side
     - I added [[Notes on archiving]]
**** 2020-11-04
    - update "jupyter scripts" in [[Template for data entry and setup notes:]]
      for new conda environment on server (now =conda activate tf-nightly=)
**** 2020-05-31
    - extend general documentation in README
    - Add code block examples
    - extend documentation on experiment workflow
    - move setup notes from README to "Template for data entry and setup notes"
    - remove emacs-lisp code for custom tmux block functions (not relevant
      enough)
    - change named "jpt-tmux" from starting a jupyter notebook to starting
      jupyter lab. Load a conda environment instead of using Lmod's =module
      load=
**** 2020-05-07
    - extend documentation on git model
    - extend documentation on jupyter setup
**** 2020-04-22
    - added parts of README which describe the experimental process
    - added templates for system metadata, tmux, jupyter setup
    - added organization of code
**** 2020-03-30
    - set up lab book and form git repo accoring to setup by Luka Stanisic et al
*** Changes in src/fluotracify
* Data
** exp-220316-publication1
*** Setup: Jupyter on local computer
   :PROPERTIES:
   :header-args:jupyter-python: :session /jpy:localhost#8888:a37e524a-8134-4d8f-b24a-367acaf1bdd3
   :END:
   1. let's start a conda environment in the sh session local and start
      jupterlab there.
      #+begin_src sh :session local
        conda activate tf
        jupyter lab --no-browser --port=8888
      #+end_src

      #+begin_example
        sh-5.1$ [I 2022-03-16 15:03:56.359 ServerApp] jupyterlab | extension was successfully linked.
        [I 2022-03-16 15:03:56.565 ServerApp] nbclassic | extension was successfully linked.
        [I 2022-03-16 15:03:56.604 LabApp] JupyterLab extension loaded from /home/lex/Programme/miniconda3/envs/tf/lib/python3.9/site-packages/jupyterlab
        [I 2022-03-16 15:03:56.604 LabApp] JupyterLab application directory is /home/lex/Programme/miniconda3/envs/tf/share/jupyter/lab
        [I 2022-03-16 15:03:56.608 ServerApp] jupyterlab | extension was successfully loaded.
        [I 2022-03-16 15:03:56.615 ServerApp] nbclassic | extension was successfully loaded.
        [I 2022-03-16 15:03:56.615 ServerApp] Serving notebooks from local directory: /home/lex/Programme/drmed-git
        [I 2022-03-16 15:03:56.615 ServerApp] Jupyter Server 1.4.1 is running at:
        [I 2022-03-16 15:03:56.615 ServerApp] http://localhost:8888/lab?token=19554a9e5e286f799f4ebeb592d24f1210f639d786d16d8e
        [I 2022-03-16 15:03:56.615 ServerApp]  or http://127.0.0.1:8888/lab?token=19554a9e5e286f799f4ebeb592d24f1210f639d786d16d8e
        [I 2022-03-16 15:03:56.615 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
        [C 2022-03-16 15:03:56.619 ServerApp]

            To access the server, open this file in a browser:
                file:///home/lex/.local/share/jupyter/runtime/jpserver-172531-open.html
            Or copy and paste one of these URLs:
                http://localhost:8888/lab?token=19554a9e5e286f799f4ebeb592d24f1210f639d786d16d8e
             or http://127.0.0.1:8888/lab?token=19554a9e5e286f799f4ebeb592d24f1210f639d786d16d8e

      #+end_example

   2. I started a Python3 kernel using =jupyter-server-list-kernels=. Then I
      added the kernel ID to the =:PROPERTIES:= drawer of this (and following)
      subtrees.

      #+begin_example
      python3           03038b73-b2b5-49ce-a1dc-21afb6247d0f   a few seconds ago    starting   0
      #+end_example

   3. Test: (~#+CALL: jp-metadata(_long='True)~)
      #+CALL: jp-metadata(_long='True)

      #+RESULTS:
      :RESULTS:
      #+begin_example
        No of CPUs in system: 4
        No of CPUs the current process can use: 4
        load average: (0.54, 0.96, 0.87)
        os.uname():  posix.uname_result(sysname='Linux', nodename='Topialex', release='5.13.19-2-MANJARO', version='#1 SMP PREEMPT Sun Sep 19 21:31:53 UTC 2021', machine='x86_64')
        PID of process: 173537
        RAM total: 16Gi, RAM used: 2,7Gi, RAM free: 8,9Gi
        the current directory: /home/lex/Programme/drmed-git
        My disk usage:
        Filesystem      Size  Used Avail Use% Mounted on
        dev             3,9G     0  3,9G   0% /dev
        run             3,9G  1,5M  3,9G   1% /run
        /dev/sda2       167G  127G   32G  81% /
        tmpfs           3,9G  115M  3,8G   3% /dev/shm
        tmpfs           3,9G  7,8M  3,9G   1% /tmp
        /dev/sda1       300M  264K  300M   1% /boot/efi
        tmpfs           784M  104K  784M   1% /run/user/1000# packages in environment at /home/lex/Programme/miniconda3/envs/tf:
        #
        # Name                    Version                   Build  Channel
        _libgcc_mutex             0.1                        main
        _openmp_mutex             4.5                       1_gnu
        absl-py                   1.0.0                    pypi_0    pypi
        alembic                   1.4.1                    pypi_0    pypi
        anyio                     2.2.0            py39h06a4308_1
        argon2-cffi               20.1.0           py39h27cfd23_1
        asteval                   0.9.25                   pypi_0    pypi
        astroid                   2.9.2                    pypi_0    pypi
        astunparse                1.6.3                    pypi_0    pypi
        async_generator           1.10               pyhd3eb1b0_0
        attrs                     21.2.0             pyhd3eb1b0_0
        babel                     2.9.1              pyhd3eb1b0_0
        backcall                  0.2.0              pyhd3eb1b0_0
        bleach                    4.0.0              pyhd3eb1b0_0
        brotlipy                  0.7.0           py39h27cfd23_1003
        ca-certificates           2021.10.26           h06a4308_2
        cachetools                4.2.4                    pypi_0    pypi
        certifi                   2021.10.8        py39h06a4308_0
        cffi                      1.14.6           py39h400218f_0
        charset-normalizer        2.0.4              pyhd3eb1b0_0
        click                     8.0.3                    pypi_0    pypi
        cloudpickle               2.0.0                    pypi_0    pypi
        cryptography              36.0.0           py39h9ce1e76_0
        cycler                    0.11.0                   pypi_0    pypi
        cython                    0.29.26                  pypi_0    pypi
        databricks-cli            0.16.2                   pypi_0    pypi
        debugpy                   1.5.1            py39h295c915_0
        decorator                 5.1.0              pyhd3eb1b0_0
        defusedxml                0.7.1              pyhd3eb1b0_0
        docker                    5.0.3                    pypi_0    pypi
        entrypoints               0.3              py39h06a4308_0
        fcsfiles                  2021.6.6                 pypi_0    pypi
        flake8                    4.0.1                    pypi_0    pypi
        flask                     2.0.2                    pypi_0    pypi
        flatbuffers               2.0                      pypi_0    pypi
        focuspoint                0.1                      pypi_0    pypi
        fonttools                 4.28.5                   pypi_0    pypi
        future                    0.18.2                   pypi_0    pypi
        gast                      0.4.0                    pypi_0    pypi
        gitdb                     4.0.9                    pypi_0    pypi
        gitpython                 3.1.24                   pypi_0    pypi
        google-auth               2.3.3                    pypi_0    pypi
        google-auth-oauthlib      0.4.6                    pypi_0    pypi
        google-pasta              0.2.0                    pypi_0    pypi
        greenlet                  1.1.2                    pypi_0    pypi
        grpcio                    1.43.0                   pypi_0    pypi
        gunicorn                  20.1.0                   pypi_0    pypi
        h5py                      3.6.0                    pypi_0    pypi
        idna                      3.3                pyhd3eb1b0_0
        importlib-metadata        4.8.2            py39h06a4308_0
        importlib_metadata        4.8.2                hd3eb1b0_0
        ipykernel                 6.4.1            py39h06a4308_1
        ipython                   7.29.0           py39hb070fc8_0
        ipython_genutils          0.2.0              pyhd3eb1b0_1
        isort                     5.10.1                   pypi_0    pypi
        itsdangerous              2.0.1                    pypi_0    pypi
        jedi                      0.18.0           py39h06a4308_1
        jinja2                    3.0.2              pyhd3eb1b0_0
        joblib                    1.1.0                    pypi_0    pypi
        json5                     0.9.6              pyhd3eb1b0_0
        jsonschema                3.2.0              pyhd3eb1b0_2
        jupyter_client            7.1.0              pyhd3eb1b0_0
        jupyter_core              4.9.1            py39h06a4308_0
        jupyter_server            1.4.1            py39h06a4308_0
        jupyterlab                3.2.1              pyhd3eb1b0_1
        jupyterlab_pygments       0.1.2                      py_0
        jupyterlab_server         2.8.2              pyhd3eb1b0_0
        keras                     2.7.0                    pypi_0    pypi
        keras-preprocessing       1.1.2                    pypi_0    pypi
        kiwisolver                1.3.2                    pypi_0    pypi
        lazy-object-proxy         1.7.1                    pypi_0    pypi
        ld_impl_linux-64          2.35.1               h7274673_9
        libclang                  12.0.0                   pypi_0    pypi
        libffi                    3.3                  he6710b0_2
        libgcc-ng                 9.3.0               h5101ec6_17
        libgomp                   9.3.0               h5101ec6_17
        libsodium                 1.0.18               h7b6447c_0
        libstdcxx-ng              9.3.0               hd4cf53a_17
        lmfit                     1.0.3                    pypi_0    pypi
        mako                      1.1.6                    pypi_0    pypi
        markdown                  3.3.6                    pypi_0    pypi
        markupsafe                2.0.1            py39h27cfd23_0
        matplotlib                3.5.1                    pypi_0    pypi
        matplotlib-inline         0.1.2              pyhd3eb1b0_2
        mccabe                    0.6.1                    pypi_0    pypi
        mistune                   0.8.4           py39h27cfd23_1000
        mlflow                    1.22.0                   pypi_0    pypi
        multipletau               0.3.3                    pypi_0    pypi
        mypy                      0.930                    pypi_0    pypi
        mypy-extensions           0.4.3                    pypi_0    pypi
        nbclassic                 0.2.6              pyhd3eb1b0_0
        nbclient                  0.5.3              pyhd3eb1b0_0
        nbconvert                 6.1.0            py39h06a4308_0
        nbformat                  5.1.3              pyhd3eb1b0_0
        ncurses                   6.3                  h7f8727e_2
        nest-asyncio              1.5.1              pyhd3eb1b0_0
        nodeenv                   1.6.0                    pypi_0    pypi
        notebook                  6.4.6            py39h06a4308_0
        numpy                     1.21.5                   pypi_0    pypi
        oauthlib                  3.1.1                    pypi_0    pypi
        openssl                   1.1.1l               h7f8727e_0
        opt-einsum                3.3.0                    pypi_0    pypi
        packaging                 21.3               pyhd3eb1b0_0
        pandas                    1.3.5                    pypi_0    pypi
        pandocfilters             1.4.3            py39h06a4308_1
        parso                     0.8.2              pyhd3eb1b0_0
        pexpect                   4.8.0              pyhd3eb1b0_3
        pickleshare               0.7.5           pyhd3eb1b0_1003
        pillow                    8.4.0                    pypi_0    pypi
        pip                       21.2.4           py39h06a4308_0
        platformdirs              2.4.1                    pypi_0    pypi
        prometheus-flask-exporter 0.18.7                   pypi_0    pypi
        prometheus_client         0.12.0             pyhd3eb1b0_0
        prompt-toolkit            3.0.20             pyhd3eb1b0_0
        protobuf                  3.19.1                   pypi_0    pypi
        ptyprocess                0.7.0              pyhd3eb1b0_2
        pyasn1                    0.4.8                    pypi_0    pypi
        pyasn1-modules            0.2.8                    pypi_0    pypi
        pycodestyle               2.8.0                    pypi_0    pypi
        pycparser                 2.21               pyhd3eb1b0_0
        pydot                     1.4.2                    pypi_0    pypi
        pyflakes                  2.4.0                    pypi_0    pypi
        pygments                  2.10.0             pyhd3eb1b0_0
        pylint                    2.12.2                   pypi_0    pypi
        pyopenssl                 21.0.0             pyhd3eb1b0_1
        pyparsing                 3.0.4              pyhd3eb1b0_0
        pyright                   0.0.13                   pypi_0    pypi
        pyrsistent                0.18.0           py39heee7806_0
        pysocks                   1.7.1            py39h06a4308_0
        python                    3.9.7                h12debd9_1
        python-dateutil           2.8.2              pyhd3eb1b0_0
        python-editor             1.0.4                    pypi_0    pypi
        pytz                      2021.3             pyhd3eb1b0_0
        pyyaml                    6.0                      pypi_0    pypi
        pyzmq                     22.3.0           py39h295c915_2
        querystring-parser        1.2.4                    pypi_0    pypi
        readline                  8.1                  h27cfd23_0
        requests                  2.26.0             pyhd3eb1b0_0
        requests-oauthlib         1.3.0                    pypi_0    pypi
        rsa                       4.8                      pypi_0    pypi
        scikit-learn              1.0.2                    pypi_0    pypi
        scipy                     1.7.3                    pypi_0    pypi
        seaborn                   0.11.2                   pypi_0    pypi
        send2trash                1.8.0              pyhd3eb1b0_1
        setuptools                58.0.4           py39h06a4308_0
        six                       1.16.0             pyhd3eb1b0_0
        smmap                     5.0.0                    pypi_0    pypi
        sniffio                   1.2.0            py39h06a4308_1
        sqlalchemy                1.4.29                   pypi_0    pypi
        sqlite                    3.37.0               hc218d9a_0
        sqlparse                  0.4.2                    pypi_0    pypi
        tabulate                  0.8.9                    pypi_0    pypi
        tensorboard               2.7.0                    pypi_0    pypi
        tensorboard-data-server   0.6.1                    pypi_0    pypi
        tensorboard-plugin-wit    1.8.0                    pypi_0    pypi
        tensorflow                2.7.0                    pypi_0    pypi
        tensorflow-estimator      2.7.0                    pypi_0    pypi
        tensorflow-io-gcs-filesystem 0.23.1                   pypi_0    pypi
        termcolor                 1.1.0                    pypi_0    pypi
        terminado                 0.9.4            py39h06a4308_0
        testpath                  0.5.0              pyhd3eb1b0_0
        threadpoolctl             3.0.0                    pypi_0    pypi
        tk                        8.6.11               h1ccaba5_0
        toml                      0.10.2                   pypi_0    pypi
        tomli                     2.0.0                    pypi_0    pypi
        tornado                   6.1              py39h27cfd23_0
        traitlets                 5.1.1              pyhd3eb1b0_0
        typing-extensions         4.0.1                    pypi_0    pypi
        tzdata                    2021e                hda174b7_0
        uncertainties             3.1.6                    pypi_0    pypi
        urllib3                   1.26.7             pyhd3eb1b0_0
        wcwidth                   0.2.5              pyhd3eb1b0_0
        webencodings              0.5.1            py39h06a4308_1
        websocket-client          1.2.3                    pypi_0    pypi
        werkzeug                  2.0.2                    pypi_0    pypi
        wheel                     0.37.0             pyhd3eb1b0_1
        wrapt                     1.13.3                   pypi_0    pypi
        xz                        5.2.5                h7b6447c_0
        zeromq                    4.3.4                h2531618_0
        zipp                      3.6.0              pyhd3eb1b0_0
        zlib                      1.2.11               h7f8727e_4

        Note: you may need to restart the kernel to use updated packages.
        {'SHELL': '/bin/bash',
         'SESSION_MANAGER': 'local/Topialex:@/tmp/.ICE-unix/982,unix/Topialex:/tmp/.ICE-unix/982',
         'WINDOWID': '0',
         'COLORTERM': 'truecolor',
         'XDG_CONFIG_DIRS': '/home/lex/.config/kdedefaults:/etc/xdg',
         'XDG_SESSION_PATH': '/org/freedesktop/DisplayManager/Session1',
         'CONDA_EXE': '/home/lex/Programme/miniconda3/bin/conda',
         '_CE_M': '',
         'LANGUAGE': 'en_GB',
         'TERMCAP': '',
         'LC_ADDRESS': 'de_DE.UTF-8',
         'LC_NAME': 'de_DE.UTF-8',
         'INSIDE_EMACS': '27.2,comint',
         'SHELL_SESSION_ID': '705160b4e8124043a99f47cc2610e9a5',
         'DESKTOP_SESSION': 'plasma',
         'LC_MONETARY': 'de_DE.UTF-8',
         'GTK_RC_FILES': '/etc/gtk/gtkrc:/home/lex/.gtkrc:/home/lex/.config/gtkrc',
         'XCURSOR_SIZE': '24',
         'GTK_MODULES': 'canberra-gtk-module',
         'XDG_SEAT': 'seat0',
         'PWD': '/home/lex/Programme/drmed-git',
         'XDG_SESSION_DESKTOP': 'KDE',
         'LOGNAME': 'lex',
         'XDG_SESSION_TYPE': 'x11',
         'CONDA_PREFIX': '/home/lex/Programme/miniconda3/envs/tf',
         'DSSI_PATH': '/home/lex/.dssi:/usr/lib/dssi:/usr/local/lib/dssi',
         'SYSTEMD_EXEC_PID': '862',
         'XAUTHORITY': '/home/lex/.Xauthority',
         'MOTD_SHOWN': 'pam',
         'GTK2_RC_FILES': '/etc/gtk-2.0/gtkrc:/home/lex/.gtkrc-2.0:/home/lex/.config/gtkrc-2.0',
         'HOME': '/home/lex',
         'LC_PAPER': 'de_DE.UTF-8',
         'LANG': 'de_DE.UTF-8',
         'VST_PATH': '/home/lex/.vst:/usr/lib/vst:/usr/local/lib/vst',
         'XDG_CURRENT_DESKTOP': 'KDE',
         'KONSOLE_DBUS_SERVICE': ':1.33',
         'COLUMNS': '80',
         'KONSOLE_DBUS_SESSION': '/Sessions/2',
         'PROFILEHOME': '',
         'CONDA_PROMPT_MODIFIER': '(tf) ',
         'XDG_SEAT_PATH': '/org/freedesktop/DisplayManager/Seat0',
         'KONSOLE_VERSION': '211202',
         'KDE_SESSION_UID': '1000',
         'XDG_SESSION_CLASS': 'user',
         'LC_IDENTIFICATION': 'de_DE.UTF-8',
         'TERM': 'xterm-color',
         '_CE_CONDA': '',
         'USER': 'lex',
         'COLORFGBG': '15;0',
         'CONDA_SHLVL': '2',
         'KDE_SESSION_VERSION': '5',
         'PAM_KWALLET5_LOGIN': '/run/user/1000/kwallet5.socket',
         'DISPLAY': ':0',
         'SHLVL': '2',
         'LC_TELEPHONE': 'de_DE.UTF-8',
         'LC_MEASUREMENT': 'de_DE.UTF-8',
         'XDG_VTNR': '1',
         'XDG_SESSION_ID': '2',
         'QT_LINUX_ACCESSIBILITY_ALWAYS_ON': '1',
         'CONDA_PYTHON_EXE': '/home/lex/Programme/miniconda3/bin/python',
         'MOZ_PLUGIN_PATH': '/usr/lib/mozilla/plugins',
         'XDG_RUNTIME_DIR': '/run/user/1000',
         'CONDA_DEFAULT_ENV': 'tf',
         'LC_TIME': 'de_DE.UTF-8',
         'QT_AUTO_SCREEN_SCALE_FACTOR': '0',
         'XCURSOR_THEME': 'breeze_cursors',
         'XDG_DATA_DIRS': '/home/lex/.local/share/flatpak/exports/share:/var/lib/flatpak/exports/share:/usr/local/share:/usr/share:/var/lib/snapd/desktop',
         'KDE_FULL_SESSION': 'true',
         'BROWSER': 'vivaldi-stable',
         'PATH': '/home/lex/Programme/miniconda3/envs/tf/bin:/home/lex/Programme/miniconda3/condabin:/home/lex/.local/bin:/bin:/usr/bin:/usr/local/bin:/usr/local/sbin:/usr/lib/jvm/default/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl:/var/lib/snapd/snap/bin:/home/lex/Programme/miniconda3/bin',
         'DBUS_SESSION_BUS_ADDRESS': 'unix:path=/run/user/1000/bus',
         'LV2_PATH': '/home/lex/.lv2:/usr/lib/lv2:/usr/local/lib/lv2',
         'KDE_APPLICATIONS_AS_SCOPE': '1',
         'MAIL': '/var/spool/mail/lex',
         'CONDA_PREFIX_1': '/home/lex/Programme/miniconda3',
         'LC_NUMERIC': 'de_DE.UTF-8',
         'LADSPA_PATH': '/home/lex/.ladspa:/usr/lib/ladspa:/usr/local/lib/ladspa',
         'CADENCE_AUTO_STARTED': 'true',
         '_': '/home/lex/Programme/miniconda3/envs/tf/bin/jupyter',
         'PYDEVD_USE_FRAME_EVAL': 'NO',
         'JPY_PARENT_PID': '172531',
         'CLICOLOR': '1',
         'PAGER': 'cat',
         'GIT_PAGER': 'cat',
         'MPLBACKEND': 'module://matplotlib_inline.backend_inline'}
      #+end_example
      : 521f8643-09b8-47dd-af7d-7f663d754c25
      :END:

   4. Branch out git branch =exp-220316-publication1= from =main= (done via
      magit) and make sure you are on the correct branch

      #+begin_src sh :session local2
        cd /home/lex/Programme/drmed-git
        git status
      #+end_src

      #+RESULTS:
      #+begin_example
        sh-5.1$ cd /home/lex/Programme/drmed-git
        sh-5.1$ git status
        On branch exp-220316-publication1
        Your branch is up to date with 'origin/exp-220316-publication1'.
      #+end_example

   5. Create experiment folder including the plot folder for jupyter plots
      #+begin_src sh :session local2
        mkdir -p ./data/exp-220316-publication1/jupyter
      #+end_src

      #+RESULTS:

   6. set output directory for matplotlib plots in jupyter
      #+begin_src emacs-lisp
        (setq org-babel-jupyter-resource-directory "./data/exp-220316-publication1/jupyter")
      #+end_src

      #+RESULTS:
      : ./data/exp-220316-publication1/jupyter

   7.
*** current git log
     #+BEGIN_SRC sh :session local2 :results org
       pwd
       git --no-pager log -5
     #+END_SRC

     #+RESULTS:
     #+begin_src org
     /home/lex/Programme/drmed-git
     commit d51b11eda090b9301e783ec35bdfd26c7bf0709c (HEAD -> exp-220316-publication1, origin/main, origin/exp-220316-publication1, origin/HEAD, main)
     Date:   Sun Feb 27 18:40:00 2022 +0100

         fix model input_size to None; else to crop_size

     commit c637444d8b798603629f6f0bd72ee55af7f81a5f
     Date:   Sun Feb 27 18:39:29 2022 +0100

         Fix function call correlate_and_fit

     commit 291c6619c12bc39d526137a43d976b3cb4881e50
     Date:   Sat Feb 26 20:04:07 2022 +0100

         Fix scale_trace; simplify tf_pad_trace call

     commit dcca8b9e17909a95b824c8a7b1fec52eeed198c3
     Date:   Thu Feb 24 16:11:39 2022 +0100

         test tf_pad_trace

     commit 6cf2da85748ef13f2e752bea8989a6d31549ced3
     Date:   Thu Feb 24 16:10:33 2022 +0100

         Fix tf_pad_trace
     #+end_src


*** Plot 1: simulation-prediction-correction pipeline
   :PROPERTIES:
   :header-args:jupyter-python: :session /jpy:localhost#8888:a37e524a-8134-4d8f-b24a-367acaf1bdd3
   :END:

   - to interprete the correlations correctly, let's plot the underlying
     experimental data.
     #+BEGIN_SRC jupyter-python
       %cd ~/Programme/drmed-git
     #+END_SRC

     #+RESULTS:
     : /home/lex/Programme/drmed-git

   - load modules
     #+BEGIN_SRC jupyter-python
       import logging
       import os
       import sys

       import matplotlib.pyplot as plt
       import numpy as np
       import pandas as pd
       import seaborn as sns

       from pathlib import Path
       from pprint import pprint
       from tensorflow.keras.optimizers import Adam
       from mlflow.keras import load_model

       FLUOTRACIFY_PATH = '/home/lex/Programme/drmed-git/src/'
       sys.path.append(FLUOTRACIFY_PATH)
       from fluotracify.applications import corr_fit_object as cfo
       from fluotracify.imports import ptu_utils as ptu
       from fluotracify.training import (build_model as bm,
                                         preprocess_data as ppd)
       from fluotracify.simulations import (
          import_simulation_from_csv as isfc,
          analyze_simulations as ans,
       )

       logging.basicConfig(filename="/home/lex/Programme/drmed-git/data/exp-220316-publication1/jupyter.log",
                           filemode='w', format='%(asctime)s - %(message)s',
                           force=True,
                           level=logging.DEBUG)

       sns.set_theme(style="whitegrid", font_scale=2, palette='colorblind',
                     context='paper')

       model_ls = ['ff67be0b68e540a9a29a36a2d0c7a5be', '347669d050f344ad9fb9e480c814f727',
                   '714af8cd12c1441eac4ca980e8c20070', '34a6d207ac594035b1009c330fb67a65']

       model_name_ls = ['ff67b', '34766', '714af', '34a6d']

       scaler_ls = ['minmax', 'robust', 'maxabs', 'l2']

       pred_thresh = 0.5

     #+END_SRC

     #+RESULTS:
     : 2022-04-12 14:56:03.263050: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
     : 2022-04-12 14:56:03.263176: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.

    #+BEGIN_SRC jupyter-python
      import importlib
      importlib.reload(ppd)
      importlib.reload(isfc)
      importlib.reload(cfo)
    #+END_SRC

    #+RESULTS:
    : <module 'fluotracify.applications.corr_fit_object' from '/home/lex/Programme/drmed-git/src/fluotracify/applications/corr_fit_object.py'>

*** Plot 1 part 1: simulated data
   :PROPERTIES:
   :header-args:jupyter-python: :session /jpy:localhost#8888:a37e524a-8134-4d8f-b24a-367acaf1bdd3
   :END:
   - load simulated data
     #+BEGIN_SRC jupyter-python :pandoc t
       col_per_example = 3
       lab_thresh = 0.04
       artifact = 0
       model_type = 1
       fwhm = 250
       sim_path = Path('/home/lex/Programme/drmed-collections/drmed-simexps/firstartifact_Nov2020_test')

       sim, _, nsamples, sim_params = isfc.import_from_csv(
           folder=sim_path,
           header=12,
           frac_train=1,
           col_per_example=col_per_example,
           dropindex=None,
           dropcolumns=None)

       diffrates = sim_params.loc['diffusion rate of molecules in micrometer^2 / s'].astype(np.float32)
       nmols = sim_params.loc['number of fast molecules'].astype(np.float32)
       clusters = sim_params.loc['diffusion rate of clusters in micrometer^2 / s'].astype(np.float32)
       sim_columns = [f'{d:.4}-{c:.4}' for d, c in zip(
           np.repeat(diffrates, nsamples[0]),
           np.repeat(clusters, nsamples[0]))]

       sim_sep = isfc.separate_data_and_labels(array=sim,
                                               nsamples=nsamples,
                                               col_per_example=col_per_example)
       sim_dirty = sim_sep['0']
       sim_dirty.columns = sim_columns

       sim_labels = sim_sep['1']
       sim_labels.columns = sim_columns
       sim_labbool = sim_labels > lab_thresh
       sim_labbool.columns = sim_columns
       sim_clean = sim_sep['2']
       sim_clean.columns = sim_columns

       sim_dirty
     #+END_SRC

     #+RESULTS:
     :RESULTS:
     |       | 1.0-0.01    | 1.0-0.01    | 1.0-0.01    | 1.0-0.01    | 1.0-0.01    | 1.0-0.01    | 1.0-0.01    | 1.0-0.01    | 1.0-0.01    | 1.0-0.01    | ... | 0.2-1.0     | 0.2-1.0     | 0.2-1.0     | 0.2-1.0     | 0.2-1.0     | 0.2-1.0     | 0.2-1.0     | 0.2-1.0     | 0.2-1.0     | 0.2-1.0     |
|-------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-----+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------|
| 0     | 1137.443359 | 1254.458618 | 1186.574097 | 637.306458  | 1521.684692 | 677.170532  | 1168.362671 | 1628.718506 | 861.776672  | 1243.531494 | ... | 2118.657471 | 2057.639404 | 2126.922119 | 2617.964111 | 2258.764160 | 1694.151733 | 2535.143555 | 1785.972900 | 1853.346924 | 1877.305298 |
| 1     | 1064.766479 | 1159.596069 | 1132.475220 | 655.323914  | 1631.872070 | 649.373230  | 1024.309570 | 1539.610596 | 691.026367  | 1204.659668 | ... | 2012.791870 | 2061.374268 | 2091.853516 | 2586.619141 | 2235.359863 | 1668.724854 | 2436.026855 | 1799.635376 | 1799.471191 | 1984.344727 |
| 2     | 1115.380615 | 1163.550903 | 1068.131348 | 545.208313  | 1413.081055 | 837.581787  | 840.185547  | 1512.560181 | 716.668457  | 1181.674438 | ... | 2006.173218 | 2163.941162 | 2082.030273 | 2646.836670 | 2196.885254 | 1640.936279 | 2431.697021 | 1876.269775 | 1787.451538 | 1962.721680 |
| 3     | 1030.869263 | 1301.564697 | 983.654480  | 606.174805  | 1435.475098 | 720.902344  | 1067.250610 | 1480.001465 | 716.686279  | 1196.109375 | ... | 2134.022217 | 2177.704102 | 1974.039673 | 2626.399170 | 2225.066895 | 1703.989258 | 2377.174805 | 1895.044922 | 1816.981689 | 2000.941528 |
| 4     | 1186.906616 | 1372.696045 | 996.091431  | 636.747498  | 1436.380249 | 762.980469  | 1070.283569 | 1540.874634 | 666.972290  | 1099.720581 | ... | 2044.460327 | 2236.866699 | 2008.952026 | 2597.411133 | 2286.010254 | 1804.218506 | 2429.243652 | 1947.826660 | 2091.532715 | 1986.765747 |
| ...   | ...         | ...         | ...         | ...         | ...         | ...         | ...         | ...         | ...         | ...         | ... | ...         | ...         | ...         | ...         | ...         | ...         | ...         | ...         | ...         | ...         |
| 16379 | 1188.258057 | 1123.808472 | 1324.048828 | 902.520935  | 661.328552  | 998.909485  | 1213.814697 | 1167.788574 | 3718.382568 | 891.315002  | ... | 1598.518677 | 2353.616699 | 2227.808350 | 1894.450806 | 2645.360352 | 2667.197021 | 1719.079224 | 1756.946533 | 1781.044312 | 2295.302734 |
| 16380 | 1275.702759 | 990.943237  | 1260.360107 | 961.117737  | 710.035889  | 919.583374  | 1208.030151 | 1169.737183 | 3433.009521 | 1090.649414 | ... | 1620.662964 | 2252.820801 | 2298.797607 | 1827.205933 | 2597.988770 | 3049.374023 | 1653.942993 | 1784.218140 | 1755.557007 | 2246.948730 |
| 16381 | 1262.139771 | 1006.014893 | 1240.076538 | 953.885193  | 758.210388  | 858.521240  | 1081.140625 | 1249.674683 | 3732.000000 | 1211.191162 | ... | 1560.511108 | 2228.376709 | 2438.944580 | 1899.014893 | 2542.006592 | 2174.580566 | 1628.585693 | 1828.769043 | 1669.530396 | 2234.492432 |
| 16382 | 1224.168945 | 1037.138672 | 1208.973633 | 956.065491  | 711.904297  | 1024.894653 | 958.588318  | 1180.880371 | 3712.922119 | 1279.076660 | ... | 1474.871338 | 2231.594971 | 2567.258545 | 1872.668457 | 2573.971191 | 1882.204590 | 1687.768677 | 1695.901855 | 1683.422363 | 2134.369629 |
| 16383 | 1217.611328 | 850.246277  | 1159.042236 | 1022.052185 | 658.546997  | 1143.581909 | 921.202820  | 1029.407349 | 3904.530762 | 1204.061890 | ... | 1408.810913 | 2243.148926 | 3569.665527 | 1926.308838 | 2456.016113 | 2522.900879 | 1738.134644 | 1684.547119 | 1698.406616 | 2192.371338 |

16384 rows × 1200 columns
     :END:


     #+BEGIN_SRC jupyter-python :pandoc t
       with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.max_seq_items', None):
           display(sim_pred.columns)
     #+END_SRC

     #+RESULTS:



   - plot simulated data with and without predictions
     #+BEGIN_SRC jupyter-python
       plot1_index = ['50.0-0.01', '50.0-0.1', '50.0-1.0',
                      '0.2-0.01', '0.2-0.1', '0.2-1.0',
                      '0.069-0.01', '0.069-0.1', '0.069-1.0']
       plot1_traceno = [1, 1, 0,
                        5, 0, 0,
                        1, 1, 0]
     #+END_SRC

     #+RESULTS:

     #+BEGIN_SRC jupyter-python :pandoc t
       plot1_titles = ['fast molecules and slow clusters:\nsimulations', 'fast molecules and medium clusters:\nsimulations', 'fast molecules and fast clusters:\nsimulations',
                       'medium molecules and slow clusters:\nsimulations', 'medium molecules and medium clusters:\nsimulations', 'medium molecules and fast clusters:\nsimulations',
                       'slow molecules and slow clusters:\nsimulations', 'slow molecules and medium clusters:\nsimulations', 'slow molecules and fast clusters:\nsimulations']

       for txt, idx, t in zip(plot1_titles, plot1_index, plot1_traceno):
           fig = plt.figure()
           ax = plt.subplot(111)
           ax.set_prop_cycle(color=[sns.color_palette()[0]])
           sns.lineplot(data=sim_dirty.loc[:, idx].iloc[:, t], label='trace')
           plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)
           plt.setp(ax, xlabel=r'Time [$ms$]', ylabel=r'Intensity [$a.u.$]',
             title=txt)
           plot1_file = f'data/exp-220316-publication1/jupyter/plot1_{txt}'.replace(' ', '_').replace('\n', '_')
           plt.savefig(f'{plot1_file}.pdf', bbox_inches='tight', dpi=300)
           os.system(f'pdf2svg {plot1_file}.pdf {plot1_file}.svg')
           os.system(f'rm {plot1_file}.pdf')

     #+END_SRC

     #+RESULTS:

     file:./data/exp-220316-publication1/jupyter/plot1_slow_molecules_and_slow_clusters:_simulations.svg
     file:./data/exp-220316-publication1/jupyter/plot1_slow_molecules_and_medium_clusters:_simulations.svg
     file:./data/exp-220316-publication1/jupyter/plot1_slow_molecules_and_fast_clusters:_simulations.svg
     file:./data/exp-220316-publication1/jupyter/plot1_medium_molecules_and_slow_clusters:_simulations.svg
     file:./data/exp-220316-publication1/jupyter/plot1_medium_molecules_and_medium_clusters:_simulations.svg
     file:./data/exp-220316-publication1/jupyter/plot1_medium_molecules_and_fast_clusters:_simulations.svg
     file:./data/exp-220316-publication1/jupyter/plot1_fast_molecules_and_slow_clusters:_simulations.svg
     file:./data/exp-220316-publication1/jupyter/plot1_fast_molecules_and_medium_clusters:_simulations.svg
     file:./data/exp-220316-publication1/jupyter/plot1_fast_molecules_and_fast_clusters:_simulations.svg


     #+BEGIN_SRC jupyter-python :pandoc t
       plot1_titles1 = ['fast molecules:\nslow cluster labels', 'fast molecules:\nmedium cluster labels', 'fast molecules:\nfast cluster labels',
                        'medium molecules:\nslow cluster labels', 'medium molecules:\nmedium cluster labels', 'medium molecules:\nfast cluster labels',
                        'slow molecules:\nslow cluster labels', 'slow molecules:\nmedium cluster labels', 'slow molecules:\nfast cluster labels']

       for txt, idx, t in zip(plot1_titles1, plot1_index, plot1_traceno):
           fig = plt.figure()
           ax = plt.subplot(111)
           ax.set_prop_cycle(color=[sns.color_palette()[4]])
           sns.lineplot(data=sim_labels.loc[:, idx].iloc[:, t], label='cluster\ntrace')
           plt.axhline(y=0.04, xmin=0, xmax=1, label='label\nthreshold',
                       color=sns.color_palette()[7], linestyle='--')
           plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)
           plt.setp(ax, xlabel=r'Time [$ms$]', ylabel=r'Intensity [$a.u.$]',
             title=txt)
           plot1_file = f'data/exp-220316-publication1/jupyter/plot1_{txt}'.replace(' ', '_').replace('\n', '_')

           plt.savefig(f'{plot1_file}.pdf', bbox_inches='tight', dpi=300)
           os.system(f'pdf2svg {plot1_file}.pdf {plot1_file}.svg')
           os.system(f'rm {plot1_file}.pdf')

     #+END_SRC

     #+RESULTS:


     file:./data/exp-220316-publication1/jupyter/plot1_slow_molecules:_slow_cluster_labels.svg
     file:./data/exp-220316-publication1/jupyter/plot1_slow_molecules:_medium_cluster_labels.svg
     file:./data/exp-220316-publication1/jupyter/plot1_slow_molecules:_fast_cluster_labels.svg
     file:./data/exp-220316-publication1/jupyter/plot1_medium_molecules:_slow_cluster_labels.svg
     file:./data/exp-220316-publication1/jupyter/plot1_medium_molecules:_medium_cluster_labels.svg
     file:./data/exp-220316-publication1/jupyter/plot1_medium_molecules:_fast_cluster_labels.svg
     file:./data/exp-220316-publication1/jupyter/plot1_fast_molecules:_slow_cluster_labels.svg
     file:./data/exp-220316-publication1/jupyter/plot1_fast_molecules:_medium_cluster_labels.svg
     file:./data/exp-220316-publication1/jupyter/plot1_fast_molecules:_fast_cluster_labels.svg


     #+BEGIN_SRC jupyter-python
       plot1_titles2 = ['fast molecules and slow clusters:\nlabel-based segmentation',
                        'fast molecules and medium clusters:\nlabel-based segmentation',
                        'fast molecules and fast clusters:\nlabel-based segmentation',
                        'medium molecules and slow clusters:\nlabel-based segmentation',
                        'medium molecules and medium clusters:\nlabel-based segmentation',
                        'medium molecules and fast clusters:\nlabel-based segmentation',
                        'slow molecules and slow clusters:\nlabel-based segmentation',
                        'slow molecules and medium clusters:\nlabel-based segmentation',
                        'slow molecules and fast clusters:\nlabel-based segmentation']


       for txt, idx, t in zip(plot1_titles2, plot1_index, plot1_traceno):
           fig = plt.figure()
           ax = plt.subplot(111)


           ax.set_prop_cycle(color=[sns.color_palette()[4]])
           sim_labbool_scaled = sim_dirty.loc[:, idx].iloc[
               :, t].max() * sim_labbool.loc[:, idx].iloc[:, t]
           sns.lineplot(data=sim_labbool_scaled, alpha=0.5)
           plt.fill_between(x=sim_labbool.loc[:, idx].iloc[:, t].index,
                            y1=sim_labbool_scaled,
                            y2=0, alpha=0.5, label='$label_{thr}=1$\n(clusters\ndominant)')

           ax.set_prop_cycle(color=[sns.color_palette()[2]])
           sim_invbool_scaled = sim_dirty.loc[:, idx].iloc[
               :, t].max() * ~sim_labbool.loc[:, idx].iloc[:, t]
           plt.fill_between(x=sim_labbool.loc[:, idx].iloc[:, t].index,
                            y1=sim_invbool_scaled,
                            y2=0, alpha=0.5, label='$label_{thr}=0$\n(molecules\ndominant)')
           ax.set_prop_cycle(color=[sns.color_palette()[0]])
           sns.lineplot(data=sim_dirty.loc[:, idx].iloc[:, t], label='trace')


           plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)
           plt.setp(ax, xlabel=r'Time [$ms$]', ylabel=r'Intensity [$a.u.$]',
             title=txt)
           plot1_file = f'data/exp-220316-publication1/jupyter/plot1_{txt}'.replace(
               ' ', '_').replace('\n', '_')
           plt.savefig(f'{plot1_file}.pdf', bbox_inches='tight', dpi=300)
           os.system(f'pdf2svg {plot1_file}.pdf {plot1_file}.svg')
           os.system(f'rm {plot1_file}.pdf')
     #+END_SRC

     #+RESULTS:


     file:./data/exp-220316-publication1/jupyter/plot1_slow_molecules_and_slow_clusters:_label-based_segmentation.svg
     file:./data/exp-220316-publication1/jupyter/plot1_slow_molecules_and_medium_clusters:_label-based_segmentation.svg
     file:./data/exp-220316-publication1/jupyter/plot1_slow_molecules_and_fast_clusters:_label-based_segmentation.svg
     file:./data/exp-220316-publication1/jupyter/plot1_medium_molecules_and_slow_clusters:_label-based_segmentation.svg
     file:./data/exp-220316-publication1/jupyter/plot1_medium_molecules_and_medium_clusters:_label-based_segmentation.svg
     file:./data/exp-220316-publication1/jupyter/plot1_medium_molecules_and_fast_clusters:_label-based_segmentation.svg
     file:./data/exp-220316-publication1/jupyter/plot1_fast_molecules_and_slow_clusters:_label-based_segmentation.svg
     file:./data/exp-220316-publication1/jupyter/plot1_fast_molecules_and_medium_clusters:_label-based_segmentation.svg
     file:./data/exp-220316-publication1/jupyter/plot1_fast_molecules_and_fast_clusters:_label-based_segmentation.svg


     #+BEGIN_SRC jupyter-python

       plot1_titles3 = ['fast molecules:\nslow cluster prediction',
                        'fast molecules:\nmedium cluster prediction',
                        'fast molecules:\nfast cluster prediction',
                        'medium molecules:\nslow cluster prediction',
                        'medium molecules:\nmedium cluster prediction',
                        'medium molecules:\nfast cluster prediction',
                        'slow molecules:\nslow cluster prediction',
                        'slow molecules:\nmedium cluster prediction',
                        'slow molecules:\nfast cluster prediction']

       plot1_titles4 = ['fast molecules and slow clusters:\nprediction-based segmentation',
                        'fast molecules and medium clusters:\nprediction-based segmentation',
                        'fast molecules and fast clusters:\nprediction-based segmentation',
                        'medium molecules and slow clusters:\nprediction-based segmentation',
                        'medium molecules and medium clusters:\nprediction-based segmentation',
                        'medium molecules and fast clusters:\nprediction-based segmentation',
                        'slow molecules and slow clusters:\nprediction-based segmentation',
                        'slow molecules and medium clusters:\nprediction-based segmentation',
                        'slow molecules and fast clusters:\nprediction-based segmentation']

       plot1_titles5 = ['fast molecules and slow clusters:\nprediction-based "cut and shift" correction',
                        'fast molecules and medium clusters:\nprediction-based "cut and shift" correction',
                        'fast molecules and fast clusters:\nprediction-based "cut and shift" correction',
                        'medium molecules and slow clusters:\nprediction-based "cut and shift" correction',
                        'medium molecules and medium clusters:\nprediction-based "cut and shift" correction',
                        'medium molecules and fast clusters:\nprediction-based "cut and shift" correction',
                        'slow molecules and slow clusters:\nprediction-based "cut and shift" correction',
                        'slow molecules and medium clusters:\nprediction-based "cut and shift" correction',
                        'slow molecules and fast clusters:\nprediction-based "cut and shift" correction']

       def plot_predictions(model_id, filename):
           def plot_cluster_prediction(filename):
               for txt, idx, t in zip(plot1_titles3, plot1_index, plot1_traceno):
                   fig = plt.figure()
                   ax = plt.subplot(111)
                   ax.set_prop_cycle(color=[sns.color_palette()[3]])
                   sns.lineplot(data=sim_pred.loc[:, idx].iloc[:, t],
                                label='prediction')
                   plt.axhline(y=pred_thresh, xmin=0, xmax=1,
                               label='prediction\nthreshold',
                               color=sns.color_palette()[7], linestyle='--')
                   plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)
                   plt.setp(ax, xlabel=r'Time [$ms$]', ylabel='artifact probability',
                     title=txt, ylim=[0, 1]
                   plot1_file) = f'{filename}_{txt}'.replace(' ', '_').replace('\n', '_')
                   plt.savefig(f'{plot1_file}.pdf', bbox_inches='tight', dpi=300)
                   os.system(f'pdf2svg {plot1_file}.pdf {plot1_file}.svg')
                   os.system(f'rm {plot1_file}.pdf')

           def plot_prediction_based_segmentation(filename):
               for txt, idx, t in zip(plot1_titles4, plot1_index, plot1_traceno):
                   fig = plt.figure()
                   ax = plt.subplot(111)


                   ax.set_prop_cycle(color=[sns.color_palette()[3]])
                   sim_predbool_scaled = sim_dirty.loc[:, idx].iloc[:, t].max() * sim_predbool.loc[:, idx].iloc[:, t]
                   sns.lineplot(data=sim_predbool_scaled, alpha=0.5)
                   plt.fill_between(x=sim_predbool.loc[:, idx].iloc[:, t].index,
                                    y1=sim_predbool_scaled,
                                    y2=0, alpha=0.5, label='$pred_{thr}=1$\n(clusters\ndominant)')

                   ax.set_prop_cycle(color=[sns.color_palette()[2]])
                   sim_invpred_scaled = sim_dirty.loc[:, idx].iloc[:, t].max() * ~sim_predbool.loc[:, idx].iloc[:, t]
                   plt.fill_between(x=sim_predbool.loc[:, idx].iloc[:, t].index,
                                    y1=sim_invpred_scaled,
                                    y2=0, alpha=0.5, label='$pred_{thr}=0$\n(molecules\ndominant)')
                   ax.set_prop_cycle(color=[sns.color_palette()[0]])
                   sns.lineplot(data=sim_dirty.loc[:, idx].iloc[:, t], label='trace')
                   plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)
                   plt.setp(ax, xlabel=r'Time [$ms$]', ylabel=r'Intensity [$a.u.$]',
                     title=txt)
                   plot1_file = f'{filename}_{txt}'.replace(' ', '_').replace('\n', '_')
                   plt.savefig(f'{plot1_file}.pdf', bbox_inches='tight', dpi=300)
                   os.system(f'pdf2svg {plot1_file}.pdf {plot1_file}.svg')
                   os.system(f'rm {plot1_file}.pdf')

           def plot_prediction_based_cut_and_shift_correction(filename):
               for txt, idx, t in zip(plot1_titles5, plot1_index, plot1_traceno):
                   fig = plt.figure()
                   ax = plt.subplot(111)
                   ax.set_prop_cycle(color=[sns.color_palette()[0]])
                   sns.lineplot(data=sim_corr.loc[:, idx].iloc[:, t], label='trace')
                   plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)
                   plt.setp(ax, xlabel=r'Time [$ms$]', ylabel=r'Intensity [$a.u.$]',
                     title=txt)
                   plot1_file = f'{filename}_{txt}'.replace(' ', '_').replace(
                       '\n', '_').replace('"', '')
                   plt.savefig(f'{plot1_file}.pdf', bbox_inches='tight', dpi=300)
                   os.system(f'pdf2svg {plot1_file}.pdf {plot1_file}.svg')
                   os.system(f'rm {plot1_file}.pdf')

           logged_model = Path(f'/home/lex/Programme/drmed-git/data/mlruns/10/{model_ls[model_id]}/artifacts/model')
           loaded_model = load_model(logged_model, compile=False)
           loaded_model.compile(loss=bm.binary_ce_dice_loss(),
                                optimizer=Adam(),
                                metrics = bm.unet_metrics([0.1, 0.3, 0.5, 0.7, 0.9]))

           sim_dirty_prepro = ppd.convert_to_tfds_for_unet(sim_dirty)
           sim_dirty_prepro = ppd.scale_pad_and_batch_tfds_for_unet(
               sim_dirty_prepro, scaler=scaler_ls[model_id])
           sim_pred = loaded_model.predict(sim_dirty_prepro, verbose=0)
           sim_pred = pd.DataFrame(sim_pred.squeeze(axis=2)).T
           sim_pred.columns = sim_columns
           sim_predbool = sim_pred > pred_thresh

           sim_corr = pd.DataFrame()
           for i in range(len(sim_dirty.columns)):
               sim_corr_trace = np.delete(sim_dirty.iloc[:, i].values,
                                          sim_predbool.iloc[:, i].values)
               sim_corr_trace = pd.DataFrame(sim_corr_trace)
               sim_corr = pd.concat([sim_corr, sim_corr_trace], axis='columns')

           sim_corr.columns = sim_columns

           plot_cluster_prediction(filename)
           plot_prediction_based_segmentation(filename)
           plot_prediction_based_cut_and_shift_correction(filename)
           plt.close('all')

     #+END_SRC

     #+RESULTS:

   - first model: =ff67be0b68e540a9a29a36a2d0c7a5be=
     #+BEGIN_SRC jupyter-python
       plot_predictions(model_id=0,
                        filename='data/exp-220316-publication1/jupyter/plot1_ff67b')
     #+END_SRC

     #+RESULTS:

     file:./data/exp-220316-publication1/jupyter/plot1_ff67b_slow_molecules:_slow_cluster_prediction.svg
     file:./data/exp-220316-publication1/jupyter/plot1_ff67b_slow_molecules:_medium_cluster_prediction.svg
     file:./data/exp-220316-publication1/jupyter/plot1_ff67b_slow_molecules:_fast_cluster_prediction.svg
     file:./data/exp-220316-publication1/jupyter/plot1_ff67b_medium_molecules:_slow_cluster_prediction.svg
     file:./data/exp-220316-publication1/jupyter/plot1_ff67b_medium_molecules:_medium_cluster_prediction.svg
     file:./data/exp-220316-publication1/jupyter/plot1_ff67b_medium_molecules:_fast_cluster_prediction.svg
     file:./data/exp-220316-publication1/jupyter/plot1_ff67b_fast_molecules:_slow_cluster_prediction.svg
     file:./data/exp-220316-publication1/jupyter/plot1_ff67b_fast_molecules:_medium_cluster_prediction.svg
     file:./data/exp-220316-publication1/jupyter/plot1_ff67b_fast_molecules:_fast_cluster_prediction.svg

     file:./data/exp-220316-publication1/jupyter/plot1_ff67b_slow_molecules_and_slow_clusters:_prediction-based_segmentation.svg
     file:./data/exp-220316-publication1/jupyter/plot1_ff67b_slow_molecules_and_medium_clusters:_prediction-based_segmentation.svg
     file:./data/exp-220316-publication1/jupyter/plot1_ff67b_slow_molecules_and_fast_clusters:_prediction-based_segmentation.svg
     file:./data/exp-220316-publication1/jupyter/plot1_ff67b_medium_molecules_and_slow_clusters:_prediction-based_segmentation.svg
     file:./data/exp-220316-publication1/jupyter/plot1_ff67b_medium_molecules_and_medium_clusters:_prediction-based_segmentation.svg
     file:./data/exp-220316-publication1/jupyter/plot1_ff67b_medium_molecules_and_fast_clusters:_prediction-based_segmentation.svg
     file:./data/exp-220316-publication1/jupyter/plot1_ff67b_fast_molecules_and_slow_clusters:_prediction-based_segmentation.svg
     file:./data/exp-220316-publication1/jupyter/plot1_ff67b_fast_molecules_and_medium_clusters:_prediction-based_segmentation.svg
     file:./data/exp-220316-publication1/jupyter/plot1_ff67b_fast_molecules_and_fast_clusters:_prediction-based_segmentation.svg

     file:./data/exp-220316-publication1/jupyter/plot1_ff67b_slow_molecules_and_slow_clusters:_prediction-based_cut_and_shift_correction.svg
     file:./data/exp-220316-publication1/jupyter/plot1_ff67b_slow_molecules_and_medium_clusters:_prediction-based_cut_and_shift_correction.svg
     file:./data/exp-220316-publication1/jupyter/plot1_ff67b_slow_molecules_and_fast_clusters:_prediction-based_cut_and_shift_correction.svg
     file:./data/exp-220316-publication1/jupyter/plot1_ff67b_medium_molecules_and_slow_clusters:_prediction-based_cut_and_shift_correction.svg
     file:./data/exp-220316-publication1/jupyter/plot1_ff67b_medium_molecules_and_medium_clusters:_prediction-based_cut_and_shift_correction.svg
     file:./data/exp-220316-publication1/jupyter/plot1_ff67b_medium_molecules_and_fast_clusters:_prediction-based_cut_and_shift_correction.svg
     file:./data/exp-220316-publication1/jupyter/plot1_ff67b_fast_molecules_and_slow_clusters:_prediction-based_cut_and_shift_correction.svg
     file:./data/exp-220316-publication1/jupyter/plot1_ff67b_fast_molecules_and_medium_clusters:_prediction-based_cut_and_shift_correction.svg
     file:./data/exp-220316-publication1/jupyter/plot1_ff67b_fast_molecules_and_fast_clusters:_prediction-based_cut_and_shift_correction.svg


   - second model: =347669d050f344ad9fb9e480c814f727=
     #+BEGIN_SRC jupyter-python
       plot_predictions(model_id=1,
                        filename='data/exp-220316-publication1/jupyter/plot1_34766')

     #+END_SRC

     #+RESULTS:
     : WARNING:tensorflow:Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.
     : /tmp/ipykernel_5495/3600776436.py:87: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
     :   fig = plt.figure()



     #+RESULTS:

     file:./data/exp-220316-publication1/jupyter/plot1_34766_slow_molecules:_slow_cluster_prediction.svg
     file:./data/exp-220316-publication1/jupyter/plot1_34766_slow_molecules:_medium_cluster_prediction.svg
     file:./data/exp-220316-publication1/jupyter/plot1_34766_slow_molecules:_fast_cluster_prediction.svg
     file:./data/exp-220316-publication1/jupyter/plot1_34766_medium_molecules:_slow_cluster_prediction.svg
     file:./data/exp-220316-publication1/jupyter/plot1_34766_medium_molecules:_medium_cluster_prediction.svg
     file:./data/exp-220316-publication1/jupyter/plot1_34766_medium_molecules:_fast_cluster_prediction.svg
     file:./data/exp-220316-publication1/jupyter/plot1_34766_fast_molecules:_slow_cluster_prediction.svg
     file:./data/exp-220316-publication1/jupyter/plot1_34766_fast_molecules:_medium_cluster_prediction.svg
     file:./data/exp-220316-publication1/jupyter/plot1_34766_fast_molecules:_fast_cluster_prediction.svg

     file:./data/exp-220316-publication1/jupyter/plot1_34766_slow_molecules_and_slow_clusters:_prediction-based_segmentation.svg
     file:./data/exp-220316-publication1/jupyter/plot1_34766_slow_molecules_and_medium_clusters:_prediction-based_segmentation.svg
     file:./data/exp-220316-publication1/jupyter/plot1_34766_slow_molecules_and_fast_clusters:_prediction-based_segmentation.svg
     file:./data/exp-220316-publication1/jupyter/plot1_34766_medium_molecules_and_slow_clusters:_prediction-based_segmentation.svg
     file:./data/exp-220316-publication1/jupyter/plot1_34766_medium_molecules_and_medium_clusters:_prediction-based_segmentation.svg
     file:./data/exp-220316-publication1/jupyter/plot1_34766_medium_molecules_and_fast_clusters:_prediction-based_segmentation.svg
     file:./data/exp-220316-publication1/jupyter/plot1_34766_fast_molecules_and_slow_clusters:_prediction-based_segmentation.svg
     file:./data/exp-220316-publication1/jupyter/plot1_34766_fast_molecules_and_medium_clusters:_prediction-based_segmentation.svg
     file:./data/exp-220316-publication1/jupyter/plot1_34766_fast_molecules_and_fast_clusters:_prediction-based_segmentation.svg

     file:./data/exp-220316-publication1/jupyter/plot1_34766_slow_molecules_and_slow_clusters:_prediction-based_cut_and_shift_correction.svg
     file:./data/exp-220316-publication1/jupyter/plot1_34766_slow_molecules_and_medium_clusters:_prediction-based_cut_and_shift_correction.svg
     file:./data/exp-220316-publication1/jupyter/plot1_34766_slow_molecules_and_fast_clusters:_prediction-based_cut_and_shift_correction.svg
     file:./data/exp-220316-publication1/jupyter/plot1_34766_medium_molecules_and_slow_clusters:_prediction-based_cut_and_shift_correction.svg
     file:./data/exp-220316-publication1/jupyter/plot1_34766_medium_molecules_and_medium_clusters:_prediction-based_cut_and_shift_correction.svg
     file:./data/exp-220316-publication1/jupyter/plot1_34766_medium_molecules_and_fast_clusters:_prediction-based_cut_and_shift_correction.svg
     file:./data/exp-220316-publication1/jupyter/plot1_34766_fast_molecules_and_slow_clusters:_prediction-based_cut_and_shift_correction.svg
     file:./data/exp-220316-publication1/jupyter/plot1_34766_fast_molecules_and_medium_clusters:_prediction-based_cut_and_shift_correction.svg
     file:./data/exp-220316-publication1/jupyter/plot1_34766_fast_molecules_and_fast_clusters:_prediction-based_cut_and_shift_correction.svg


   - third model: =714af8cd12c1441eac4ca980e8c20070=
     #+BEGIN_SRC jupyter-python
       plot_predictions(model_id=2,
                        filename='data/exp-220316-publication1/jupyter/plot1_714af')
     #+END_SRC

     #+RESULTS:
     :   fig = plt.figure()
     : WARNING:tensorflow:Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.
     : /tmp/ipykernel_5495/3600776436.py:87: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
     :   fig = plt.figure()







     file:./data/exp-220316-publication1/jupyter/plot1_714af_slow_molecules:_slow_cluster_prediction.svg
     file:./data/exp-220316-publication1/jupyter/plot1_714af_slow_molecules:_medium_cluster_prediction.svg
     file:./data/exp-220316-publication1/jupyter/plot1_714af_slow_molecules:_fast_cluster_prediction.svg
     file:./data/exp-220316-publication1/jupyter/plot1_714af_medium_molecules:_slow_cluster_prediction.svg
     file:./data/exp-220316-publication1/jupyter/plot1_714af_medium_molecules:_medium_cluster_prediction.svg
     file:./data/exp-220316-publication1/jupyter/plot1_714af_medium_molecules:_fast_cluster_prediction.svg
     file:./data/exp-220316-publication1/jupyter/plot1_714af_fast_molecules:_slow_cluster_prediction.svg
     file:./data/exp-220316-publication1/jupyter/plot1_714af_fast_molecules:_medium_cluster_prediction.svg
     file:./data/exp-220316-publication1/jupyter/plot1_714af_fast_molecules:_fast_cluster_prediction.svg

     file:./data/exp-220316-publication1/jupyter/plot1_714af_slow_molecules_and_slow_clusters:_prediction-based_segmentation.svg
     file:./data/exp-220316-publication1/jupyter/plot1_714af_slow_molecules_and_medium_clusters:_prediction-based_segmentation.svg
     file:./data/exp-220316-publication1/jupyter/plot1_714af_slow_molecules_and_fast_clusters:_prediction-based_segmentation.svg
     file:./data/exp-220316-publication1/jupyter/plot1_714af_medium_molecules_and_slow_clusters:_prediction-based_segmentation.svg
     file:./data/exp-220316-publication1/jupyter/plot1_714af_medium_molecules_and_medium_clusters:_prediction-based_segmentation.svg
     file:./data/exp-220316-publication1/jupyter/plot1_714af_medium_molecules_and_fast_clusters:_prediction-based_segmentation.svg
     file:./data/exp-220316-publication1/jupyter/plot1_714af_fast_molecules_and_slow_clusters:_prediction-based_segmentation.svg
     file:./data/exp-220316-publication1/jupyter/plot1_714af_fast_molecules_and_medium_clusters:_prediction-based_segmentation.svg
     file:./data/exp-220316-publication1/jupyter/plot1_714af_fast_molecules_and_fast_clusters:_prediction-based_segmentation.svg

     file:./data/exp-220316-publication1/jupyter/plot1_714af_slow_molecules_and_slow_clusters:_prediction-based_cut_and_shift_correction.svg
     file:./data/exp-220316-publication1/jupyter/plot1_714af_slow_molecules_and_medium_clusters:_prediction-based_cut_and_shift_correction.svg
     file:./data/exp-220316-publication1/jupyter/plot1_714af_slow_molecules_and_fast_clusters:_prediction-based_cut_and_shift_correction.svg
     file:./data/exp-220316-publication1/jupyter/plot1_714af_medium_molecules_and_slow_clusters:_prediction-based_cut_and_shift_correction.svg
     file:./data/exp-220316-publication1/jupyter/plot1_714af_medium_molecules_and_medium_clusters:_prediction-based_cut_and_shift_correction.svg
     file:./data/exp-220316-publication1/jupyter/plot1_714af_medium_molecules_and_fast_clusters:_prediction-based_cut_and_shift_correction.svg
     file:./data/exp-220316-publication1/jupyter/plot1_714af_fast_molecules_and_slow_clusters:_prediction-based_cut_and_shift_correction.svg
     file:./data/exp-220316-publication1/jupyter/plot1_714af_fast_molecules_and_medium_clusters:_prediction-based_cut_and_shift_correction.svg
     file:./data/exp-220316-publication1/jupyter/plot1_714af_fast_molecules_and_fast_clusters:_prediction-based_cut_and_shift_correction.svg

   - fourth model: =34a6d207ac594035b1009c330fb67a65=
     #+BEGIN_SRC jupyter-python
       plot_predictions(model_id=3,
                        filename='data/exp-220316-publication1/jupyter/plot1_34a6d')

     #+END_SRC

     #+RESULTS:
     : WARNING:tensorflow:Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.
     : /tmp/ipykernel_5495/3600776436.py:87: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
     :   fig = plt.figure()

     file:./data/exp-220316-publication1/jupyter/plot1_34a6d_slow_molecules:_slow_cluster_prediction.svg
     file:./data/exp-220316-publication1/jupyter/plot1_34a6d_slow_molecules:_medium_cluster_prediction.svg
     file:./data/exp-220316-publication1/jupyter/plot1_34a6d_slow_molecules:_fast_cluster_prediction.svg
     file:./data/exp-220316-publication1/jupyter/plot1_34a6d_medium_molecules:_slow_cluster_prediction.svg
     file:./data/exp-220316-publication1/jupyter/plot1_34a6d_medium_molecules:_medium_cluster_prediction.svg
     file:./data/exp-220316-publication1/jupyter/plot1_34a6d_medium_molecules:_fast_cluster_prediction.svg
     file:./data/exp-220316-publication1/jupyter/plot1_34a6d_fast_molecules:_slow_cluster_prediction.svg
     file:./data/exp-220316-publication1/jupyter/plot1_34a6d_fast_molecules:_medium_cluster_prediction.svg
     file:./data/exp-220316-publication1/jupyter/plot1_34a6d_fast_molecules:_fast_cluster_prediction.svg


     file:./data/exp-220316-publication1/jupyter/plot1_34a6d_slow_molecules_and_slow_clusters:_prediction-based_segmentation.svg
     file:./data/exp-220316-publication1/jupyter/plot1_34a6d_slow_molecules_and_medium_clusters:_prediction-based_segmentation.svg
     file:./data/exp-220316-publication1/jupyter/plot1_34a6d_slow_molecules_and_fast_clusters:_prediction-based_segmentation.svg
     file:./data/exp-220316-publication1/jupyter/plot1_34a6d_medium_molecules_and_slow_clusters:_prediction-based_segmentation.svg
     file:./data/exp-220316-publication1/jupyter/plot1_34a6d_medium_molecules_and_medium_clusters:_prediction-based_segmentation.svg
     file:./data/exp-220316-publication1/jupyter/plot1_34a6d_medium_molecules_and_fast_clusters:_prediction-based_segmentation.svg
     file:./data/exp-220316-publication1/jupyter/plot1_34a6d_fast_molecules_and_slow_clusters:_prediction-based_segmentation.svg
     file:./data/exp-220316-publication1/jupyter/plot1_34a6d_fast_molecules_and_medium_clusters:_prediction-based_segmentation.svg
     file:./data/exp-220316-publication1/jupyter/plot1_34a6d_fast_molecules_and_fast_clusters:_prediction-based_segmentation.svg


     file:./data/exp-220316-publication1/jupyter/plot1_34a6d_slow_molecules_and_slow_clusters:_prediction-based_cut_and_shift_correction.svg
     file:./data/exp-220316-publication1/jupyter/plot1_34a6d_slow_molecules_and_medium_clusters:_prediction-based_cut_and_shift_correction.svg
     file:./data/exp-220316-publication1/jupyter/plot1_34a6d_slow_molecules_and_fast_clusters:_prediction-based_cut_and_shift_correction.svg
     file:./data/exp-220316-publication1/jupyter/plot1_34a6d_medium_molecules_and_slow_clusters:_prediction-based_cut_and_shift_correction.svg
     file:./data/exp-220316-publication1/jupyter/plot1_34a6d_medium_molecules_and_medium_clusters:_prediction-based_cut_and_shift_correction.svg
     file:./data/exp-220316-publication1/jupyter/plot1_34a6d_medium_molecules_and_fast_clusters:_prediction-based_cut_and_shift_correction.svg
     file:./data/exp-220316-publication1/jupyter/plot1_34a6d_fast_molecules_and_slow_clusters:_prediction-based_cut_and_shift_correction.svg
     file:./data/exp-220316-publication1/jupyter/plot1_34a6d_fast_molecules_and_medium_clusters:_prediction-based_cut_and_shift_correction.svg
     file:./data/exp-220316-publication1/jupyter/plot1_34a6d_fast_molecules_and_fast_clusters:_prediction-based_cut_and_shift_correction.svg

   - plot correlations, then fit with =focus-fit-js=
     #+BEGIN_SRC jupyter-python :pandoc t
       # sns.lineplot(data=sim_pred.loc[:, idx].iloc[:, 0], label='trace')
       # sim_dirty.drop(sim_predbool, axis='index')





       corr_fn = multipletau.autocorrelate(
                a=self.timeSeries[f'{name[0]}'][f'{name[1]}'],
                m=16,
                deltat=self.photonCountBin[f'{name[0]}'],
                normalize=True)


     #+END_SRC

     #+RESULTS:

     [[file:./data/exp-220316-publication1/jupyter/edf056e7749bf85f1d2b663fea4e3e33b4ece5f7.png]]

*** Plot 1 part 2: biological data
   :PROPERTIES:
   :header-args:jupyter-python: :session /jpy:localhost#8888:a37e524a-8134-4d8f-b24a-367acaf1bdd3 :pandoc t
   :END:
   - first, we prepare our correction functions as we did before
     #+BEGIN_SRC jupyter-python
       data_path = Path("/home/lex/Programme/drmed-collections/drmed-bioexps/brightbursts/1911DD_alexafluor488+LUVs")
       path_clean = data_path / 'clean_subsample/'
       path_dirty = data_path / 'dirty_subsample/'
       output_path = Path("data/exp-220316-publication1/220323_bioexps")

       def get_traces_and_predictions_from_ptu(path, model_id, output_path):
           class ParameterClass():
               """Stores parameters for correlation """
               def __init__(self):
                   self.data = []
                   self.objectRef = []
                   self.numOfLoaded = 0
                   self.colors = ['blue', 'green', 'red', 'cyan', 'magenta',
                       'yellow', 'black']
                   # very fast from Ncasc ~ 14 onwards
                   self.NcascStart = 0
                   self.NcascEnd = 30  # 25
                   self.Nsub = 6  # 6
                   self.photonLifetimeBin = 10  # used for photon decay
                   self.photonCountBin = 1  # used for time series

           par_obj = ParameterClass()

           scaler = scaler_ls[model_id]
           logged_model = Path(f'/home/lex/Programme/drmed-git/data/mlruns/10/{model_ls[model_id]}/artifacts/model')
           loaded_model = load_model(logged_model, compile=False)
           loaded_model.compile(loss=bm.binary_ce_dice_loss(),
                                optimizer=Adam(),
                                metrics = bm.unet_metrics([0.1, 0.3, 0.5, 0.7, 0.9]))

           files = [path / f for f in os.listdir(path) if f.endswith('.ptu')]
           traces = pd.DataFrame()
           predtraces = pd.DataFrame()
           preds = pd.DataFrame()
           corrtraces = pd.DataFrame()


           for myfile in (files):
               ptufile = cfo.PicoObject(myfile, par_obj)
               break
               ptufile.predictTimeSeries(model=loaded_model,
                                         scaler=scaler)
               ptufile.correctTCSPC(method='delete_and_shift')
               for key in list(ptufile.trueTimeArr.keys()):
                   ptufile.get_autocorrelation(method='tttr2xfcs', name=key)
               for key in list(ptufile.timeSeries.keys()):
                   if "DELSHIFT" in key:
                       for k, i in ptufile.timeSeries[key].items():
                           if "1.0" in k:
                               corrtraces = pd.concat([corrtraces, pd.DataFrame(
                                   i, columns=[f'{key}_{k}'])],
                                                      axis='columns')
                   else:
                       for k, i in ptufile.timeSeries[key].items():
                           if "PREPRO" in k:
                               if "1.0" in k:
                                   predtraces = pd.concat([predtraces, pd.DataFrame(
                                       i, columns=[f'{key}_{k}'])],
                                                          axis='columns')
                           elif "1.0" in k:
                               traces = pd.concat([traces, pd.DataFrame(
                                   i, columns=[f'{key}_{k}'])],
                                                  axis='columns')
                               preds = pd.concat([preds, pd.DataFrame(
                                   data=ptufile.predictions[key][k],
                                   columns=[f'{key}_{k}'])],
                                                 axis='columns')

               for m in ['multipletau', 'tttr2xfcs', 'tttr2xfcs_with_weights']:
                   if m in list(ptufile.autoNorm.keys()):
                       for key, item in list(ptufile.autoNorm[m].items()):
                           ptufile.save_autocorrelation(name=key, method=m,
                                                        output_path=output_path)
           predtraces.to_csv(Path(output_path) / f'{path.name}_predtraces.csv')
           traces.to_csv(Path(output_path) / f'{path.name}_traces.csv')
           preds.to_csv(Path(output_path) / f'{path.name}_preds.csv')
           corrtraces.to_csv(Path(output_path) / f'{path.name}_corrtraces.csv')

     #+END_SRC

     #+RESULTS:

     #+BEGIN_SRC jupyter-python
       model_id = 0
       ptufile = get_traces_and_predictions_from_ptu(path_dirty, model_id, output_path)
     #+END_SRC

     #+RESULTS:
     : WARNING:tensorflow:Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.

     #+BEGIN_SRC jupyter-python
       model_id = 0
       get_traces_and_predictions_from_ptu(path_clean, model_id, output_path)
     #+END_SRC

     #+RESULTS:
     : WARNING:tensorflow:Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.


     #+BEGIN_SRC jupyter-python
       key = 'DiO LUV 10uM in 20 nM AF48889_T1067s_1'
       key2 = 'DiO LUV 10uM in 20 nM AF48889_T1067s_1_DELSHIFT'
       tt_key = 'CH2_DiO LUV 10uM in 20 nM AF48889_T1067s_1_CORRECTED'
       ts_key = 'CH2_BIN1.0'
       ts = ptufile.timeSeries[key2][ts_key]
       # ptufile.predictions['20 nM AF48897_T1160s_1']['CH2_BIN1.0']
       ptufile.timeSeriesSize[key2][ts_key]
       pprint(ptufile.trueTimeArr.keys())
       pprint(ptufile.kcount.keys())
       pprint(ptufile.timeSeries.keys())
       pprint(ptufile.autoNorm['tttr2xfcs'].keys())
       for n in ptufile.trueTimeArr.keys():
           if 'DiO LUV 10uM in 20 nM AF48889_T1067s_1_DELSHIFT' in n:
               print(n)
     #+END_SRC

     #+RESULTS:
     : dict_keys(['DiO LUV 10uM in 20 nM AF48889_T1067s_1', 'CH2_DiO LUV 10uM in 20 nM AF48889_T1067s_1_DELSHIFT'])
     : dict_keys(['DiO LUV 10uM in 20 nM AF48889_T1067s_1', 'DiO LUV 10uM in 20 nM AF48889_T1067s_1_DELSHIFT'])
     : dict_keys(['DiO LUV 10uM in 20 nM AF48889_T1067s_1', 'DiO LUV 10uM in 20 nM AF48889_T1067s_1_DELSHIFT'])
     : dict_keys(['CH2_BIN1.0_DiO LUV 10uM in 20 nM AF48889_T1067s_1', 'CH2_BIN1.0_DiO LUV 10uM in 20 nM AF48889_T1067s_1_DELSHIFT'])
     : CH2_DiO LUV 10uM in 20 nM AF48889_T1067s_1_DELSHIFT
     'CH2_DiO LUV 10uM in 20 nM AF48889_T1067s_1_CORRECTED'])
     : dict_keys(['DiO LUV 10uM in 20 nM AF48889_T1067s_1',
      'DiO LUV 10uM in 20 nM AF48889_T1067s_1_DELSHIFT'])

     #+BEGIN_SRC jupyter-python
       corr_clean = pd.read_csv(Path(output_path) / 'clean_nocorrection_3D-AR5_1spec_rawFitData.csv',
                                index_col=0, usecols=[0, 1, 3, 5], na_values=' ')
       fit_clean = pd.read_csv(Path(output_path) / 'clean_nocorrection_3D-AR5_1spec_rawFitData.csv',
                               index_col=0, usecols=[0, 2, 4, 6], na_values=' ')
       param_clean = pd.read_csv(Path(output_path) / 'clean_nocorrection_3D-AR5_1spec_outputParam.csv',
                                 index_col=0)

       preds_clean = pd.read_csv(Path(output_path) / 'clean_subsample_preds.csv', index_col=0)
       predtraces_clean = pd.read_csv(Path(output_path) / 'clean_subsample_predtraces.csv', index_col=0)
       traces_clean = pd.read_csv(Path(output_path) / 'clean_subsample_traces.csv', index_col=0)
       corrtraces_clean = pd.read_csv(Path(output_path) / 'clean_subsample_corrtraces.csv', index_col=0)

       corr_noc_dirty = pd.read_csv(Path(output_path) / 'dirty_nocorrection_3D-AR5_2spec_rawFitData.csv',
                                index_col=0, usecols=[0, 1, 3, 5], na_values=' ')
       fit_noc_dirty = pd.read_csv(Path(output_path) / 'dirty_nocorrection_3D-AR5_2spec_rawFitData.csv',
                               index_col=0, usecols=[0, 2, 4, 6], na_values=' ')
       param_noc_dirty = pd.read_csv(Path(output_path) / 'dirty_nocorrection_3D-AR5_2spec_outputParam.csv',
                                     index_col=0)
       corr_cas_dirty = pd.read_csv(Path(output_path) / 'dirty_cutandshift_3D-AR5_2spec_rawFitData.csv',
                                index_col=0, usecols=[0, 1, 3, 5], na_values=' ')
       fit_cas_dirty = pd.read_csv(Path(output_path) / 'dirty_cutandshift_3D-AR5_2spec_rawFitData.csv',
                               index_col=0, usecols=[0, 2, 4, 6], na_values=' ')
       param_cas_dirty = pd.read_csv(Path(output_path) / 'dirty_cutandshift_3D-AR5_2spec_outputParam.csv',
                                index_col=0)

       preds_dirty = pd.read_csv(Path(output_path) / 'dirty_subsample_preds.csv', index_col=0)
       predtraces_dirty = pd.read_csv(Path(output_path) / 'dirty_subsample_predtraces.csv', index_col=0)
       traces_dirty = pd.read_csv(Path(output_path) / 'dirty_subsample_traces.csv', index_col=0)
       corrtraces_dirty = pd.read_csv(Path(output_path) / 'dirty_subsample_corrtraces.csv', index_col=0)
     #+END_SRC

     #+RESULTS:

     #+BEGIN_SRC jupyter-python
       def plot_bio_traces(df, txt):
           for i, t in enumerate(df.items()):
               t = t[1]
               fig = plt.figure()
               ax = plt.subplot(111)
               ax.set_prop_cycle(color=[sns.color_palette()[0]])
               sns.lineplot(data=t, label='trace')
               plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)
               plt.setp(ax, xlabel=r'Time [$ms$]', ylabel=r'Intensity [$a.u.$]',
                 title=txt)
               plot1_file = f'data/exp-220316-publication1/jupyter/plot1_{txt}_{i}'.replace(
                   ' ', '_').replace('\n', '_').replace('(', '').replace(')', '').replace('"', '')
               plt.savefig(f'{plot1_file}.pdf', bbox_inches='tight', dpi=300)
               os.system(f'pdf2svg {plot1_file}.pdf {plot1_file}.svg')
               os.system(f'rm {plot1_file}.pdf')
           plt.close('all')

       def plot_bio_prediction_based_cut_and_shift_correction(df, txt):
           for i, t in enumerate(df.items()):
               t = t[1]
               fig = plt.figure()
               ax = plt.subplot(111)
               ax.set_prop_cycle(color=[sns.color_palette()[0]])
               sns.lineplot(data=t, label='trace')
               plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)
               plt.setp(ax, xlabel=r'Time [$ms$]', ylabel=r'Intensity [$a.u.$]',
                 title=txt)
               break
               plot1_file = f'{filename}_{txt}'.replace(' ', '_').replace(
                   '\n', '_').replace('"', '')
               plt.savefig(f'{plot1_file}.pdf', bbox_inches='tight', dpi=300)
               os.system(f'pdf2svg {plot1_file}.pdf {plot1_file}.svg')
               os.system(f'rm {plot1_file}.pdf')

       def plot_bio_cluster_prediction(df, txt):
           for i, col in enumerate(df.columns):
               fig = plt.figure()
               ax = plt.subplot(111)
               ax.set_prop_cycle(color=[sns.color_palette()[3]])
               sns.lineplot(data=df.loc[:, col],
                            label='prediction')
               plt.axhline(y=pred_thresh, xmin=0, xmax=1,
                           label='prediction\nthreshold',
                           color=sns.color_palette()[7], linestyle='--')
               plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)
               plt.setp(ax, xlabel=r'Time [$ms$]', ylabel=r'artifact probability',
                 title=txt, ylim=[0, 1])
               plot1_file = f'data/exp-220316-publication1/jupyter/plot1_{txt}_preds_{i}'.replace(
                   ' ', '_').replace('\n', '_').replace('(', '').replace(')', '')
               plt.savefig(f'{plot1_file}.pdf', bbox_inches='tight', dpi=300)
               os.system(f'pdf2svg {plot1_file}.pdf {plot1_file}.svg')
               os.system(f'rm {plot1_file}.pdf')
           plt.close('all')

       def plot_bio_prediction_based_segmentation(traces_df, pred_df, txt):
           for i, (t, p) in enumerate(zip(traces_df.items(), pred_df.items())):
               t = t[1]
               p = p[1] > pred_thresh
               p = p[:t.size]
               p_bool = t.max() * p
               p_invbool = t.max() * ~p

               fig = plt.figure()
               ax = plt.subplot(111)
               ax.set_prop_cycle(color=[sns.color_palette()[3]])
               sns.lineplot(data=p_bool, alpha=0.5)
               plt.fill_between(x=p_bool.index,
                                y1=p_bool,
                                y2=0, alpha=0.5, label='$pred_{thr}=1$\n(LUVs\ndominant)')

               ax.set_prop_cycle(color=[sns.color_palette()[2]])
               plt.fill_between(x=p_invbool.index,
                                y1=p_invbool,
                                y2=0, alpha=0.5, label='$pred_{thr}=0$\n(AF488\ndominant)')
               ax.set_prop_cycle(color=[sns.color_palette()[0]])
               sns.lineplot(data=t, label='trace')
               plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)
               plt.setp(ax, xlabel=r'Time [$ms$]', ylabel=r'Intensity [$a.u.$]',
                 title=txt)
               plot1_file = f'data/exp-220316-publication1/jupyter/plot1_{txt}_{i}'.replace(
                   ' ', '_').replace('\n', '_').replace('(', '').replace(')', '')
               plt.savefig(f'{plot1_file}.pdf', bbox_inches='tight', dpi=300)
               os.system(f'pdf2svg {plot1_file}.pdf {plot1_file}.svg')
               os.system(f'rm {plot1_file}.pdf')
           plt.close('all')

       def plot_bio_clean_corr_and_fit(corr_df, fit_df, param_df, txt):
           for i, (c, f, param) in enumerate(zip(corr_df.items(), fit_df.items(), param_df.T.items())):
               c = c[1]
               f = f[1]
               txy = param[1].loc['txy1']
               f_xmin = f.dropna().index[0]
               f_xmax = f.dropna().index[-1]
               xlims = [f_xmin - 0.5*f_xmin, f_xmax + f_xmax]
               ylims = [f.dropna().iloc[-1] - 0.01, f.dropna().iloc[0] + 0.01]

               fig = plt.figure()
               ax = plt.subplot(111)
               ax.set_prop_cycle(color=[sns.color_palette()[0]])
               plt.semilogx(c.index, c, '.', label='Correlation')
               ax.set_prop_cycle(color=[sns.color_palette()[2]])
               plt.semilogx(f.index, f, '-', lw=3.0, label='Fit\n'+rf'$\tau_D=${txy:.3f}')
               plt.axvline(x=txy, color=sns.color_palette()[2])
               plt.setp(ax, xlim=xlims, ylim=ylims, title=txt,
                        xlabel=r'$\tau$ (ms)', ylabel=r'Correlation G($\tau$)')
               ax.grid(False)
               plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)
               plot1_file = f'data/exp-220316-publication1/jupyter/plot1_{txt}_{i}'.replace(
                   ' ', '_').replace('\n', '_').replace('(', '').replace(')', '')
               plt.savefig(f'{plot1_file}.pdf', bbox_inches='tight', dpi=300)
               os.system(f'pdf2svg {plot1_file}.pdf {plot1_file}.svg')
               os.system(f'rm {plot1_file}.pdf')
           plt.close('all')

       def plot_bio_corrs_and_fits(corr_df1, corr_df2, fit_df1, fit_df2, param_df1, param_df2, txt):
           for i, (c1, c2, f1, f2, param1, param2) in enumerate(zip(corr_df1.items(), corr_df2.items(),
                      fit_df1.items(), fit_df2.items(), param_df1.T.items(), param_df2.T.items())):
               c1, c2 = c1[1], c2[1]
               f1, f2 = f1[1], f2[1]
               txy11, txy12 = param1[1].loc['txy1'], param2[1].loc['txy1']
               txy21, txy22 = param1[1].loc['txy2'], param2[1].loc['txy2']
               f_xmin = np.min([f1.dropna().index[0], f2.dropna().index[0]])
               f_xmax = np.max([f1.dropna().index[-1], f2.dropna().index[-1]])
               f_ymin1, f_ymin2 = f1.dropna().iloc[-1], f2.dropna().iloc[-1]
               f_ymax1, f_ymax2 = f1.dropna().iloc[0], f2.dropna().iloc[0]
               xlims = [f_xmin - 0.5*f_xmin, f_xmax + f_xmax]
               ylims1 = [f_ymin1 - np.abs(2*f_ymin1), f_ymax1 + 0.1*f_ymax1]
               ylims2= [f_ymin2 - np.abs(2*f_ymin2), f_ymax2 + 0.1*f_ymax2]
               fig = plt.figure()
               ax1 = plt.subplot(111)
               ax1.set_prop_cycle(color=[sns.color_palette()[0]])
               p1, = ax1.semilogx(c1.index, c1, '.', label='Correlation\n(no correction)')
               ax1.set_prop_cycle(color=[sns.color_palette()[2]])
               f1_label = 'Fit (no correction)\n'+r'$\tau_{D,1}=$'+f'{txy11:.3f}\n'+r'$\tau_{D,2}=$'+f'{txy21:.3f}'
               p2, = ax1.semilogx(f1.index, f1, '-', lw=3.0, label=f1_label)

               ax2 = ax1.twinx()
               ax2.set_prop_cycle(color=[sns.color_palette()[1]])
               p3, = ax2.semilogx(c2.index, c2, '.', label='Correlation\n(cut and shift)')
               ax2.set_prop_cycle(color=[sns.color_palette()[3]])
               f2_label = 'Fit (cut and shift)\n'+r'$\tau_{D,1}=$'+f'{txy12:.3f}\n'+r'$\tau_{D,2}=$'+f'{txy22:.3f}'
               p4, = ax2.semilogx(f2.index, f2, '-', lw=3.0, label=f2_label)
               plt.axvline(x=txy11, color=sns.color_palette()[2])
               plt.axvline(x=txy12, color=sns.color_palette()[3])
               plt.axvline(x=txy21, color=sns.color_palette()[2])
               plt.axvline(x=txy22, color=sns.color_palette()[3])

               plt.setp(ax1, xlim=xlims, ylim=ylims1, title=txt,
                        xlabel=r'$\tau$ (ms)', ylabel=r'Normalized $G(\tau)$',
                        yticklabels=[], yticks=[])
               ax1.grid(False)
               plt.setp(ax2, xlim=xlims, ylim=ylims2, title=txt,
                        xlabel=r'$\tau$ (ms)', yticklabels=[], yticks=[])
               ax2.grid(False)
               plt.legend(handles=[p1, p3, p2, p4], bbox_to_anchor=(1.02, 1),
                          loc='upper left', borderaxespad=0)
               plot1_file = f'data/exp-220316-publication1/jupyter/plot1_{txt}_{i}'.replace(
                   ' ', '_').replace('\n', '_').replace('(', '').replace(')', '')
               plt.savefig(f'{plot1_file}.pdf', bbox_inches='tight', dpi=300)
               os.system(f'pdf2svg {plot1_file}.pdf {plot1_file}.svg')
               os.system(f'rm {plot1_file}.pdf')
           plt.close('all')
     #+END_SRC

     #+RESULTS:

     #+BEGIN_SRC jupyter-python
       plot_bio_traces(traces_clean, txt='AlexaFluor 488 in solution (clean)')
       plot_bio_traces(traces_dirty, txt='AlexaFluor 488 + DiO LUVs in solution (dirty)')
     #+END_SRC

     #+RESULTS:


     file:./data/exp-220316-publication1/jupyter/plot1_AlexaFluor_488_in_solution_clean_0.svg
     file:./data/exp-220316-publication1/jupyter/plot1_AlexaFluor_488_in_solution_clean_1.svg
     file:./data/exp-220316-publication1/jupyter/plot1_AlexaFluor_488_in_solution_clean_2.svg
     file:./data/exp-220316-publication1/jupyter/plot1_AlexaFluor_488_+_DiO_LUVs_in_solution_dirty_0.svg
     file:./data/exp-220316-publication1/jupyter/plot1_AlexaFluor_488_+_DiO_LUVs_in_solution_dirty_1.svg
     file:./data/exp-220316-publication1/jupyter/plot1_AlexaFluor_488_+_DiO_LUVs_in_solution_dirty_2.svg


     #+BEGIN_SRC jupyter-python
       plot_bio_cluster_prediction(preds_clean, txt='AlexaFluor 488 in solution (clean):\nLUV prediction')
       plot_bio_cluster_prediction(preds_dirty, txt='AlexaFluor 488 + DiO LUVs in solution (dirty):\nLUV prediction')
     #+END_SRC

     #+RESULTS:

     file:./data/exp-220316-publication1/jupyter/plot1_AlexaFluor_488_+_DiO_LUVs_in_solution_dirty:_LUV_prediction_preds_0.svg
     file:./data/exp-220316-publication1/jupyter/plot1_AlexaFluor_488_+_DiO_LUVs_in_solution_dirty:_LUV_prediction_preds_1.svg
     file:./data/exp-220316-publication1/jupyter/plot1_AlexaFluor_488_+_DiO_LUVs_in_solution_dirty:_LUV_prediction_preds_2.svg
     [[file:./data/exp-220316-publication1/jupyter/plot1_AlexaFluor_488_in_solution_clean:_LUV_prediction_preds_0.svg]]
     [[file:./data/exp-220316-publication1/jupyter/plot1_AlexaFluor_488_in_solution_clean:_LUV_prediction_preds_1.svg]]
     [[file:./data/exp-220316-publication1/jupyter/plot1_AlexaFluor_488_in_solution_clean:_LUV_prediction_preds_2.svg]]


     #+BEGIN_SRC jupyter-python
       plot_bio_prediction_based_segmentation(
           traces_dirty, preds_dirty,
           txt='AlexaFluor 488 + DiO LUVs in solution (dirty):\nprediction-based segmentation')
       plot_bio_prediction_based_segmentation(
           traces_clean, preds_clean,
           txt='AlexaFluor 488 in solution (clean):\nprediction-based segmentation')
     #+END_SRC

     #+RESULTS:

     file:./data/exp-220316-publication1/jupyter/plot1_AlexaFluor_488_+_DiO_LUVs_in_solution_dirty:_prediction-based_segmentation_0.svg
     file:./data/exp-220316-publication1/jupyter/plot1_AlexaFluor_488_+_DiO_LUVs_in_solution_dirty:_prediction-based_segmentation_1.svg
     file:./data/exp-220316-publication1/jupyter/plot1_AlexaFluor_488_+_DiO_LUVs_in_solution_dirty:_prediction-based_segmentation_2.svg
     [[file:./data/exp-220316-publication1/jupyter/plot1_AlexaFluor_488_in_solution_clean:_prediction-based_segmentation_0.svg]]
     [[file:./data/exp-220316-publication1/jupyter/plot1_AlexaFluor_488_in_solution_clean:_prediction-based_segmentation_1.svg]]
     [[file:./data/exp-220316-publication1/jupyter/plot1_AlexaFluor_488_in_solution_clean:_prediction-based_segmentation_2.svg]]


     #+BEGIN_SRC jupyter-python
       plot_bio_traces(corrtraces_clean,
                       txt='AlexaFluor 488 in solution (clean):\nprediction-based "cut and shift" correction')
       plot_bio_traces(corrtraces_dirty,
                       txt='AlexaFluor 488 + DiO LUVs in solution (dirty):\nprediction-based "cut and shift" correction')
     #+END_SRC

     #+RESULTS:

     file:./data/exp-220316-publication1/jupyter/plot1_AlexaFluor_488_+_DiO_LUVs_in_solution_dirty:_prediction-based_cut_and_shift_correction_0.svg
     file:./data/exp-220316-publication1/jupyter/plot1_AlexaFluor_488_+_DiO_LUVs_in_solution_dirty:_prediction-based_cut_and_shift_correction_1.svg
     file:./data/exp-220316-publication1/jupyter/plot1_AlexaFluor_488_+_DiO_LUVs_in_solution_dirty:_prediction-based_cut_and_shift_correction_2.svg
     [[file:./data/exp-220316-publication1/jupyter/plot1_AlexaFluor_488_in_solution_clean:_prediction-based_cut_and_shift_correction_0.svg]]
     [[file:./data/exp-220316-publication1/jupyter/plot1_AlexaFluor_488_in_solution_clean:_prediction-based_cut_and_shift_correction_1.svg]]
     [[file:./data/exp-220316-publication1/jupyter/plot1_AlexaFluor_488_in_solution_clean:_prediction-based_cut_and_shift_correction_2.svg]]


     #+BEGIN_SRC jupyter-python
       plot_bio_clean_corr_and_fit(corr_clean, fit_clean, param_clean,
                                   txt='AlexaFluor 488 in solution (clean):\nCorrelation and 1-component Fit')
       plot_bio_corrs_and_fits(corr_noc_dirty, corr_cas_dirty, fit_noc_dirty, fit_cas_dirty, param_noc_dirty, param_cas_dirty,
                               txt='AlexaFluor 488 + DiO LUVs in solution (dirty):\nCorrelations and 2-component Fits')

     #+END_SRC

     #+RESULTS:

     file:./data/exp-220316-publication1/jupyter/plot1_AlexaFluor_488_+_DiO_LUVs_in_solution_dirty:_Correlations_and_2-component_Fits_0.svg
     file:./data/exp-220316-publication1/jupyter/plot1_AlexaFluor_488_+_DiO_LUVs_in_solution_dirty:_Correlations_and_2-component_Fits_1.svg
     file:./data/exp-220316-publication1/jupyter/plot1_AlexaFluor_488_+_DiO_LUVs_in_solution_dirty:_Correlations_and_2-component_Fits_2.svg
     [[file:./data/exp-220316-publication1/jupyter/plot1_AlexaFluor_488_in_solution_clean:_Correlation_and_1-component_Fit_0.svg]]
     [[file:./data/exp-220316-publication1/jupyter/plot1_AlexaFluor_488_in_solution_clean:_Correlation_and_1-component_Fit_1.svg]]
     [[file:./data/exp-220316-publication1/jupyter/plot1_AlexaFluor_488_in_solution_clean:_Correlation_and_1-component_Fit_2.svg]]


   - now we load a ptufile just to visualize the correction method on TCSPC data
     (here: TTTR)
     #+BEGIN_SRC jupyter-python
       data_path = Path("/home/lex/Programme/drmed-collections/drmed-bioexps/brightbursts/1911DD_alexafluor488+LUVs")
       path_clean = data_path / 'clean_subsample/'
       path_dirty = data_path / 'dirty_subsample/'
       output_path = Path("data/exp-220316-publication1/220323_bioexps")

       def get_ptu(path, output_path):

           files = [path / f for f in os.listdir(path) if f.endswith('.ptu')]

           for myfile in (files):
               myfile = Path(myfile)
               (out, ptu_tags, ptu_num_records, glob_res) = ptu.import_ptu(myfile)
               (subChanArrFull, trueTimeArrFull, dTimeArrFull,
                resolution) = (out["chanArr"], out["trueTimeArr"],
                               out["dTimeArr"], out["resolution"])
                # Remove Overflow and Markers; they are not handled at the
                # moment.
               subChanArr = np.array([i for i in subChanArrFull
                                      if not isinstance(i, tuple)])
               trueTimeArr = np.array([i for i in trueTimeArrFull
                                       if not isinstance(i, tuple)])
               dTimeArr = np.array([i for i in dTimeArrFull
                                    if not isinstance(i, tuple)])
               return trueTimeArr, dTimeArr

       ptufile = get_ptu(path_dirty, output_path)
     #+END_SRC

     #+RESULTS:

     #+BEGIN_SRC jupyter-python
       test = pd.DataFrame(data=[ptufile[0], ptufile[1]], index=['macroscopic times', 'microscobpic times'], dtype=int)
       test
     #+END_SRC

     #+RESULTS:
     :RESULTS:
     : /home/lex/Programme/miniconda3/envs/tf/lib/python3.9/site-packages/pandas/core/frame.py:702: FutureWarning: In a future version, passing float-dtype values and an integer dtype to DataFrame will retain floating dtype if they cannot be cast losslessly (matching Series behavior). To retain the old behavior, use DataFrame(data).astype(dtype)
     :   mgr = arrays_to_mgr(
     |                    |   0 |    1 |    2 | ... |    5018346 |    5018347 |    5018348 |
     |--------------------+-----+------+------+-----+------------+------------+------------|
     | macroscopic times  | 875 | 1400 | 2525 | ... | 9996191498 | 9996192848 | 9996196673 |
     | microscobpic times |  39 |   46 |  602 | ... |        238 |        111 |         42 |

     2 rows × 5018349 columns
     :END:

     #+BEGIN_SRC jupyter-python

       display(test.iloc[:, 750:753])
       display(test.iloc[:, 1540:1543])
     #+END_SRC

     #+RESULTS:
     :RESULTS:
     |                    |     750 |     751 |     752 |
     |--------------------+---------+---------+---------|
     | macroscopic times  |  999781 | 1001257 | 1001582 |
     | microscobpic times |      84 |     101 |     220 |

     |                    |    1540 |    1541 |    1542 |
     |--------------------+---------+---------+---------|
     | macroscopic times  | 1999613 | 2000339 | 2000814 |
     | microscopic times  |      72 |     103 |     970 |
     :END:

*** the trained models

   10. all metrics after 100th epoch with hparams
       | run                                | val_auc | val_f1 0.5 | val_prec 0.5 | val_recall 0.5 | model size | hp_batch_size | hp_first_filters | hp_input_size       | hp_lr_power |        hp_lr_start | hp_n_levels | hp_pool_size | hp_scaler |
       |------------------------------------+---------+------------+--------------+----------------+------------+---------------+------------------+---------------------+-------------+--------------------+-------------+--------------+-----------|
       | 484af471c61943fa90e5f78e78a229f0   |  0.9814 |     0.9187 |       0.9091 |         0.9285 | 275 MB     |            26 |               44 | 16384 (here: 14000) |           1 | 0.0136170138242663 |           7 |            2 | standard  |
       | 0cd2023eeaf745aca0d3e8ad5e1fc653   |  0.9818 |     0.9069 |       0.8955 |         0.9185 | 200 MB     |            15 |               23 | 16384 (here: 14000) |           7 | 0.0305060808685107 |           6 |            4 | quant_g   |
       | fe81d71c52404ed790b3a32051258da9   |  0.9849 |     0.9260 |       0.9184 |         0.9338 | 186 MB     |            20 |               78 | 16384 (here: 14000) |           4 | 0.0584071108418767 |           4 |            4 | standard  |
       | ff67be0b68e540a9a29a36a2d0c7a5be + |  0.9859 |     0.9298 |       0.9230 |         0.9367 | 14 MB      |            28 |                6 | 16384 (here: 14000) |           1 | 0.0553313915596308 |           5 |            4 | minmax    |
       | 19e3e786e1bc4e2b93856f5dc9de8216   |  0.9595 |     0.8911 |       0.8983 |         0.8839 | 172 MB     |            20 |              128 | 16384 (here: 14000) |           1 |  0.043549707353273 |           3 |            4 | standard  |
       | 347669d050f344ad9fb9e480c814f727 + |  0.9848 |     0.9246 |       0.9254 |         0.9238 | 73 MB      |            10 |               16 | 8192 (here: 14000)  |           1 | 0.0627676336651573 |           5 |            4 | robust    |
       | c1204e3a8a1e4c40a35b5b7b1922d1ce   |  0.9858 |     0.9207 |       0.9179 |         0.9234 | 312 MB     |            14 |               16 | 16384 (here: 14000) |           5 | 0.0192390310290551 |           9 |            2 | robust    |
       | 714af8cd12c1441eac4ca980e8c20070 + |  0.9843 |     0.9304 |       0.9257 |         0.9352 | 234 MB     |             9 |               64 | 4096 (here: 14000)  |           1 | 0.0100697459464075 |           5 |            4 | maxabs    |
       | 34a6d207ac594035b1009c330fb67a65 + |  0.9652 |     0.8613 |       0.8598 |         0.8629 | 7 MB       |            17 |               16 | 16384 (here: 14000) |           5 | 0.0101590069352232 |           3 |            4 | l2        |
